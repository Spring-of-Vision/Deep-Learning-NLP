{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i feel awful about it too because it s my job ...      0\n",
       "1                              im alone i feel awful      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################## IMPORT DATA ##################\n",
    "\n",
    "import pandas as pd\n",
    "# Read the Parquet file\n",
    "file_path = \"train-00000-of-00001.parquet\"  # Replace with your Parquet file path\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after undersampling:\n",
      "label\n",
      "0    14972\n",
      "1    14972\n",
      "2    14972\n",
      "3    14972\n",
      "4    14972\n",
      "5    14972\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noams\\AppData\\Local\\Temp\\ipykernel_10456\\3946450975.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('label')\n"
     ]
    }
   ],
   "source": [
    "################## BALANCED DF CREATION ##################\n",
    "\n",
    "# Count samples for each label\n",
    "label_counts = df['label'].value_counts()\n",
    "minority_class_size = label_counts.min()\n",
    "\n",
    "# Group by the category and sample the minority class size for each group\n",
    "balanced_df = (\n",
    "    df.groupby('label')\n",
    "    .apply(lambda x: x.sample(n=minority_class_size, random_state=42))  # Randomly select samples\n",
    "    .reset_index(drop=True)  # Reset the index\n",
    ")\n",
    "\n",
    "print(\"Class distribution after undersampling:\")\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89832, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel sorry about you because your point of v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like he s watching quietly because he s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i feel sorry about you because your point of v...      0\n",
       "1  i feel like he s watching quietly because he s...      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = balanced_df\n",
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84900, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel sorry about you because your point of v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like he s watching quietly because he s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i feel sorry about you because your point of v...      0\n",
       "1  i feel like he s watching quietly because he s...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### HASN'T BEEN EXECUTED!! TO AVOID FITTING TO THE DATA #######\n",
    "\n",
    "# # remove all the sentences that are longer than 40 words\n",
    "# df_preprocessed = df_preprocessed[df_preprocessed['text'].apply(lambda x: len(x.split()) <= 40 and len(x.split()) >= 3)]\n",
    "# print(df_preprocessed.shape)\n",
    "# df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing The Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89832/89832 [00:07<00:00, 11970.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel sorry about you because your point of v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, feel, sorry, about, you, because, your, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like he s watching quietly because he s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, feel, like, he, s, watching, quietly, beca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  i feel sorry about you because your point of v...      0   \n",
       "1  i feel like he s watching quietly because he s...      0   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [i, feel, sorry, about, you, because, your, po...  \n",
       "1  [i, feel, like, he, s, watching, quietly, beca...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Tokenize each text and store tokenized output in a new column\n",
    "df_preprocessed['tokenized_text'] = df_preprocessed['text'].progress_apply(lambda x: tokenizer.tokenize(x))\n",
    "df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using The Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.15019275e-03  1.05880331e-02  5.85333593e-02 -1.80088747e-02\n",
      "  2.17381623e-02 -6.36548251e-02  4.79345582e-02  5.82478940e-04\n",
      " -2.71546952e-02  4.41303067e-02 -2.70192716e-02  6.18359353e-03\n",
      " -1.98998721e-03  3.27444300e-02  4.41248150e-04  3.95078957e-02\n",
      " -3.66007313e-02 -2.67524342e-03  5.90687105e-03  2.27801464e-02\n",
      " -3.65645699e-02 -4.19695750e-02 -7.16437120e-03 -4.24787998e-02\n",
      "  4.29837480e-02  2.15609614e-02  2.45211683e-02  1.77936815e-02\n",
      "  1.41702415e-02 -2.20726691e-02  3.10446555e-03 -7.80420452e-02\n",
      "  1.17998419e-03 -9.21797939e-03  6.47169817e-03 -2.28024814e-02\n",
      "  4.87457886e-02 -4.71891044e-03 -2.68589742e-02  2.32070964e-02\n",
      " -1.11313999e-01 -4.35941219e-02 -2.73997393e-02  9.44887754e-03\n",
      " -3.80694109e-04  7.03306049e-02  2.92210635e-02  2.60734255e-03\n",
      " -1.09368563e-01  3.23175937e-02  1.05939796e-02 -1.16323661e-02\n",
      "  3.07511836e-02  2.60094590e-02 -8.07798654e-02  1.67720411e-02\n",
      "  3.48081253e-02  2.77459081e-02 -1.08297467e-01 -5.31493500e-03\n",
      "  1.44641800e-02 -4.21016067e-02  1.74080562e-02  4.53213602e-02\n",
      " -2.03130450e-02  2.74548884e-02 -6.88411202e-03 -2.46204473e-02\n",
      "  2.61567142e-02 -3.83009203e-02  1.16500808e-02 -8.17218609e-03\n",
      "  8.02695658e-03  3.42647592e-03  3.51003483e-02 -1.82733461e-02\n",
      "  4.32076678e-03 -4.56346609e-02 -3.58041981e-03  7.33157969e-04\n",
      " -7.00480789e-02  1.51639162e-02 -2.93170307e-02 -2.26665963e-03\n",
      " -1.46285852e-03 -4.92869085e-03  1.23468535e-02 -1.64976879e-03\n",
      "  8.98252148e-03  2.62601525e-02 -2.32274830e-02  3.29418071e-02\n",
      " -6.57121825e-04 -1.07547581e-01  2.33253278e-02  1.39847517e-01\n",
      "  1.58148892e-02 -1.64823269e-03 -2.84349229e-02  5.61736058e-03\n",
      " -1.67369023e-02  2.66228225e-02  6.94100326e-03  8.41293857e-03\n",
      "  1.51664426e-03 -3.75236198e-02 -1.37651367e-02  1.14708664e-02\n",
      "  2.40539201e-02  4.66846898e-02 -6.71437234e-02 -1.71261318e-02\n",
      "  3.35687393e-04 -5.62422201e-02 -3.22531536e-02 -4.00833748e-02\n",
      " -4.11019251e-02  2.01759152e-02 -2.96448208e-02 -1.05031177e-01\n",
      " -3.46056521e-02  2.21012570e-02  4.47731242e-02 -1.29172336e-02\n",
      "  8.09648912e-03  4.79777828e-02  1.44451531e-02  2.04787706e-03\n",
      " -3.01583912e-02 -2.26560347e-02 -5.61325066e-03 -1.58980675e-02\n",
      " -2.46041715e-02  6.54169312e-03  1.08528119e-02 -1.99720380e-03\n",
      " -3.64154987e-02 -2.72105075e-03 -1.79823264e-02 -1.05963647e-02\n",
      "  2.41233371e-02 -5.69446757e-03 -6.53078929e-02  1.23347845e-02\n",
      " -8.17020983e-02 -3.35735152e-03 -8.71597230e-03 -8.98809284e-02\n",
      "  3.94005841e-03  3.25443298e-02 -2.04956699e-02 -5.04665375e-02\n",
      " -3.28555182e-02 -2.24030763e-03  7.86474533e-03 -2.72138901e-02\n",
      "  7.01845298e-03  2.00957097e-02  2.62387749e-02 -1.19036958e-02\n",
      "  1.24580692e-02  4.23062369e-02  1.99226998e-02 -3.18395346e-02\n",
      " -2.77881511e-02  6.25803843e-02 -7.63805816e-03  2.92812139e-02\n",
      "  2.07703561e-02 -2.04395689e-02  1.50343403e-02  1.24745467e-03\n",
      " -9.26262699e-03  1.06976014e-02 -1.53755967e-03  1.53396921e-02\n",
      " -1.47519382e-02  4.05351724e-03 -2.72588097e-02 -2.23288313e-02\n",
      " -1.43179251e-02 -1.48736862e-02 -6.97966572e-03 -6.25730827e-02\n",
      " -9.24157277e-02 -4.99901269e-03  3.90528282e-03  1.08032107e-01\n",
      " -2.22198945e-02  5.37365675e-04 -5.61015308e-03 -1.26408292e-02\n",
      " -3.95569131e-02 -3.68211940e-02 -1.47302449e-02 -4.51780204e-03\n",
      " -8.49625387e-04  1.00242789e-03 -4.06150566e-03  1.16499308e-02\n",
      " -8.09082165e-02 -3.61052516e-04  2.45568417e-02  5.52267116e-03\n",
      "  1.62120517e-02 -5.66453338e-02  3.00804935e-02  1.02142263e-02\n",
      "  7.48731475e-03  1.39939431e-02 -7.50672892e-02  1.99853852e-02\n",
      "  9.86382961e-02  6.40084501e-03 -1.68929789e-02  2.70522330e-02\n",
      "  1.90184899e-02 -5.54622570e-03 -2.61672381e-02 -1.32325664e-02\n",
      " -2.06757500e-03 -2.54046312e-03 -5.19485585e-02 -9.45406593e-03\n",
      "  9.06954631e-02  1.59165375e-02 -9.35550686e-03  8.86306074e-03\n",
      "  6.12023054e-03  1.54464273e-02  8.62891413e-03 -3.93600389e-02\n",
      "  4.45931181e-02 -6.83805253e-03  1.77116469e-02 -9.93025862e-03\n",
      " -1.07441898e-02  9.28017893e-04  1.87607743e-02  2.53707031e-03\n",
      "  4.24202196e-02 -1.79975517e-02  3.28217214e-03 -1.37960566e-02\n",
      " -2.63354816e-02 -2.29472443e-02 -9.94787086e-03  1.50219081e-02\n",
      " -2.90881302e-02  2.60935370e-02  1.01650074e-01 -2.07196809e-02\n",
      " -1.18812323e-02  1.88614316e-02 -2.04383582e-02  2.92565790e-03\n",
      "  1.22541916e-02 -7.16641406e-03  5.64003875e-03  9.38325282e-03\n",
      " -5.17074578e-02  6.79351836e-02 -1.79398097e-02  2.81262994e-02\n",
      "  1.78447235e-02  6.18008785e-02 -2.20022872e-02  7.90594984e-03\n",
      "  7.04661012e-03 -5.23740351e-02 -2.28937902e-02 -1.43831018e-02\n",
      " -3.28972042e-02  6.78121019e-03 -4.62064613e-03  3.64981256e-02\n",
      " -1.44333867e-02  5.65662235e-03  2.54673953e-03  1.99225899e-02\n",
      "  2.87093278e-02  3.56950378e-03 -1.34036522e-02  3.17565687e-02\n",
      " -3.86744738e-03  1.56376343e-02 -4.04994860e-02  4.72432235e-03\n",
      " -3.55616203e-06  2.50393637e-02  3.22945490e-02 -9.45789367e-03\n",
      "  3.54374829e-03  4.07337630e-03  5.78961801e-03  5.26164994e-02\n",
      "  8.23039282e-03  2.14898866e-02  3.39484848e-02  1.17234122e-02]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "fasttext_path = \"crawl-300d-2M-subword/crawl-300d-2M-subword.bin\"\n",
    "fasttext = load_facebook_model(fasttext_path)\n",
    "\n",
    "# Check an example word vector\n",
    "print(fasttext.wv['happy'])  # This should now work without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "# it's assumed that the FastText model is already loaded and stored in the `fasttext` variable\n",
    "fasttext\n",
    "\n",
    "# data\n",
    "X = df_preprocessed['tokenized_text'].tolist()\n",
    "y = df_preprocessed['label'].tolist()\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Preprocess data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y, fasttext):\n",
    "        # Use fasttext.wv for word lookup\n",
    "        self.X = [torch.tensor([fasttext.wv[token] if token in fasttext.wv else np.zeros(fasttext.vector_size)\n",
    "                                for token in seq]) for seq in X]\n",
    "        self.y = torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, fasttext)\n",
    "val_dataset = TextDataset(X_val, y_val, fasttext)\n",
    "test_dataset = TextDataset(X_test, y_test, fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [10/1438] Batch Loss: 1.8047, Training Accuracy: 0.1703, Validation Accuracy: 0.1682\n",
      "Epoch [1/30], Batch [20/1438] Batch Loss: 1.8137, Training Accuracy: 0.1674, Validation Accuracy: 0.1702\n",
      "Epoch [1/30], Batch [30/1438] Batch Loss: 1.7844, Training Accuracy: 0.1828, Validation Accuracy: 0.1809\n",
      "Epoch [1/30], Batch [40/1438] Batch Loss: 1.7838, Training Accuracy: 0.2020, Validation Accuracy: 0.1975\n",
      "Epoch [1/30], Batch [50/1438] Batch Loss: 1.7922, Training Accuracy: 0.1945, Validation Accuracy: 0.1984\n",
      "Epoch [1/30], Batch [60/1438] Batch Loss: 1.7956, Training Accuracy: 0.1756, Validation Accuracy: 0.1737\n",
      "Epoch [1/30], Batch [70/1438] Batch Loss: 1.7993, Training Accuracy: 0.1903, Validation Accuracy: 0.1926\n",
      "Epoch [1/30], Batch [80/1438] Batch Loss: 1.7860, Training Accuracy: 0.1994, Validation Accuracy: 0.1990\n",
      "Epoch [1/30], Batch [90/1438] Batch Loss: 1.7954, Training Accuracy: 0.2570, Validation Accuracy: 0.2596\n",
      "Epoch [1/30], Batch [100/1438] Batch Loss: 1.8084, Training Accuracy: 0.2245, Validation Accuracy: 0.2202\n",
      "Epoch [1/30], Batch [110/1438] Batch Loss: 1.7839, Training Accuracy: 0.2828, Validation Accuracy: 0.2781\n",
      "Epoch [1/30], Batch [120/1438] Batch Loss: 1.7829, Training Accuracy: 0.2326, Validation Accuracy: 0.2338\n",
      "Epoch [1/30], Batch [130/1438] Batch Loss: 1.7687, Training Accuracy: 0.2370, Validation Accuracy: 0.2379\n",
      "Epoch [1/30], Batch [140/1438] Batch Loss: 1.7472, Training Accuracy: 0.2327, Validation Accuracy: 0.2326\n",
      "Epoch [1/30], Batch [150/1438] Batch Loss: 1.6953, Training Accuracy: 0.2991, Validation Accuracy: 0.3033\n",
      "Epoch [1/30], Batch [160/1438] Batch Loss: 1.6866, Training Accuracy: 0.2646, Validation Accuracy: 0.2631\n",
      "Epoch [1/30], Batch [170/1438] Batch Loss: 1.6772, Training Accuracy: 0.3291, Validation Accuracy: 0.3314\n",
      "Epoch [1/30], Batch [180/1438] Batch Loss: 1.7488, Training Accuracy: 0.2386, Validation Accuracy: 0.2443\n",
      "Epoch [1/30], Batch [190/1438] Batch Loss: 1.7368, Training Accuracy: 0.3123, Validation Accuracy: 0.3180\n",
      "Epoch [1/30], Batch [200/1438] Batch Loss: 1.4998, Training Accuracy: 0.3047, Validation Accuracy: 0.3107\n",
      "Epoch [1/30], Batch [210/1438] Batch Loss: 1.3522, Training Accuracy: 0.3194, Validation Accuracy: 0.3191\n",
      "Epoch [1/30], Batch [220/1438] Batch Loss: 1.5391, Training Accuracy: 0.3631, Validation Accuracy: 0.3639\n",
      "Epoch [1/30], Batch [230/1438] Batch Loss: 1.3425, Training Accuracy: 0.3958, Validation Accuracy: 0.4010\n",
      "Epoch [1/30], Batch [240/1438] Batch Loss: 1.4201, Training Accuracy: 0.4669, Validation Accuracy: 0.4682\n",
      "Epoch [1/30], Batch [250/1438] Batch Loss: 1.2184, Training Accuracy: 0.4207, Validation Accuracy: 0.4241\n",
      "Epoch [1/30], Batch [260/1438] Batch Loss: 1.3363, Training Accuracy: 0.4975, Validation Accuracy: 0.5014\n",
      "Epoch [1/30], Batch [270/1438] Batch Loss: 1.1788, Training Accuracy: 0.5409, Validation Accuracy: 0.5441\n",
      "Epoch [1/30], Batch [280/1438] Batch Loss: 1.2363, Training Accuracy: 0.5093, Validation Accuracy: 0.5123\n",
      "Epoch [1/30], Batch [290/1438] Batch Loss: 1.1657, Training Accuracy: 0.5464, Validation Accuracy: 0.5451\n",
      "Epoch [1/30], Batch [300/1438] Batch Loss: 1.2181, Training Accuracy: 0.5514, Validation Accuracy: 0.5559\n",
      "Epoch [1/30], Batch [310/1438] Batch Loss: 1.1065, Training Accuracy: 0.5728, Validation Accuracy: 0.5765\n",
      "Epoch [1/30], Batch [320/1438] Batch Loss: 1.0378, Training Accuracy: 0.5428, Validation Accuracy: 0.5417\n",
      "Epoch [1/30], Batch [330/1438] Batch Loss: 1.1786, Training Accuracy: 0.6085, Validation Accuracy: 0.6110\n",
      "Epoch [1/30], Batch [340/1438] Batch Loss: 0.8817, Training Accuracy: 0.5971, Validation Accuracy: 0.5955\n",
      "Epoch [1/30], Batch [350/1438] Batch Loss: 1.0970, Training Accuracy: 0.6526, Validation Accuracy: 0.6614\n",
      "Epoch [1/30], Batch [360/1438] Batch Loss: 0.8941, Training Accuracy: 0.6736, Validation Accuracy: 0.6784\n",
      "Epoch [1/30], Batch [370/1438] Batch Loss: 0.7436, Training Accuracy: 0.6795, Validation Accuracy: 0.6922\n",
      "Epoch [1/30], Batch [380/1438] Batch Loss: 0.8080, Training Accuracy: 0.6990, Validation Accuracy: 0.7056\n",
      "Epoch [1/30], Batch [390/1438] Batch Loss: 1.0868, Training Accuracy: 0.6973, Validation Accuracy: 0.7038\n",
      "Epoch [1/30], Batch [400/1438] Batch Loss: 0.9149, Training Accuracy: 0.7141, Validation Accuracy: 0.7202\n",
      "Epoch [1/30], Batch [410/1438] Batch Loss: 1.1887, Training Accuracy: 0.6734, Validation Accuracy: 0.6792\n",
      "Epoch [1/30], Batch [420/1438] Batch Loss: 1.0450, Training Accuracy: 0.6858, Validation Accuracy: 0.6899\n",
      "Epoch [1/30], Batch [430/1438] Batch Loss: 0.9428, Training Accuracy: 0.7193, Validation Accuracy: 0.7236\n",
      "Epoch [1/30], Batch [440/1438] Batch Loss: 0.8376, Training Accuracy: 0.7439, Validation Accuracy: 0.7482\n",
      "Epoch [1/30], Batch [450/1438] Batch Loss: 1.0182, Training Accuracy: 0.7188, Validation Accuracy: 0.7246\n",
      "Epoch [1/30], Batch [460/1438] Batch Loss: 0.8189, Training Accuracy: 0.7464, Validation Accuracy: 0.7531\n",
      "Epoch [1/30], Batch [470/1438] Batch Loss: 0.9032, Training Accuracy: 0.7504, Validation Accuracy: 0.7566\n",
      "Epoch [1/30], Batch [480/1438] Batch Loss: 0.6927, Training Accuracy: 0.7613, Validation Accuracy: 0.7653\n",
      "Epoch [1/30], Batch [490/1438] Batch Loss: 0.7926, Training Accuracy: 0.7665, Validation Accuracy: 0.7698\n",
      "Epoch [1/30], Batch [500/1438] Batch Loss: 0.9450, Training Accuracy: 0.7674, Validation Accuracy: 0.7719\n",
      "Epoch [1/30], Batch [510/1438] Batch Loss: 0.7568, Training Accuracy: 0.7673, Validation Accuracy: 0.7730\n",
      "Epoch [1/30], Batch [520/1438] Batch Loss: 0.7943, Training Accuracy: 0.7902, Validation Accuracy: 0.7916\n",
      "Epoch [1/30], Batch [530/1438] Batch Loss: 0.7145, Training Accuracy: 0.7845, Validation Accuracy: 0.7868\n",
      "Epoch [1/30], Batch [540/1438] Batch Loss: 0.6222, Training Accuracy: 0.8077, Validation Accuracy: 0.8121\n",
      "Epoch [1/30], Batch [550/1438] Batch Loss: 0.4771, Training Accuracy: 0.8142, Validation Accuracy: 0.8151\n",
      "Epoch [1/30], Batch [560/1438] Batch Loss: 0.8284, Training Accuracy: 0.8280, Validation Accuracy: 0.8295\n",
      "Epoch [1/30], Batch [570/1438] Batch Loss: 0.4282, Training Accuracy: 0.7975, Validation Accuracy: 0.7980\n",
      "Epoch [1/30], Batch [580/1438] Batch Loss: 0.6615, Training Accuracy: 0.8291, Validation Accuracy: 0.8291\n",
      "Epoch [1/30], Batch [590/1438] Batch Loss: 0.5806, Training Accuracy: 0.8239, Validation Accuracy: 0.8231\n",
      "Epoch [1/30], Batch [600/1438] Batch Loss: 0.5043, Training Accuracy: 0.8327, Validation Accuracy: 0.8323\n",
      "Epoch [1/30], Batch [610/1438] Batch Loss: 0.5457, Training Accuracy: 0.8353, Validation Accuracy: 0.8337\n",
      "Epoch [1/30], Batch [620/1438] Batch Loss: 0.5232, Training Accuracy: 0.8277, Validation Accuracy: 0.8253\n",
      "Epoch [1/30], Batch [630/1438] Batch Loss: 0.4684, Training Accuracy: 0.8196, Validation Accuracy: 0.8191\n",
      "Epoch [1/30], Batch [640/1438] Batch Loss: 0.4916, Training Accuracy: 0.8237, Validation Accuracy: 0.8223\n",
      "Epoch [1/30], Batch [650/1438] Batch Loss: 0.3594, Training Accuracy: 0.8288, Validation Accuracy: 0.8263\n",
      "Epoch [1/30], Batch [660/1438] Batch Loss: 0.5261, Training Accuracy: 0.8394, Validation Accuracy: 0.8390\n",
      "Epoch [1/30], Batch [670/1438] Batch Loss: 0.7697, Training Accuracy: 0.8404, Validation Accuracy: 0.8423\n",
      "Epoch [1/30], Batch [680/1438] Batch Loss: 0.5683, Training Accuracy: 0.8500, Validation Accuracy: 0.8486\n",
      "Epoch [1/30], Batch [690/1438] Batch Loss: 0.5009, Training Accuracy: 0.8537, Validation Accuracy: 0.8547\n",
      "Epoch [1/30], Batch [700/1438] Batch Loss: 0.5282, Training Accuracy: 0.8529, Validation Accuracy: 0.8538\n",
      "Epoch [1/30], Batch [710/1438] Batch Loss: 0.6046, Training Accuracy: 0.8483, Validation Accuracy: 0.8490\n",
      "Epoch [1/30], Batch [720/1438] Batch Loss: 0.4577, Training Accuracy: 0.8335, Validation Accuracy: 0.8359\n",
      "Epoch [1/30], Batch [730/1438] Batch Loss: 0.5615, Training Accuracy: 0.8584, Validation Accuracy: 0.8579\n",
      "Epoch [1/30], Batch [740/1438] Batch Loss: 0.4455, Training Accuracy: 0.8520, Validation Accuracy: 0.8499\n",
      "Epoch [1/30], Batch [750/1438] Batch Loss: 0.5288, Training Accuracy: 0.8615, Validation Accuracy: 0.8592\n",
      "Epoch [1/30], Batch [760/1438] Batch Loss: 0.4698, Training Accuracy: 0.8632, Validation Accuracy: 0.8634\n",
      "Epoch [1/30], Batch [770/1438] Batch Loss: 0.7006, Training Accuracy: 0.8553, Validation Accuracy: 0.8551\n",
      "Epoch [1/30], Batch [780/1438] Batch Loss: 0.4145, Training Accuracy: 0.8460, Validation Accuracy: 0.8503\n",
      "Epoch [1/30], Batch [790/1438] Batch Loss: 0.6677, Training Accuracy: 0.8622, Validation Accuracy: 0.8645\n",
      "Epoch [1/30], Batch [800/1438] Batch Loss: 0.7453, Training Accuracy: 0.8731, Validation Accuracy: 0.8727\n",
      "Epoch [1/30], Batch [810/1438] Batch Loss: 0.5029, Training Accuracy: 0.8799, Validation Accuracy: 0.8767\n",
      "Epoch [1/30], Batch [820/1438] Batch Loss: 0.4763, Training Accuracy: 0.8762, Validation Accuracy: 0.8743\n",
      "Epoch [1/30], Batch [830/1438] Batch Loss: 0.4595, Training Accuracy: 0.8669, Validation Accuracy: 0.8647\n",
      "Epoch [1/30], Batch [840/1438] Batch Loss: 0.5676, Training Accuracy: 0.8786, Validation Accuracy: 0.8753\n",
      "Epoch [1/30], Batch [850/1438] Batch Loss: 0.3596, Training Accuracy: 0.8826, Validation Accuracy: 0.8783\n",
      "Epoch [1/30], Batch [860/1438] Batch Loss: 0.6646, Training Accuracy: 0.8453, Validation Accuracy: 0.8416\n",
      "Epoch [1/30], Batch [870/1438] Batch Loss: 0.5361, Training Accuracy: 0.8548, Validation Accuracy: 0.8511\n",
      "Epoch [1/30], Batch [880/1438] Batch Loss: 0.4844, Training Accuracy: 0.8707, Validation Accuracy: 0.8654\n",
      "Epoch [1/30], Batch [890/1438] Batch Loss: 0.5899, Training Accuracy: 0.8667, Validation Accuracy: 0.8627\n",
      "Epoch [1/30], Batch [900/1438] Batch Loss: 0.5602, Training Accuracy: 0.8571, Validation Accuracy: 0.8582\n",
      "Epoch [1/30], Batch [910/1438] Batch Loss: 0.4480, Training Accuracy: 0.8779, Validation Accuracy: 0.8760\n",
      "Epoch [1/30], Batch [920/1438] Batch Loss: 0.6984, Training Accuracy: 0.8804, Validation Accuracy: 0.8766\n",
      "Epoch [1/30], Batch [930/1438] Batch Loss: 0.2883, Training Accuracy: 0.8819, Validation Accuracy: 0.8764\n",
      "Epoch [1/30], Batch [940/1438] Batch Loss: 0.4513, Training Accuracy: 0.8814, Validation Accuracy: 0.8764\n",
      "Epoch [1/30], Batch [950/1438] Batch Loss: 0.4167, Training Accuracy: 0.8760, Validation Accuracy: 0.8750\n",
      "Epoch [1/30], Batch [960/1438] Batch Loss: 0.4712, Training Accuracy: 0.8850, Validation Accuracy: 0.8808\n",
      "Epoch [1/30], Batch [970/1438] Batch Loss: 0.6413, Training Accuracy: 0.8848, Validation Accuracy: 0.8823\n",
      "Epoch [1/30], Batch [980/1438] Batch Loss: 0.3446, Training Accuracy: 0.8874, Validation Accuracy: 0.8856\n",
      "Epoch [1/30], Batch [990/1438] Batch Loss: 0.5403, Training Accuracy: 0.8785, Validation Accuracy: 0.8787\n",
      "Epoch [1/30], Batch [1000/1438] Batch Loss: 0.5932, Training Accuracy: 0.8674, Validation Accuracy: 0.8652\n",
      "Epoch [1/30], Batch [1010/1438] Batch Loss: 0.3989, Training Accuracy: 0.8766, Validation Accuracy: 0.8721\n",
      "Epoch [1/30], Batch [1020/1438] Batch Loss: 0.3994, Training Accuracy: 0.8651, Validation Accuracy: 0.8631\n",
      "Epoch [1/30], Batch [1030/1438] Batch Loss: 0.7638, Training Accuracy: 0.8683, Validation Accuracy: 0.8644\n",
      "Epoch [1/30], Batch [1040/1438] Batch Loss: 0.4488, Training Accuracy: 0.8791, Validation Accuracy: 0.8774\n",
      "Epoch [1/30], Batch [1050/1438] Batch Loss: 0.3241, Training Accuracy: 0.8861, Validation Accuracy: 0.8832\n",
      "Epoch [1/30], Batch [1060/1438] Batch Loss: 0.5002, Training Accuracy: 0.8898, Validation Accuracy: 0.8861\n",
      "Epoch [1/30], Batch [1070/1438] Batch Loss: 0.4460, Training Accuracy: 0.8908, Validation Accuracy: 0.8885\n",
      "Epoch [1/30], Batch [1080/1438] Batch Loss: 0.3255, Training Accuracy: 0.8981, Validation Accuracy: 0.8949\n",
      "Epoch [1/30], Batch [1090/1438] Batch Loss: 0.3300, Training Accuracy: 0.8886, Validation Accuracy: 0.8851\n",
      "Epoch [1/30], Batch [1100/1438] Batch Loss: 0.3962, Training Accuracy: 0.8920, Validation Accuracy: 0.8906\n",
      "Epoch [1/30], Batch [1110/1438] Batch Loss: 0.2911, Training Accuracy: 0.9037, Validation Accuracy: 0.9016\n",
      "Epoch [1/30], Batch [1120/1438] Batch Loss: 0.4000, Training Accuracy: 0.8962, Validation Accuracy: 0.8946\n",
      "Epoch [1/30], Batch [1130/1438] Batch Loss: 0.4509, Training Accuracy: 0.9015, Validation Accuracy: 0.8970\n",
      "Epoch [1/30], Batch [1140/1438] Batch Loss: 0.5499, Training Accuracy: 0.9012, Validation Accuracy: 0.8995\n",
      "Epoch [1/30], Batch [1150/1438] Batch Loss: 0.4677, Training Accuracy: 0.9073, Validation Accuracy: 0.9046\n",
      "Epoch [1/30], Batch [1160/1438] Batch Loss: 0.4066, Training Accuracy: 0.9066, Validation Accuracy: 0.9046\n",
      "Epoch [1/30], Batch [1170/1438] Batch Loss: 0.3461, Training Accuracy: 0.9020, Validation Accuracy: 0.8990\n",
      "Epoch [1/30], Batch [1180/1438] Batch Loss: 0.5824, Training Accuracy: 0.9029, Validation Accuracy: 0.9004\n",
      "Epoch [1/30], Batch [1190/1438] Batch Loss: 0.5164, Training Accuracy: 0.8982, Validation Accuracy: 0.8954\n",
      "Epoch [1/30], Batch [1200/1438] Batch Loss: 0.2533, Training Accuracy: 0.8964, Validation Accuracy: 0.8968\n",
      "Epoch [1/30], Batch [1210/1438] Batch Loss: 0.4034, Training Accuracy: 0.8971, Validation Accuracy: 0.8950\n",
      "Epoch [1/30], Batch [1220/1438] Batch Loss: 0.4588, Training Accuracy: 0.8972, Validation Accuracy: 0.8963\n",
      "Epoch [1/30], Batch [1230/1438] Batch Loss: 0.5537, Training Accuracy: 0.8836, Validation Accuracy: 0.8828\n",
      "Epoch [1/30], Batch [1240/1438] Batch Loss: 0.4718, Training Accuracy: 0.8952, Validation Accuracy: 0.8940\n",
      "Epoch [1/30], Batch [1250/1438] Batch Loss: 0.6397, Training Accuracy: 0.8910, Validation Accuracy: 0.8903\n",
      "Epoch [1/30], Batch [1260/1438] Batch Loss: 0.4458, Training Accuracy: 0.8936, Validation Accuracy: 0.8896\n",
      "Epoch [1/30], Batch [1270/1438] Batch Loss: 0.5239, Training Accuracy: 0.9009, Validation Accuracy: 0.8993\n",
      "Epoch [1/30], Batch [1280/1438] Batch Loss: 0.2692, Training Accuracy: 0.8976, Validation Accuracy: 0.8945\n",
      "Epoch [1/30], Batch [1290/1438] Batch Loss: 0.2786, Training Accuracy: 0.8889, Validation Accuracy: 0.8874\n",
      "Epoch [1/30], Batch [1300/1438] Batch Loss: 0.5525, Training Accuracy: 0.8748, Validation Accuracy: 0.8704\n",
      "Epoch [1/30], Batch [1310/1438] Batch Loss: 0.3709, Training Accuracy: 0.9038, Validation Accuracy: 0.8981\n",
      "Epoch [1/30], Batch [1320/1438] Batch Loss: 0.4440, Training Accuracy: 0.9029, Validation Accuracy: 0.8995\n",
      "Epoch [1/30], Batch [1330/1438] Batch Loss: 0.2828, Training Accuracy: 0.9075, Validation Accuracy: 0.9037\n",
      "Epoch [1/30], Batch [1340/1438] Batch Loss: 1.0214, Training Accuracy: 0.9080, Validation Accuracy: 0.9044\n",
      "Epoch [1/30], Batch [1350/1438] Batch Loss: 0.3124, Training Accuracy: 0.8969, Validation Accuracy: 0.8941\n",
      "Epoch [1/30], Batch [1360/1438] Batch Loss: 0.3978, Training Accuracy: 0.9092, Validation Accuracy: 0.9030\n",
      "Epoch [1/30], Batch [1370/1438] Batch Loss: 0.2490, Training Accuracy: 0.9106, Validation Accuracy: 0.9062\n",
      "Epoch [1/30], Batch [1380/1438] Batch Loss: 0.4324, Training Accuracy: 0.9015, Validation Accuracy: 0.8988\n",
      "Epoch [1/30], Batch [1390/1438] Batch Loss: 0.2617, Training Accuracy: 0.9091, Validation Accuracy: 0.9061\n",
      "Epoch [1/30], Batch [1400/1438] Batch Loss: 0.3984, Training Accuracy: 0.9122, Validation Accuracy: 0.9073\n",
      "Epoch [1/30], Batch [1410/1438] Batch Loss: 0.3534, Training Accuracy: 0.9185, Validation Accuracy: 0.9137\n",
      "Epoch [1/30], Batch [1420/1438] Batch Loss: 0.2526, Training Accuracy: 0.9153, Validation Accuracy: 0.9116\n",
      "Epoch [1/30], Batch [1430/1438] Batch Loss: 0.3510, Training Accuracy: 0.9061, Validation Accuracy: 0.9040\n",
      "Epoch [1/30], Epoch Loss: 1157.4401\n",
      "Epoch [2/30], Batch [10/1438] Batch Loss: 0.4915, Training Accuracy: 0.9072, Validation Accuracy: 0.9067\n",
      "Epoch [2/30], Batch [20/1438] Batch Loss: 0.2676, Training Accuracy: 0.9100, Validation Accuracy: 0.9066\n",
      "Epoch [2/30], Batch [30/1438] Batch Loss: 0.3641, Training Accuracy: 0.9057, Validation Accuracy: 0.9040\n",
      "Epoch [2/30], Batch [40/1438] Batch Loss: 0.4008, Training Accuracy: 0.9100, Validation Accuracy: 0.9070\n",
      "Epoch [2/30], Batch [50/1438] Batch Loss: 0.5307, Training Accuracy: 0.9090, Validation Accuracy: 0.9062\n",
      "Epoch [2/30], Batch [60/1438] Batch Loss: 0.4292, Training Accuracy: 0.8963, Validation Accuracy: 0.8915\n",
      "Epoch [2/30], Batch [70/1438] Batch Loss: 0.2592, Training Accuracy: 0.9163, Validation Accuracy: 0.9119\n",
      "Epoch [2/30], Batch [80/1438] Batch Loss: 0.2427, Training Accuracy: 0.9157, Validation Accuracy: 0.9118\n",
      "Epoch [2/30], Batch [90/1438] Batch Loss: 0.1719, Training Accuracy: 0.9161, Validation Accuracy: 0.9109\n",
      "Epoch [2/30], Batch [100/1438] Batch Loss: 0.2583, Training Accuracy: 0.9185, Validation Accuracy: 0.9138\n",
      "Epoch [2/30], Batch [110/1438] Batch Loss: 0.3927, Training Accuracy: 0.9168, Validation Accuracy: 0.9113\n",
      "Epoch [2/30], Batch [120/1438] Batch Loss: 0.3433, Training Accuracy: 0.9020, Validation Accuracy: 0.8988\n",
      "Epoch [2/30], Batch [130/1438] Batch Loss: 0.2795, Training Accuracy: 0.9131, Validation Accuracy: 0.9100\n",
      "Epoch [2/30], Batch [140/1438] Batch Loss: 0.3348, Training Accuracy: 0.9173, Validation Accuracy: 0.9135\n",
      "Epoch [2/30], Batch [150/1438] Batch Loss: 0.3748, Training Accuracy: 0.9156, Validation Accuracy: 0.9119\n",
      "Epoch [2/30], Batch [160/1438] Batch Loss: 0.3684, Training Accuracy: 0.9182, Validation Accuracy: 0.9148\n",
      "Epoch [2/30], Batch [170/1438] Batch Loss: 0.2967, Training Accuracy: 0.9197, Validation Accuracy: 0.9175\n",
      "Epoch [2/30], Batch [180/1438] Batch Loss: 0.2888, Training Accuracy: 0.9141, Validation Accuracy: 0.9107\n",
      "Epoch [2/30], Batch [190/1438] Batch Loss: 0.2500, Training Accuracy: 0.9217, Validation Accuracy: 0.9180\n",
      "Epoch [2/30], Batch [200/1438] Batch Loss: 0.6130, Training Accuracy: 0.9059, Validation Accuracy: 0.9044\n",
      "Epoch [2/30], Batch [210/1438] Batch Loss: 0.2757, Training Accuracy: 0.9156, Validation Accuracy: 0.9114\n",
      "Epoch [2/30], Batch [220/1438] Batch Loss: 0.2022, Training Accuracy: 0.9104, Validation Accuracy: 0.9081\n",
      "Epoch [2/30], Batch [230/1438] Batch Loss: 0.3553, Training Accuracy: 0.9049, Validation Accuracy: 0.9006\n",
      "Epoch [2/30], Batch [240/1438] Batch Loss: 0.5281, Training Accuracy: 0.9065, Validation Accuracy: 0.9032\n",
      "Epoch [2/30], Batch [250/1438] Batch Loss: 0.3407, Training Accuracy: 0.9107, Validation Accuracy: 0.9080\n",
      "Epoch [2/30], Batch [260/1438] Batch Loss: 0.3758, Training Accuracy: 0.9121, Validation Accuracy: 0.9091\n",
      "Epoch [2/30], Batch [270/1438] Batch Loss: 0.2582, Training Accuracy: 0.9057, Validation Accuracy: 0.9018\n",
      "Epoch [2/30], Batch [280/1438] Batch Loss: 0.2283, Training Accuracy: 0.9126, Validation Accuracy: 0.9107\n",
      "Epoch [2/30], Batch [290/1438] Batch Loss: 0.3088, Training Accuracy: 0.9158, Validation Accuracy: 0.9114\n",
      "Epoch [2/30], Batch [300/1438] Batch Loss: 0.4496, Training Accuracy: 0.9155, Validation Accuracy: 0.9123\n",
      "Epoch [2/30], Batch [310/1438] Batch Loss: 0.1895, Training Accuracy: 0.9148, Validation Accuracy: 0.9098\n",
      "Epoch [2/30], Batch [320/1438] Batch Loss: 0.3248, Training Accuracy: 0.8876, Validation Accuracy: 0.8826\n",
      "Epoch [2/30], Batch [330/1438] Batch Loss: 0.2345, Training Accuracy: 0.9132, Validation Accuracy: 0.9097\n",
      "Epoch [2/30], Batch [340/1438] Batch Loss: 0.2934, Training Accuracy: 0.9111, Validation Accuracy: 0.9075\n",
      "Epoch [2/30], Batch [350/1438] Batch Loss: 0.4605, Training Accuracy: 0.9139, Validation Accuracy: 0.9081\n",
      "Epoch [2/30], Batch [360/1438] Batch Loss: 0.3035, Training Accuracy: 0.9114, Validation Accuracy: 0.9091\n",
      "Epoch [2/30], Batch [370/1438] Batch Loss: 0.5791, Training Accuracy: 0.9231, Validation Accuracy: 0.9166\n",
      "Epoch [2/30], Batch [380/1438] Batch Loss: 0.3422, Training Accuracy: 0.9216, Validation Accuracy: 0.9160\n",
      "Epoch [2/30], Batch [390/1438] Batch Loss: 0.4159, Training Accuracy: 0.9208, Validation Accuracy: 0.9162\n",
      "Epoch [2/30], Batch [400/1438] Batch Loss: 0.2871, Training Accuracy: 0.9189, Validation Accuracy: 0.9138\n",
      "Epoch [2/30], Batch [410/1438] Batch Loss: 0.4843, Training Accuracy: 0.9176, Validation Accuracy: 0.9121\n",
      "Epoch [2/30], Batch [420/1438] Batch Loss: 0.2882, Training Accuracy: 0.9150, Validation Accuracy: 0.9100\n",
      "Epoch [2/30], Batch [430/1438] Batch Loss: 0.2398, Training Accuracy: 0.9202, Validation Accuracy: 0.9150\n",
      "Epoch [2/30], Batch [440/1438] Batch Loss: 0.2442, Training Accuracy: 0.9231, Validation Accuracy: 0.9154\n",
      "Epoch [2/30], Batch [450/1438] Batch Loss: 0.4990, Training Accuracy: 0.9221, Validation Accuracy: 0.9155\n",
      "Epoch [2/30], Batch [460/1438] Batch Loss: 0.3124, Training Accuracy: 0.9180, Validation Accuracy: 0.9122\n",
      "Epoch [2/30], Batch [470/1438] Batch Loss: 0.5117, Training Accuracy: 0.9214, Validation Accuracy: 0.9148\n",
      "Epoch [2/30], Batch [480/1438] Batch Loss: 0.3718, Training Accuracy: 0.9088, Validation Accuracy: 0.9032\n",
      "Epoch [2/30], Batch [490/1438] Batch Loss: 0.2336, Training Accuracy: 0.9227, Validation Accuracy: 0.9154\n",
      "Epoch [2/30], Batch [500/1438] Batch Loss: 0.3422, Training Accuracy: 0.9192, Validation Accuracy: 0.9145\n",
      "Epoch [2/30], Batch [510/1438] Batch Loss: 0.2474, Training Accuracy: 0.9177, Validation Accuracy: 0.9118\n",
      "Epoch [2/30], Batch [520/1438] Batch Loss: 0.2269, Training Accuracy: 0.9233, Validation Accuracy: 0.9167\n",
      "Epoch [2/30], Batch [530/1438] Batch Loss: 0.3323, Training Accuracy: 0.9233, Validation Accuracy: 0.9173\n",
      "Epoch [2/30], Batch [540/1438] Batch Loss: 0.3422, Training Accuracy: 0.9202, Validation Accuracy: 0.9148\n",
      "Epoch [2/30], Batch [550/1438] Batch Loss: 0.2249, Training Accuracy: 0.9238, Validation Accuracy: 0.9201\n",
      "Epoch [2/30], Batch [560/1438] Batch Loss: 0.1461, Training Accuracy: 0.9294, Validation Accuracy: 0.9226\n",
      "Epoch [2/30], Batch [570/1438] Batch Loss: 0.2596, Training Accuracy: 0.9292, Validation Accuracy: 0.9240\n",
      "Epoch [2/30], Batch [580/1438] Batch Loss: 0.4349, Training Accuracy: 0.9243, Validation Accuracy: 0.9164\n",
      "Epoch [2/30], Batch [590/1438] Batch Loss: 0.2264, Training Accuracy: 0.9246, Validation Accuracy: 0.9164\n",
      "Epoch [2/30], Batch [600/1438] Batch Loss: 0.3238, Training Accuracy: 0.9275, Validation Accuracy: 0.9199\n",
      "Epoch [2/30], Batch [610/1438] Batch Loss: 0.3933, Training Accuracy: 0.9257, Validation Accuracy: 0.9194\n",
      "Epoch [2/30], Batch [620/1438] Batch Loss: 0.3892, Training Accuracy: 0.9253, Validation Accuracy: 0.9191\n",
      "Epoch [2/30], Batch [630/1438] Batch Loss: 0.3176, Training Accuracy: 0.9243, Validation Accuracy: 0.9194\n",
      "Epoch [2/30], Batch [640/1438] Batch Loss: 0.3616, Training Accuracy: 0.9201, Validation Accuracy: 0.9154\n",
      "Epoch [2/30], Batch [650/1438] Batch Loss: 0.2668, Training Accuracy: 0.9250, Validation Accuracy: 0.9210\n",
      "Epoch [2/30], Batch [660/1438] Batch Loss: 0.2902, Training Accuracy: 0.9266, Validation Accuracy: 0.9223\n",
      "Epoch [2/30], Batch [670/1438] Batch Loss: 0.2431, Training Accuracy: 0.9281, Validation Accuracy: 0.9236\n",
      "Epoch [2/30], Batch [680/1438] Batch Loss: 0.4180, Training Accuracy: 0.9281, Validation Accuracy: 0.9216\n",
      "Epoch [2/30], Batch [690/1438] Batch Loss: 0.2997, Training Accuracy: 0.9263, Validation Accuracy: 0.9219\n",
      "Epoch [2/30], Batch [700/1438] Batch Loss: 0.2434, Training Accuracy: 0.9268, Validation Accuracy: 0.9227\n",
      "Epoch [2/30], Batch [710/1438] Batch Loss: 0.2067, Training Accuracy: 0.9283, Validation Accuracy: 0.9231\n",
      "Epoch [2/30], Batch [720/1438] Batch Loss: 0.1616, Training Accuracy: 0.9269, Validation Accuracy: 0.9226\n",
      "Epoch [2/30], Batch [730/1438] Batch Loss: 0.2388, Training Accuracy: 0.9245, Validation Accuracy: 0.9193\n",
      "Epoch [2/30], Batch [740/1438] Batch Loss: 0.1690, Training Accuracy: 0.9253, Validation Accuracy: 0.9207\n",
      "Epoch [2/30], Batch [750/1438] Batch Loss: 0.1242, Training Accuracy: 0.9288, Validation Accuracy: 0.9228\n",
      "Epoch [2/30], Batch [760/1438] Batch Loss: 0.2451, Training Accuracy: 0.9228, Validation Accuracy: 0.9164\n",
      "Epoch [2/30], Batch [770/1438] Batch Loss: 0.3709, Training Accuracy: 0.9230, Validation Accuracy: 0.9182\n",
      "Epoch [2/30], Batch [780/1438] Batch Loss: 0.4018, Training Accuracy: 0.9247, Validation Accuracy: 0.9178\n",
      "Epoch [2/30], Batch [790/1438] Batch Loss: 0.3444, Training Accuracy: 0.9277, Validation Accuracy: 0.9246\n",
      "Epoch [2/30], Batch [800/1438] Batch Loss: 0.3737, Training Accuracy: 0.9254, Validation Accuracy: 0.9213\n",
      "Epoch [2/30], Batch [810/1438] Batch Loss: 0.3407, Training Accuracy: 0.9301, Validation Accuracy: 0.9235\n",
      "Epoch [2/30], Batch [820/1438] Batch Loss: 0.2498, Training Accuracy: 0.9273, Validation Accuracy: 0.9210\n",
      "Epoch [2/30], Batch [830/1438] Batch Loss: 0.2258, Training Accuracy: 0.9237, Validation Accuracy: 0.9173\n",
      "Epoch [2/30], Batch [840/1438] Batch Loss: 0.2062, Training Accuracy: 0.9247, Validation Accuracy: 0.9191\n",
      "Epoch [2/30], Batch [850/1438] Batch Loss: 0.2365, Training Accuracy: 0.9293, Validation Accuracy: 0.9234\n",
      "Epoch [2/30], Batch [860/1438] Batch Loss: 0.2035, Training Accuracy: 0.9266, Validation Accuracy: 0.9205\n",
      "Epoch [2/30], Batch [870/1438] Batch Loss: 0.1057, Training Accuracy: 0.9325, Validation Accuracy: 0.9267\n",
      "Epoch [2/30], Batch [880/1438] Batch Loss: 0.2228, Training Accuracy: 0.9342, Validation Accuracy: 0.9275\n",
      "Epoch [2/30], Batch [890/1438] Batch Loss: 0.4613, Training Accuracy: 0.9278, Validation Accuracy: 0.9217\n",
      "Epoch [2/30], Batch [900/1438] Batch Loss: 0.3218, Training Accuracy: 0.9279, Validation Accuracy: 0.9233\n",
      "Epoch [2/30], Batch [910/1438] Batch Loss: 0.5436, Training Accuracy: 0.9171, Validation Accuracy: 0.9150\n",
      "Epoch [2/30], Batch [920/1438] Batch Loss: 0.2165, Training Accuracy: 0.9139, Validation Accuracy: 0.9062\n",
      "Epoch [2/30], Batch [930/1438] Batch Loss: 0.1953, Training Accuracy: 0.9199, Validation Accuracy: 0.9121\n",
      "Epoch [2/30], Batch [940/1438] Batch Loss: 0.3673, Training Accuracy: 0.9277, Validation Accuracy: 0.9203\n",
      "Epoch [2/30], Batch [950/1438] Batch Loss: 0.3867, Training Accuracy: 0.9314, Validation Accuracy: 0.9251\n",
      "Epoch [2/30], Batch [960/1438] Batch Loss: 0.1619, Training Accuracy: 0.9295, Validation Accuracy: 0.9244\n",
      "Epoch [2/30], Batch [970/1438] Batch Loss: 0.3822, Training Accuracy: 0.9243, Validation Accuracy: 0.9193\n",
      "Epoch [2/30], Batch [980/1438] Batch Loss: 0.3490, Training Accuracy: 0.9263, Validation Accuracy: 0.9220\n",
      "Epoch [2/30], Batch [990/1438] Batch Loss: 0.4605, Training Accuracy: 0.9304, Validation Accuracy: 0.9247\n",
      "Epoch [2/30], Batch [1000/1438] Batch Loss: 0.0610, Training Accuracy: 0.9331, Validation Accuracy: 0.9252\n",
      "Epoch [2/30], Batch [1010/1438] Batch Loss: 0.2739, Training Accuracy: 0.9327, Validation Accuracy: 0.9243\n",
      "Epoch [2/30], Batch [1020/1438] Batch Loss: 0.3315, Training Accuracy: 0.9313, Validation Accuracy: 0.9238\n",
      "Epoch [2/30], Batch [1030/1438] Batch Loss: 0.3968, Training Accuracy: 0.9329, Validation Accuracy: 0.9268\n",
      "Epoch [2/30], Batch [1040/1438] Batch Loss: 0.2543, Training Accuracy: 0.9327, Validation Accuracy: 0.9274\n",
      "Epoch [2/30], Batch [1050/1438] Batch Loss: 0.1973, Training Accuracy: 0.9320, Validation Accuracy: 0.9281\n",
      "Epoch [2/30], Batch [1060/1438] Batch Loss: 0.1568, Training Accuracy: 0.9312, Validation Accuracy: 0.9255\n",
      "Epoch [2/30], Batch [1070/1438] Batch Loss: 0.2807, Training Accuracy: 0.9259, Validation Accuracy: 0.9187\n",
      "Epoch [2/30], Batch [1080/1438] Batch Loss: 0.3998, Training Accuracy: 0.9237, Validation Accuracy: 0.9191\n",
      "Epoch [2/30], Batch [1090/1438] Batch Loss: 0.3019, Training Accuracy: 0.9240, Validation Accuracy: 0.9181\n",
      "Epoch [2/30], Batch [1100/1438] Batch Loss: 0.2202, Training Accuracy: 0.9319, Validation Accuracy: 0.9269\n",
      "Epoch [2/30], Batch [1110/1438] Batch Loss: 0.2204, Training Accuracy: 0.9306, Validation Accuracy: 0.9262\n",
      "Epoch [2/30], Batch [1120/1438] Batch Loss: 0.2569, Training Accuracy: 0.9337, Validation Accuracy: 0.9299\n",
      "Epoch [2/30], Batch [1130/1438] Batch Loss: 0.2290, Training Accuracy: 0.9343, Validation Accuracy: 0.9292\n",
      "Epoch [2/30], Batch [1140/1438] Batch Loss: 0.3879, Training Accuracy: 0.9332, Validation Accuracy: 0.9271\n",
      "Epoch [2/30], Batch [1150/1438] Batch Loss: 0.3511, Training Accuracy: 0.9308, Validation Accuracy: 0.9249\n",
      "Epoch [2/30], Batch [1160/1438] Batch Loss: 0.2386, Training Accuracy: 0.9295, Validation Accuracy: 0.9259\n",
      "Epoch [2/30], Batch [1170/1438] Batch Loss: 0.2492, Training Accuracy: 0.9320, Validation Accuracy: 0.9283\n",
      "Epoch [2/30], Batch [1180/1438] Batch Loss: 0.3986, Training Accuracy: 0.9336, Validation Accuracy: 0.9295\n",
      "Epoch [2/30], Batch [1190/1438] Batch Loss: 0.2189, Training Accuracy: 0.9354, Validation Accuracy: 0.9316\n",
      "Epoch [2/30], Batch [1200/1438] Batch Loss: 0.2955, Training Accuracy: 0.9370, Validation Accuracy: 0.9313\n",
      "Epoch [2/30], Batch [1210/1438] Batch Loss: 0.1673, Training Accuracy: 0.9273, Validation Accuracy: 0.9207\n",
      "Epoch [2/30], Batch [1220/1438] Batch Loss: 0.3315, Training Accuracy: 0.9341, Validation Accuracy: 0.9288\n",
      "Epoch [2/30], Batch [1230/1438] Batch Loss: 0.2639, Training Accuracy: 0.9319, Validation Accuracy: 0.9276\n",
      "Epoch [2/30], Batch [1240/1438] Batch Loss: 0.3379, Training Accuracy: 0.9301, Validation Accuracy: 0.9248\n",
      "Epoch [2/30], Batch [1250/1438] Batch Loss: 0.3301, Training Accuracy: 0.9282, Validation Accuracy: 0.9238\n",
      "Epoch [2/30], Batch [1260/1438] Batch Loss: 0.2230, Training Accuracy: 0.9381, Validation Accuracy: 0.9334\n",
      "Epoch [2/30], Batch [1270/1438] Batch Loss: 0.2517, Training Accuracy: 0.9388, Validation Accuracy: 0.9331\n",
      "Epoch [2/30], Batch [1280/1438] Batch Loss: 0.2799, Training Accuracy: 0.9387, Validation Accuracy: 0.9336\n",
      "Epoch [2/30], Batch [1290/1438] Batch Loss: 0.1135, Training Accuracy: 0.9376, Validation Accuracy: 0.9317\n",
      "Epoch [2/30], Batch [1300/1438] Batch Loss: 0.2064, Training Accuracy: 0.9391, Validation Accuracy: 0.9328\n",
      "Epoch [2/30], Batch [1310/1438] Batch Loss: 0.1296, Training Accuracy: 0.9348, Validation Accuracy: 0.9297\n",
      "Epoch [2/30], Batch [1320/1438] Batch Loss: 0.1795, Training Accuracy: 0.9359, Validation Accuracy: 0.9306\n",
      "Epoch [2/30], Batch [1330/1438] Batch Loss: 0.4134, Training Accuracy: 0.9377, Validation Accuracy: 0.9317\n",
      "Epoch [2/30], Batch [1340/1438] Batch Loss: 0.2265, Training Accuracy: 0.9394, Validation Accuracy: 0.9333\n",
      "Epoch [2/30], Batch [1350/1438] Batch Loss: 0.2929, Training Accuracy: 0.9397, Validation Accuracy: 0.9332\n",
      "Epoch [2/30], Batch [1360/1438] Batch Loss: 0.0915, Training Accuracy: 0.9383, Validation Accuracy: 0.9327\n",
      "Epoch [2/30], Batch [1370/1438] Batch Loss: 0.2492, Training Accuracy: 0.9238, Validation Accuracy: 0.9199\n",
      "Epoch [2/30], Batch [1380/1438] Batch Loss: 0.2803, Training Accuracy: 0.9340, Validation Accuracy: 0.9280\n",
      "Epoch [2/30], Batch [1390/1438] Batch Loss: 0.2181, Training Accuracy: 0.9420, Validation Accuracy: 0.9362\n",
      "Epoch [2/30], Batch [1400/1438] Batch Loss: 0.3376, Training Accuracy: 0.9409, Validation Accuracy: 0.9340\n",
      "Epoch [2/30], Batch [1410/1438] Batch Loss: 0.2204, Training Accuracy: 0.9362, Validation Accuracy: 0.9298\n",
      "Epoch [2/30], Batch [1420/1438] Batch Loss: 0.2736, Training Accuracy: 0.9342, Validation Accuracy: 0.9267\n",
      "Epoch [2/30], Batch [1430/1438] Batch Loss: 0.1887, Training Accuracy: 0.9380, Validation Accuracy: 0.9330\n",
      "Epoch [2/30], Epoch Loss: 439.5870\n",
      "Epoch [3/30], Batch [10/1438] Batch Loss: 0.1774, Training Accuracy: 0.9355, Validation Accuracy: 0.9284\n",
      "Epoch [3/30], Batch [20/1438] Batch Loss: 0.3230, Training Accuracy: 0.9348, Validation Accuracy: 0.9292\n",
      "Epoch [3/30], Batch [30/1438] Batch Loss: 0.1876, Training Accuracy: 0.9353, Validation Accuracy: 0.9292\n",
      "Epoch [3/30], Batch [40/1438] Batch Loss: 0.1813, Training Accuracy: 0.9286, Validation Accuracy: 0.9226\n",
      "Epoch [3/30], Batch [50/1438] Batch Loss: 0.2463, Training Accuracy: 0.9344, Validation Accuracy: 0.9274\n",
      "Epoch [3/30], Batch [60/1438] Batch Loss: 0.1473, Training Accuracy: 0.9369, Validation Accuracy: 0.9309\n",
      "Epoch [3/30], Batch [70/1438] Batch Loss: 0.4094, Training Accuracy: 0.9412, Validation Accuracy: 0.9343\n",
      "Epoch [3/30], Batch [80/1438] Batch Loss: 0.2697, Training Accuracy: 0.9405, Validation Accuracy: 0.9349\n",
      "Epoch [3/30], Batch [90/1438] Batch Loss: 0.2285, Training Accuracy: 0.9409, Validation Accuracy: 0.9339\n",
      "Epoch [3/30], Batch [100/1438] Batch Loss: 0.4295, Training Accuracy: 0.9363, Validation Accuracy: 0.9299\n",
      "Epoch [3/30], Batch [110/1438] Batch Loss: 0.2506, Training Accuracy: 0.9357, Validation Accuracy: 0.9272\n",
      "Epoch [3/30], Batch [120/1438] Batch Loss: 0.2709, Training Accuracy: 0.9407, Validation Accuracy: 0.9341\n",
      "Epoch [3/30], Batch [130/1438] Batch Loss: 0.0938, Training Accuracy: 0.9401, Validation Accuracy: 0.9328\n",
      "Epoch [3/30], Batch [140/1438] Batch Loss: 0.2681, Training Accuracy: 0.9373, Validation Accuracy: 0.9297\n",
      "Epoch [3/30], Batch [150/1438] Batch Loss: 0.2506, Training Accuracy: 0.9369, Validation Accuracy: 0.9330\n",
      "Epoch [3/30], Batch [160/1438] Batch Loss: 0.2141, Training Accuracy: 0.9379, Validation Accuracy: 0.9325\n",
      "Epoch [3/30], Batch [170/1438] Batch Loss: 0.2200, Training Accuracy: 0.9384, Validation Accuracy: 0.9320\n",
      "Epoch [3/30], Batch [180/1438] Batch Loss: 0.2692, Training Accuracy: 0.9360, Validation Accuracy: 0.9313\n",
      "Epoch [3/30], Batch [190/1438] Batch Loss: 0.1938, Training Accuracy: 0.9357, Validation Accuracy: 0.9270\n",
      "Epoch [3/30], Batch [200/1438] Batch Loss: 0.2068, Training Accuracy: 0.9406, Validation Accuracy: 0.9326\n",
      "Epoch [3/30], Batch [210/1438] Batch Loss: 0.2390, Training Accuracy: 0.9359, Validation Accuracy: 0.9296\n",
      "Epoch [3/30], Batch [220/1438] Batch Loss: 0.2939, Training Accuracy: 0.9352, Validation Accuracy: 0.9287\n",
      "Epoch [3/30], Batch [230/1438] Batch Loss: 0.2959, Training Accuracy: 0.9356, Validation Accuracy: 0.9299\n",
      "Epoch [3/30], Batch [240/1438] Batch Loss: 0.1925, Training Accuracy: 0.9316, Validation Accuracy: 0.9223\n",
      "Epoch [3/30], Batch [250/1438] Batch Loss: 0.2416, Training Accuracy: 0.9363, Validation Accuracy: 0.9284\n",
      "Epoch [3/30], Batch [260/1438] Batch Loss: 0.1902, Training Accuracy: 0.9270, Validation Accuracy: 0.9180\n",
      "Epoch [3/30], Batch [270/1438] Batch Loss: 0.2668, Training Accuracy: 0.9424, Validation Accuracy: 0.9343\n",
      "Epoch [3/30], Batch [280/1438] Batch Loss: 0.1451, Training Accuracy: 0.9394, Validation Accuracy: 0.9306\n",
      "Epoch [3/30], Batch [290/1438] Batch Loss: 0.1877, Training Accuracy: 0.9398, Validation Accuracy: 0.9311\n",
      "Epoch [3/30], Batch [300/1438] Batch Loss: 0.1306, Training Accuracy: 0.9403, Validation Accuracy: 0.9314\n",
      "Epoch [3/30], Batch [310/1438] Batch Loss: 0.3043, Training Accuracy: 0.9426, Validation Accuracy: 0.9356\n",
      "Epoch [3/30], Batch [320/1438] Batch Loss: 0.4185, Training Accuracy: 0.9391, Validation Accuracy: 0.9333\n",
      "Epoch [3/30], Batch [330/1438] Batch Loss: 0.2125, Training Accuracy: 0.9417, Validation Accuracy: 0.9377\n",
      "Epoch [3/30], Batch [340/1438] Batch Loss: 0.4434, Training Accuracy: 0.9407, Validation Accuracy: 0.9350\n",
      "Epoch [3/30], Batch [350/1438] Batch Loss: 0.0846, Training Accuracy: 0.9415, Validation Accuracy: 0.9326\n",
      "Epoch [3/30], Batch [360/1438] Batch Loss: 0.1949, Training Accuracy: 0.9435, Validation Accuracy: 0.9354\n",
      "Epoch [3/30], Batch [370/1438] Batch Loss: 0.1458, Training Accuracy: 0.9446, Validation Accuracy: 0.9386\n",
      "Epoch [3/30], Batch [380/1438] Batch Loss: 0.2594, Training Accuracy: 0.9450, Validation Accuracy: 0.9377\n",
      "Epoch [3/30], Batch [390/1438] Batch Loss: 0.3592, Training Accuracy: 0.9437, Validation Accuracy: 0.9360\n",
      "Epoch [3/30], Batch [400/1438] Batch Loss: 0.1935, Training Accuracy: 0.9446, Validation Accuracy: 0.9380\n",
      "Epoch [3/30], Batch [410/1438] Batch Loss: 0.0740, Training Accuracy: 0.9430, Validation Accuracy: 0.9359\n",
      "Epoch [3/30], Batch [420/1438] Batch Loss: 0.3493, Training Accuracy: 0.9420, Validation Accuracy: 0.9349\n",
      "Epoch [3/30], Batch [430/1438] Batch Loss: 0.1132, Training Accuracy: 0.9415, Validation Accuracy: 0.9336\n",
      "Epoch [3/30], Batch [440/1438] Batch Loss: 0.1136, Training Accuracy: 0.9440, Validation Accuracy: 0.9363\n",
      "Epoch [3/30], Batch [450/1438] Batch Loss: 0.3172, Training Accuracy: 0.9417, Validation Accuracy: 0.9345\n",
      "Epoch [3/30], Batch [460/1438] Batch Loss: 0.1987, Training Accuracy: 0.9408, Validation Accuracy: 0.9324\n",
      "Epoch [3/30], Batch [470/1438] Batch Loss: 0.1042, Training Accuracy: 0.9307, Validation Accuracy: 0.9247\n",
      "Epoch [3/30], Batch [480/1438] Batch Loss: 0.2072, Training Accuracy: 0.9382, Validation Accuracy: 0.9319\n",
      "Epoch [3/30], Batch [490/1438] Batch Loss: 0.1625, Training Accuracy: 0.9393, Validation Accuracy: 0.9343\n",
      "Epoch [3/30], Batch [500/1438] Batch Loss: 0.1452, Training Accuracy: 0.9255, Validation Accuracy: 0.9187\n",
      "Epoch [3/30], Batch [510/1438] Batch Loss: 0.2471, Training Accuracy: 0.9424, Validation Accuracy: 0.9349\n",
      "Epoch [3/30], Batch [520/1438] Batch Loss: 0.1774, Training Accuracy: 0.9404, Validation Accuracy: 0.9328\n",
      "Epoch [3/30], Batch [530/1438] Batch Loss: 0.3422, Training Accuracy: 0.9413, Validation Accuracy: 0.9365\n",
      "Epoch [3/30], Batch [540/1438] Batch Loss: 0.2674, Training Accuracy: 0.9417, Validation Accuracy: 0.9364\n",
      "Epoch [3/30], Batch [550/1438] Batch Loss: 0.1857, Training Accuracy: 0.9416, Validation Accuracy: 0.9361\n",
      "Epoch [3/30], Batch [560/1438] Batch Loss: 0.1510, Training Accuracy: 0.9393, Validation Accuracy: 0.9338\n",
      "Epoch [3/30], Batch [570/1438] Batch Loss: 0.2795, Training Accuracy: 0.9386, Validation Accuracy: 0.9333\n",
      "Epoch [3/30], Batch [580/1438] Batch Loss: 0.3539, Training Accuracy: 0.9385, Validation Accuracy: 0.9319\n",
      "Epoch [3/30], Batch [590/1438] Batch Loss: 0.3795, Training Accuracy: 0.9413, Validation Accuracy: 0.9351\n",
      "Epoch [3/30], Batch [600/1438] Batch Loss: 0.2753, Training Accuracy: 0.9388, Validation Accuracy: 0.9320\n",
      "Epoch [3/30], Batch [610/1438] Batch Loss: 0.4366, Training Accuracy: 0.9399, Validation Accuracy: 0.9333\n",
      "Epoch [3/30], Batch [620/1438] Batch Loss: 0.3287, Training Accuracy: 0.9408, Validation Accuracy: 0.9326\n",
      "Epoch [3/30], Batch [630/1438] Batch Loss: 0.1471, Training Accuracy: 0.9352, Validation Accuracy: 0.9275\n",
      "Epoch [3/30], Batch [640/1438] Batch Loss: 0.5030, Training Accuracy: 0.9319, Validation Accuracy: 0.9254\n",
      "Epoch [3/30], Batch [650/1438] Batch Loss: 0.1167, Training Accuracy: 0.9317, Validation Accuracy: 0.9251\n",
      "Epoch [3/30], Batch [660/1438] Batch Loss: 0.1440, Training Accuracy: 0.9344, Validation Accuracy: 0.9297\n",
      "Epoch [3/30], Batch [670/1438] Batch Loss: 0.3553, Training Accuracy: 0.9346, Validation Accuracy: 0.9302\n",
      "Epoch [3/30], Batch [680/1438] Batch Loss: 0.1858, Training Accuracy: 0.9380, Validation Accuracy: 0.9313\n",
      "Epoch [3/30], Batch [690/1438] Batch Loss: 0.3221, Training Accuracy: 0.9399, Validation Accuracy: 0.9317\n",
      "Epoch [3/30], Batch [700/1438] Batch Loss: 0.2013, Training Accuracy: 0.9408, Validation Accuracy: 0.9327\n",
      "Epoch [3/30], Batch [710/1438] Batch Loss: 0.3377, Training Accuracy: 0.9394, Validation Accuracy: 0.9300\n",
      "Epoch [3/30], Batch [720/1438] Batch Loss: 0.2915, Training Accuracy: 0.9395, Validation Accuracy: 0.9320\n",
      "Epoch [3/30], Batch [730/1438] Batch Loss: 0.1672, Training Accuracy: 0.9335, Validation Accuracy: 0.9256\n",
      "Epoch [3/30], Batch [740/1438] Batch Loss: 0.2747, Training Accuracy: 0.9412, Validation Accuracy: 0.9346\n",
      "Epoch [3/30], Batch [750/1438] Batch Loss: 0.1837, Training Accuracy: 0.9394, Validation Accuracy: 0.9354\n",
      "Epoch [3/30], Batch [760/1438] Batch Loss: 0.2650, Training Accuracy: 0.9414, Validation Accuracy: 0.9381\n",
      "Epoch [3/30], Batch [770/1438] Batch Loss: 0.0545, Training Accuracy: 0.9390, Validation Accuracy: 0.9361\n",
      "Epoch [3/30], Batch [780/1438] Batch Loss: 0.2393, Training Accuracy: 0.9407, Validation Accuracy: 0.9343\n",
      "Epoch [3/30], Batch [790/1438] Batch Loss: 0.0292, Training Accuracy: 0.9425, Validation Accuracy: 0.9377\n",
      "Epoch [3/30], Batch [800/1438] Batch Loss: 0.1477, Training Accuracy: 0.9445, Validation Accuracy: 0.9392\n",
      "Epoch [3/30], Batch [810/1438] Batch Loss: 0.2898, Training Accuracy: 0.9437, Validation Accuracy: 0.9394\n",
      "Epoch [3/30], Batch [820/1438] Batch Loss: 0.2531, Training Accuracy: 0.9431, Validation Accuracy: 0.9365\n",
      "Epoch [3/30], Batch [830/1438] Batch Loss: 0.1177, Training Accuracy: 0.9451, Validation Accuracy: 0.9377\n",
      "Epoch [3/30], Batch [840/1438] Batch Loss: 0.1685, Training Accuracy: 0.9461, Validation Accuracy: 0.9398\n",
      "Epoch [3/30], Batch [850/1438] Batch Loss: 0.3021, Training Accuracy: 0.9447, Validation Accuracy: 0.9388\n",
      "Epoch [3/30], Batch [860/1438] Batch Loss: 0.1560, Training Accuracy: 0.9431, Validation Accuracy: 0.9365\n",
      "Epoch [3/30], Batch [870/1438] Batch Loss: 0.1154, Training Accuracy: 0.9433, Validation Accuracy: 0.9358\n",
      "Epoch [3/30], Batch [880/1438] Batch Loss: 0.1667, Training Accuracy: 0.9432, Validation Accuracy: 0.9368\n",
      "Epoch [3/30], Batch [890/1438] Batch Loss: 0.1325, Training Accuracy: 0.9458, Validation Accuracy: 0.9379\n",
      "Epoch [3/30], Batch [900/1438] Batch Loss: 0.3214, Training Accuracy: 0.9419, Validation Accuracy: 0.9336\n",
      "Epoch [3/30], Batch [910/1438] Batch Loss: 0.1554, Training Accuracy: 0.9414, Validation Accuracy: 0.9343\n",
      "Epoch [3/30], Batch [920/1438] Batch Loss: 0.2811, Training Accuracy: 0.9427, Validation Accuracy: 0.9377\n",
      "Epoch [3/30], Batch [930/1438] Batch Loss: 0.2500, Training Accuracy: 0.9452, Validation Accuracy: 0.9376\n",
      "Epoch [3/30], Batch [940/1438] Batch Loss: 0.1980, Training Accuracy: 0.9421, Validation Accuracy: 0.9352\n",
      "Epoch [3/30], Batch [950/1438] Batch Loss: 0.2482, Training Accuracy: 0.9437, Validation Accuracy: 0.9377\n",
      "Epoch [3/30], Batch [960/1438] Batch Loss: 0.0947, Training Accuracy: 0.9457, Validation Accuracy: 0.9369\n",
      "Epoch [3/30], Batch [970/1438] Batch Loss: 0.0932, Training Accuracy: 0.9441, Validation Accuracy: 0.9381\n",
      "Epoch [3/30], Batch [980/1438] Batch Loss: 0.1585, Training Accuracy: 0.9450, Validation Accuracy: 0.9395\n",
      "Epoch [3/30], Batch [990/1438] Batch Loss: 0.2268, Training Accuracy: 0.9430, Validation Accuracy: 0.9364\n",
      "Epoch [3/30], Batch [1000/1438] Batch Loss: 0.1117, Training Accuracy: 0.9440, Validation Accuracy: 0.9388\n",
      "Epoch [3/30], Batch [1010/1438] Batch Loss: 0.1374, Training Accuracy: 0.9404, Validation Accuracy: 0.9348\n",
      "Epoch [3/30], Batch [1020/1438] Batch Loss: 0.2041, Training Accuracy: 0.9325, Validation Accuracy: 0.9263\n",
      "Epoch [3/30], Batch [1030/1438] Batch Loss: 0.2454, Training Accuracy: 0.9434, Validation Accuracy: 0.9357\n",
      "Epoch [3/30], Batch [1040/1438] Batch Loss: 0.0739, Training Accuracy: 0.9443, Validation Accuracy: 0.9370\n",
      "Epoch [3/30], Batch [1050/1438] Batch Loss: 0.3008, Training Accuracy: 0.9433, Validation Accuracy: 0.9365\n",
      "Epoch [3/30], Batch [1060/1438] Batch Loss: 0.1476, Training Accuracy: 0.9427, Validation Accuracy: 0.9343\n",
      "Epoch [3/30], Batch [1070/1438] Batch Loss: 0.1269, Training Accuracy: 0.9423, Validation Accuracy: 0.9334\n",
      "Epoch [3/30], Batch [1080/1438] Batch Loss: 0.1699, Training Accuracy: 0.9400, Validation Accuracy: 0.9314\n",
      "Epoch [3/30], Batch [1090/1438] Batch Loss: 0.3780, Training Accuracy: 0.9411, Validation Accuracy: 0.9350\n",
      "Epoch [3/30], Batch [1100/1438] Batch Loss: 0.1212, Training Accuracy: 0.9363, Validation Accuracy: 0.9318\n",
      "Epoch [3/30], Batch [1110/1438] Batch Loss: 0.1840, Training Accuracy: 0.9398, Validation Accuracy: 0.9318\n",
      "Epoch [3/30], Batch [1120/1438] Batch Loss: 0.1554, Training Accuracy: 0.9446, Validation Accuracy: 0.9352\n",
      "Epoch [3/30], Batch [1130/1438] Batch Loss: 0.1680, Training Accuracy: 0.9448, Validation Accuracy: 0.9369\n",
      "Epoch [3/30], Batch [1140/1438] Batch Loss: 0.1961, Training Accuracy: 0.9432, Validation Accuracy: 0.9368\n",
      "Epoch [3/30], Batch [1150/1438] Batch Loss: 0.2001, Training Accuracy: 0.9433, Validation Accuracy: 0.9368\n",
      "Epoch [3/30], Batch [1160/1438] Batch Loss: 0.1770, Training Accuracy: 0.9439, Validation Accuracy: 0.9359\n",
      "Epoch [3/30], Batch [1170/1438] Batch Loss: 0.3310, Training Accuracy: 0.9440, Validation Accuracy: 0.9373\n",
      "Epoch [3/30], Batch [1180/1438] Batch Loss: 0.1326, Training Accuracy: 0.9444, Validation Accuracy: 0.9376\n",
      "Epoch [3/30], Batch [1190/1438] Batch Loss: 0.1637, Training Accuracy: 0.9451, Validation Accuracy: 0.9374\n",
      "Epoch [3/30], Batch [1200/1438] Batch Loss: 0.2501, Training Accuracy: 0.9406, Validation Accuracy: 0.9341\n",
      "Epoch [3/30], Batch [1210/1438] Batch Loss: 0.1450, Training Accuracy: 0.9447, Validation Accuracy: 0.9372\n",
      "Epoch [3/30], Batch [1220/1438] Batch Loss: 0.2762, Training Accuracy: 0.9445, Validation Accuracy: 0.9354\n",
      "Epoch [3/30], Batch [1230/1438] Batch Loss: 0.1275, Training Accuracy: 0.9448, Validation Accuracy: 0.9372\n",
      "Epoch [3/30], Batch [1240/1438] Batch Loss: 0.1418, Training Accuracy: 0.9419, Validation Accuracy: 0.9318\n",
      "Epoch [3/30], Batch [1250/1438] Batch Loss: 0.4146, Training Accuracy: 0.9411, Validation Accuracy: 0.9311\n",
      "Epoch [3/30], Batch [1260/1438] Batch Loss: 0.1949, Training Accuracy: 0.9433, Validation Accuracy: 0.9356\n",
      "Epoch [3/30], Batch [1270/1438] Batch Loss: 0.1179, Training Accuracy: 0.9460, Validation Accuracy: 0.9391\n",
      "Epoch [3/30], Batch [1280/1438] Batch Loss: 0.2206, Training Accuracy: 0.9464, Validation Accuracy: 0.9394\n",
      "Epoch [3/30], Batch [1290/1438] Batch Loss: 0.1029, Training Accuracy: 0.9442, Validation Accuracy: 0.9354\n",
      "Epoch [3/30], Batch [1300/1438] Batch Loss: 0.1632, Training Accuracy: 0.9453, Validation Accuracy: 0.9370\n",
      "Epoch [3/30], Batch [1310/1438] Batch Loss: 0.1867, Training Accuracy: 0.9456, Validation Accuracy: 0.9378\n",
      "Epoch [3/30], Batch [1320/1438] Batch Loss: 0.2087, Training Accuracy: 0.9440, Validation Accuracy: 0.9340\n",
      "Epoch [3/30], Batch [1330/1438] Batch Loss: 0.3500, Training Accuracy: 0.9301, Validation Accuracy: 0.9224\n",
      "Epoch [3/30], Batch [1340/1438] Batch Loss: 0.1549, Training Accuracy: 0.9432, Validation Accuracy: 0.9356\n",
      "Epoch [3/30], Batch [1350/1438] Batch Loss: 0.2176, Training Accuracy: 0.9457, Validation Accuracy: 0.9375\n",
      "Epoch [3/30], Batch [1360/1438] Batch Loss: 0.3361, Training Accuracy: 0.9453, Validation Accuracy: 0.9373\n",
      "Epoch [3/30], Batch [1370/1438] Batch Loss: 0.2241, Training Accuracy: 0.9412, Validation Accuracy: 0.9314\n",
      "Epoch [3/30], Batch [1380/1438] Batch Loss: 0.1844, Training Accuracy: 0.9351, Validation Accuracy: 0.9277\n",
      "Epoch [3/30], Batch [1390/1438] Batch Loss: 0.2239, Training Accuracy: 0.9456, Validation Accuracy: 0.9363\n",
      "Epoch [3/30], Batch [1400/1438] Batch Loss: 0.2160, Training Accuracy: 0.9472, Validation Accuracy: 0.9379\n",
      "Epoch [3/30], Batch [1410/1438] Batch Loss: 0.1800, Training Accuracy: 0.9475, Validation Accuracy: 0.9384\n",
      "Epoch [3/30], Batch [1420/1438] Batch Loss: 0.1378, Training Accuracy: 0.9398, Validation Accuracy: 0.9304\n",
      "Epoch [3/30], Batch [1430/1438] Batch Loss: 0.1241, Training Accuracy: 0.9462, Validation Accuracy: 0.9375\n",
      "Epoch [3/30], Epoch Loss: 311.9472\n",
      "Epoch [4/30], Batch [10/1438] Batch Loss: 0.1262, Training Accuracy: 0.9458, Validation Accuracy: 0.9377\n",
      "Epoch [4/30], Batch [20/1438] Batch Loss: 0.0932, Training Accuracy: 0.9444, Validation Accuracy: 0.9365\n",
      "Epoch [4/30], Batch [30/1438] Batch Loss: 0.1614, Training Accuracy: 0.9445, Validation Accuracy: 0.9369\n",
      "Epoch [4/30], Batch [40/1438] Batch Loss: 0.1080, Training Accuracy: 0.9455, Validation Accuracy: 0.9371\n",
      "Epoch [4/30], Batch [50/1438] Batch Loss: 0.1262, Training Accuracy: 0.9425, Validation Accuracy: 0.9339\n",
      "Epoch [4/30], Batch [60/1438] Batch Loss: 0.1314, Training Accuracy: 0.9418, Validation Accuracy: 0.9317\n",
      "Epoch [4/30], Batch [70/1438] Batch Loss: 0.1423, Training Accuracy: 0.9464, Validation Accuracy: 0.9398\n",
      "Epoch [4/30], Batch [80/1438] Batch Loss: 0.1412, Training Accuracy: 0.9446, Validation Accuracy: 0.9390\n",
      "Epoch [4/30], Batch [90/1438] Batch Loss: 0.2659, Training Accuracy: 0.9456, Validation Accuracy: 0.9397\n",
      "Epoch [4/30], Batch [100/1438] Batch Loss: 0.1289, Training Accuracy: 0.9471, Validation Accuracy: 0.9403\n",
      "Epoch [4/30], Batch [110/1438] Batch Loss: 0.1782, Training Accuracy: 0.9469, Validation Accuracy: 0.9395\n",
      "Epoch [4/30], Batch [120/1438] Batch Loss: 0.1317, Training Accuracy: 0.9480, Validation Accuracy: 0.9390\n",
      "Epoch [4/30], Batch [130/1438] Batch Loss: 0.0926, Training Accuracy: 0.9483, Validation Accuracy: 0.9418\n",
      "Epoch [4/30], Batch [140/1438] Batch Loss: 0.1450, Training Accuracy: 0.9467, Validation Accuracy: 0.9388\n",
      "Epoch [4/30], Batch [150/1438] Batch Loss: 0.0957, Training Accuracy: 0.9472, Validation Accuracy: 0.9409\n",
      "Epoch [4/30], Batch [160/1438] Batch Loss: 0.1169, Training Accuracy: 0.9484, Validation Accuracy: 0.9398\n",
      "Epoch [4/30], Batch [170/1438] Batch Loss: 0.1386, Training Accuracy: 0.9434, Validation Accuracy: 0.9343\n",
      "Epoch [4/30], Batch [180/1438] Batch Loss: 0.3283, Training Accuracy: 0.9430, Validation Accuracy: 0.9352\n",
      "Epoch [4/30], Batch [190/1438] Batch Loss: 0.1928, Training Accuracy: 0.9451, Validation Accuracy: 0.9372\n",
      "Epoch [4/30], Batch [200/1438] Batch Loss: 0.1749, Training Accuracy: 0.9467, Validation Accuracy: 0.9381\n",
      "Epoch [4/30], Batch [210/1438] Batch Loss: 0.2047, Training Accuracy: 0.9389, Validation Accuracy: 0.9293\n",
      "Epoch [4/30], Batch [220/1438] Batch Loss: 0.1617, Training Accuracy: 0.9480, Validation Accuracy: 0.9379\n",
      "Epoch [4/30], Batch [230/1438] Batch Loss: 0.0992, Training Accuracy: 0.9479, Validation Accuracy: 0.9396\n",
      "Epoch [4/30], Batch [240/1438] Batch Loss: 0.1909, Training Accuracy: 0.9484, Validation Accuracy: 0.9409\n",
      "Epoch [4/30], Batch [250/1438] Batch Loss: 0.1518, Training Accuracy: 0.9483, Validation Accuracy: 0.9398\n",
      "Epoch [4/30], Batch [260/1438] Batch Loss: 0.1754, Training Accuracy: 0.9463, Validation Accuracy: 0.9375\n",
      "Epoch [4/30], Batch [270/1438] Batch Loss: 0.1030, Training Accuracy: 0.9433, Validation Accuracy: 0.9328\n",
      "Epoch [4/30], Batch [280/1438] Batch Loss: 0.1468, Training Accuracy: 0.9432, Validation Accuracy: 0.9337\n",
      "Epoch [4/30], Batch [290/1438] Batch Loss: 0.2772, Training Accuracy: 0.9448, Validation Accuracy: 0.9362\n",
      "Epoch [4/30], Batch [300/1438] Batch Loss: 0.2140, Training Accuracy: 0.9453, Validation Accuracy: 0.9363\n",
      "Epoch [4/30], Batch [310/1438] Batch Loss: 0.1655, Training Accuracy: 0.9449, Validation Accuracy: 0.9365\n",
      "Epoch [4/30], Batch [320/1438] Batch Loss: 0.2484, Training Accuracy: 0.9367, Validation Accuracy: 0.9276\n",
      "Epoch [4/30], Batch [330/1438] Batch Loss: 0.3428, Training Accuracy: 0.9424, Validation Accuracy: 0.9347\n",
      "Epoch [4/30], Batch [340/1438] Batch Loss: 0.0766, Training Accuracy: 0.9483, Validation Accuracy: 0.9381\n",
      "Epoch [4/30], Batch [350/1438] Batch Loss: 0.1107, Training Accuracy: 0.9472, Validation Accuracy: 0.9379\n",
      "Epoch [4/30], Batch [360/1438] Batch Loss: 0.1352, Training Accuracy: 0.9480, Validation Accuracy: 0.9393\n",
      "Epoch [4/30], Batch [370/1438] Batch Loss: 0.2141, Training Accuracy: 0.9478, Validation Accuracy: 0.9391\n",
      "Epoch [4/30], Batch [380/1438] Batch Loss: 0.0893, Training Accuracy: 0.9446, Validation Accuracy: 0.9359\n",
      "Epoch [4/30], Batch [390/1438] Batch Loss: 0.3129, Training Accuracy: 0.9449, Validation Accuracy: 0.9354\n",
      "Epoch [4/30], Batch [400/1438] Batch Loss: 0.2394, Training Accuracy: 0.9472, Validation Accuracy: 0.9395\n",
      "Epoch [4/30], Batch [410/1438] Batch Loss: 0.1876, Training Accuracy: 0.9465, Validation Accuracy: 0.9395\n",
      "Epoch [4/30], Batch [420/1438] Batch Loss: 0.1350, Training Accuracy: 0.9477, Validation Accuracy: 0.9388\n",
      "Epoch [4/30], Batch [430/1438] Batch Loss: 0.1363, Training Accuracy: 0.9433, Validation Accuracy: 0.9360\n",
      "Epoch [4/30], Batch [440/1438] Batch Loss: 0.0580, Training Accuracy: 0.9446, Validation Accuracy: 0.9370\n",
      "Epoch [4/30], Batch [450/1438] Batch Loss: 0.1329, Training Accuracy: 0.9469, Validation Accuracy: 0.9394\n",
      "Epoch [4/30], Batch [460/1438] Batch Loss: 0.2195, Training Accuracy: 0.9436, Validation Accuracy: 0.9359\n",
      "Epoch [4/30], Batch [470/1438] Batch Loss: 0.1700, Training Accuracy: 0.9449, Validation Accuracy: 0.9363\n",
      "Epoch [4/30], Batch [480/1438] Batch Loss: 0.1459, Training Accuracy: 0.9471, Validation Accuracy: 0.9370\n",
      "Epoch [4/30], Batch [490/1438] Batch Loss: 0.2866, Training Accuracy: 0.9448, Validation Accuracy: 0.9365\n",
      "Epoch [4/30], Batch [500/1438] Batch Loss: 0.2267, Training Accuracy: 0.9475, Validation Accuracy: 0.9395\n",
      "Epoch [4/30], Batch [510/1438] Batch Loss: 0.1758, Training Accuracy: 0.9447, Validation Accuracy: 0.9375\n",
      "Epoch [4/30], Batch [520/1438] Batch Loss: 0.0912, Training Accuracy: 0.9430, Validation Accuracy: 0.9356\n",
      "Epoch [4/30], Batch [530/1438] Batch Loss: 0.1491, Training Accuracy: 0.9442, Validation Accuracy: 0.9356\n",
      "Epoch [4/30], Batch [540/1438] Batch Loss: 0.2282, Training Accuracy: 0.9475, Validation Accuracy: 0.9390\n",
      "Epoch [4/30], Batch [550/1438] Batch Loss: 0.1646, Training Accuracy: 0.9476, Validation Accuracy: 0.9391\n",
      "Epoch [4/30], Batch [560/1438] Batch Loss: 0.2325, Training Accuracy: 0.9461, Validation Accuracy: 0.9379\n",
      "Epoch [4/30], Batch [570/1438] Batch Loss: 0.2849, Training Accuracy: 0.9456, Validation Accuracy: 0.9379\n",
      "Epoch [4/30], Batch [580/1438] Batch Loss: 0.2444, Training Accuracy: 0.9474, Validation Accuracy: 0.9391\n",
      "Epoch [4/30], Batch [590/1438] Batch Loss: 0.2938, Training Accuracy: 0.9468, Validation Accuracy: 0.9387\n",
      "Epoch [4/30], Batch [600/1438] Batch Loss: 0.3693, Training Accuracy: 0.9485, Validation Accuracy: 0.9393\n",
      "Epoch [4/30], Batch [610/1438] Batch Loss: 0.1279, Training Accuracy: 0.9437, Validation Accuracy: 0.9353\n",
      "Epoch [4/30], Batch [620/1438] Batch Loss: 0.0436, Training Accuracy: 0.9405, Validation Accuracy: 0.9310\n",
      "Epoch [4/30], Batch [630/1438] Batch Loss: 0.1421, Training Accuracy: 0.9474, Validation Accuracy: 0.9389\n",
      "Epoch [4/30], Batch [640/1438] Batch Loss: 0.1423, Training Accuracy: 0.9424, Validation Accuracy: 0.9340\n",
      "Epoch [4/30], Batch [650/1438] Batch Loss: 0.1585, Training Accuracy: 0.9407, Validation Accuracy: 0.9316\n",
      "Epoch [4/30], Batch [660/1438] Batch Loss: 0.1583, Training Accuracy: 0.9406, Validation Accuracy: 0.9336\n",
      "Epoch [4/30], Batch [670/1438] Batch Loss: 0.2375, Training Accuracy: 0.9428, Validation Accuracy: 0.9356\n",
      "Epoch [4/30], Batch [680/1438] Batch Loss: 0.0504, Training Accuracy: 0.9434, Validation Accuracy: 0.9337\n",
      "Epoch [4/30], Batch [690/1438] Batch Loss: 0.1927, Training Accuracy: 0.9416, Validation Accuracy: 0.9327\n",
      "Epoch [4/30], Batch [700/1438] Batch Loss: 0.1362, Training Accuracy: 0.9464, Validation Accuracy: 0.9375\n",
      "Epoch [4/30], Batch [710/1438] Batch Loss: 0.2493, Training Accuracy: 0.9463, Validation Accuracy: 0.9363\n",
      "Epoch [4/30], Batch [720/1438] Batch Loss: 0.2769, Training Accuracy: 0.9472, Validation Accuracy: 0.9395\n",
      "Epoch [4/30], Batch [730/1438] Batch Loss: 0.2092, Training Accuracy: 0.9460, Validation Accuracy: 0.9386\n",
      "Epoch [4/30], Batch [740/1438] Batch Loss: 0.0859, Training Accuracy: 0.9466, Validation Accuracy: 0.9409\n",
      "Epoch [4/30], Batch [750/1438] Batch Loss: 0.1662, Training Accuracy: 0.9479, Validation Accuracy: 0.9386\n",
      "Epoch [4/30], Batch [760/1438] Batch Loss: 0.1880, Training Accuracy: 0.9475, Validation Accuracy: 0.9375\n",
      "Epoch [4/30], Batch [770/1438] Batch Loss: 0.1685, Training Accuracy: 0.9478, Validation Accuracy: 0.9389\n",
      "Epoch [4/30], Batch [780/1438] Batch Loss: 0.1851, Training Accuracy: 0.9452, Validation Accuracy: 0.9367\n",
      "Epoch [4/30], Batch [790/1438] Batch Loss: 0.1291, Training Accuracy: 0.9418, Validation Accuracy: 0.9317\n",
      "Epoch [4/30], Batch [800/1438] Batch Loss: 0.1762, Training Accuracy: 0.9454, Validation Accuracy: 0.9378\n",
      "Epoch [4/30], Batch [810/1438] Batch Loss: 0.3255, Training Accuracy: 0.9468, Validation Accuracy: 0.9374\n",
      "Epoch [4/30], Batch [820/1438] Batch Loss: 0.0921, Training Accuracy: 0.9480, Validation Accuracy: 0.9390\n",
      "Epoch [4/30], Batch [830/1438] Batch Loss: 0.1211, Training Accuracy: 0.9476, Validation Accuracy: 0.9385\n",
      "Epoch [4/30], Batch [840/1438] Batch Loss: 0.1311, Training Accuracy: 0.9475, Validation Accuracy: 0.9392\n",
      "Epoch [4/30], Batch [850/1438] Batch Loss: 0.0906, Training Accuracy: 0.9469, Validation Accuracy: 0.9389\n",
      "Epoch [4/30], Batch [860/1438] Batch Loss: 0.0987, Training Accuracy: 0.9485, Validation Accuracy: 0.9393\n",
      "Epoch [4/30], Batch [870/1438] Batch Loss: 0.1837, Training Accuracy: 0.9489, Validation Accuracy: 0.9416\n",
      "Epoch [4/30], Batch [880/1438] Batch Loss: 0.1284, Training Accuracy: 0.9497, Validation Accuracy: 0.9422\n",
      "Epoch [4/30], Batch [890/1438] Batch Loss: 0.1831, Training Accuracy: 0.9504, Validation Accuracy: 0.9411\n",
      "Epoch [4/30], Batch [900/1438] Batch Loss: 0.1286, Training Accuracy: 0.9493, Validation Accuracy: 0.9413\n",
      "Epoch [4/30], Batch [910/1438] Batch Loss: 0.1429, Training Accuracy: 0.9450, Validation Accuracy: 0.9363\n",
      "Epoch [4/30], Batch [920/1438] Batch Loss: 0.1170, Training Accuracy: 0.9466, Validation Accuracy: 0.9368\n",
      "Epoch [4/30], Batch [930/1438] Batch Loss: 0.0569, Training Accuracy: 0.9488, Validation Accuracy: 0.9413\n",
      "Epoch [4/30], Batch [940/1438] Batch Loss: 0.1016, Training Accuracy: 0.9496, Validation Accuracy: 0.9415\n",
      "Epoch [4/30], Batch [950/1438] Batch Loss: 0.0613, Training Accuracy: 0.9498, Validation Accuracy: 0.9427\n",
      "Epoch [4/30], Batch [960/1438] Batch Loss: 0.0954, Training Accuracy: 0.9501, Validation Accuracy: 0.9430\n",
      "Epoch [4/30], Batch [970/1438] Batch Loss: 0.1534, Training Accuracy: 0.9473, Validation Accuracy: 0.9382\n",
      "Epoch [4/30], Batch [980/1438] Batch Loss: 0.1153, Training Accuracy: 0.9455, Validation Accuracy: 0.9359\n",
      "Epoch [4/30], Batch [990/1438] Batch Loss: 0.2448, Training Accuracy: 0.9469, Validation Accuracy: 0.9398\n",
      "Epoch [4/30], Batch [1000/1438] Batch Loss: 0.3406, Training Accuracy: 0.9389, Validation Accuracy: 0.9325\n",
      "Epoch [4/30], Batch [1010/1438] Batch Loss: 0.2240, Training Accuracy: 0.9354, Validation Accuracy: 0.9264\n",
      "Epoch [4/30], Batch [1020/1438] Batch Loss: 0.2947, Training Accuracy: 0.9347, Validation Accuracy: 0.9244\n",
      "Epoch [4/30], Batch [1030/1438] Batch Loss: 0.2293, Training Accuracy: 0.9416, Validation Accuracy: 0.9340\n",
      "Epoch [4/30], Batch [1040/1438] Batch Loss: 0.1756, Training Accuracy: 0.9375, Validation Accuracy: 0.9315\n",
      "Epoch [4/30], Batch [1050/1438] Batch Loss: 0.1531, Training Accuracy: 0.9370, Validation Accuracy: 0.9303\n",
      "Epoch [4/30], Batch [1060/1438] Batch Loss: 0.1023, Training Accuracy: 0.9441, Validation Accuracy: 0.9371\n",
      "Epoch [4/30], Batch [1070/1438] Batch Loss: 0.1442, Training Accuracy: 0.9450, Validation Accuracy: 0.9379\n",
      "Epoch [4/30], Batch [1080/1438] Batch Loss: 0.0807, Training Accuracy: 0.9436, Validation Accuracy: 0.9372\n",
      "Epoch [4/30], Batch [1090/1438] Batch Loss: 0.1088, Training Accuracy: 0.9432, Validation Accuracy: 0.9363\n",
      "Epoch [4/30], Batch [1100/1438] Batch Loss: 0.2915, Training Accuracy: 0.9371, Validation Accuracy: 0.9291\n",
      "Epoch [4/30], Batch [1110/1438] Batch Loss: 0.1325, Training Accuracy: 0.9353, Validation Accuracy: 0.9249\n",
      "Epoch [4/30], Batch [1120/1438] Batch Loss: 0.1916, Training Accuracy: 0.9435, Validation Accuracy: 0.9359\n",
      "Epoch [4/30], Batch [1130/1438] Batch Loss: 0.1880, Training Accuracy: 0.9445, Validation Accuracy: 0.9381\n",
      "Epoch [4/30], Batch [1140/1438] Batch Loss: 0.2840, Training Accuracy: 0.9458, Validation Accuracy: 0.9394\n",
      "Epoch [4/30], Batch [1150/1438] Batch Loss: 0.0601, Training Accuracy: 0.9440, Validation Accuracy: 0.9367\n",
      "Epoch [4/30], Batch [1160/1438] Batch Loss: 0.0870, Training Accuracy: 0.9460, Validation Accuracy: 0.9386\n",
      "Epoch [4/30], Batch [1170/1438] Batch Loss: 0.2158, Training Accuracy: 0.9429, Validation Accuracy: 0.9340\n",
      "Epoch [4/30], Batch [1180/1438] Batch Loss: 0.1349, Training Accuracy: 0.9433, Validation Accuracy: 0.9329\n",
      "Epoch [4/30], Batch [1190/1438] Batch Loss: 0.1179, Training Accuracy: 0.9328, Validation Accuracy: 0.9213\n",
      "Epoch [4/30], Batch [1200/1438] Batch Loss: 0.0808, Training Accuracy: 0.9368, Validation Accuracy: 0.9276\n",
      "Epoch [4/30], Batch [1210/1438] Batch Loss: 0.0650, Training Accuracy: 0.9437, Validation Accuracy: 0.9338\n",
      "Epoch [4/30], Batch [1220/1438] Batch Loss: 0.2603, Training Accuracy: 0.9420, Validation Accuracy: 0.9331\n",
      "Epoch [4/30], Batch [1230/1438] Batch Loss: 0.0757, Training Accuracy: 0.9434, Validation Accuracy: 0.9340\n",
      "Epoch [4/30], Batch [1240/1438] Batch Loss: 0.1858, Training Accuracy: 0.9449, Validation Accuracy: 0.9354\n",
      "Epoch [4/30], Batch [1250/1438] Batch Loss: 0.2012, Training Accuracy: 0.9482, Validation Accuracy: 0.9386\n",
      "Epoch [4/30], Batch [1260/1438] Batch Loss: 0.1067, Training Accuracy: 0.9480, Validation Accuracy: 0.9382\n",
      "Epoch [4/30], Batch [1270/1438] Batch Loss: 0.3339, Training Accuracy: 0.9447, Validation Accuracy: 0.9360\n",
      "Epoch [4/30], Batch [1280/1438] Batch Loss: 0.1714, Training Accuracy: 0.9439, Validation Accuracy: 0.9346\n",
      "Epoch [4/30], Batch [1290/1438] Batch Loss: 0.0975, Training Accuracy: 0.9465, Validation Accuracy: 0.9351\n",
      "Epoch [4/30], Batch [1300/1438] Batch Loss: 0.1904, Training Accuracy: 0.9462, Validation Accuracy: 0.9338\n",
      "Epoch [4/30], Batch [1310/1438] Batch Loss: 0.2487, Training Accuracy: 0.9409, Validation Accuracy: 0.9288\n",
      "Epoch [4/30], Batch [1320/1438] Batch Loss: 0.2253, Training Accuracy: 0.9435, Validation Accuracy: 0.9358\n",
      "Epoch [4/30], Batch [1330/1438] Batch Loss: 0.1316, Training Accuracy: 0.9492, Validation Accuracy: 0.9407\n",
      "Epoch [4/30], Batch [1340/1438] Batch Loss: 0.0614, Training Accuracy: 0.9489, Validation Accuracy: 0.9405\n",
      "Epoch [4/30], Batch [1350/1438] Batch Loss: 0.1516, Training Accuracy: 0.9453, Validation Accuracy: 0.9389\n",
      "Epoch [4/30], Batch [1360/1438] Batch Loss: 0.1970, Training Accuracy: 0.9468, Validation Accuracy: 0.9383\n",
      "Epoch [4/30], Batch [1370/1438] Batch Loss: 0.1771, Training Accuracy: 0.9431, Validation Accuracy: 0.9341\n",
      "Epoch [4/30], Batch [1380/1438] Batch Loss: 0.1662, Training Accuracy: 0.9426, Validation Accuracy: 0.9330\n",
      "Epoch [4/30], Batch [1390/1438] Batch Loss: 0.0985, Training Accuracy: 0.9447, Validation Accuracy: 0.9343\n",
      "Epoch [4/30], Batch [1400/1438] Batch Loss: 0.0922, Training Accuracy: 0.9444, Validation Accuracy: 0.9365\n",
      "Epoch [4/30], Batch [1410/1438] Batch Loss: 0.5140, Training Accuracy: 0.9440, Validation Accuracy: 0.9348\n",
      "Epoch [4/30], Batch [1420/1438] Batch Loss: 0.1003, Training Accuracy: 0.9486, Validation Accuracy: 0.9404\n",
      "Epoch [4/30], Batch [1430/1438] Batch Loss: 0.3561, Training Accuracy: 0.9487, Validation Accuracy: 0.9411\n",
      "Epoch [4/30], Epoch Loss: 256.7122\n",
      "Epoch [5/30], Batch [10/1438] Batch Loss: 0.1115, Training Accuracy: 0.9492, Validation Accuracy: 0.9394\n",
      "Epoch [5/30], Batch [20/1438] Batch Loss: 0.1570, Training Accuracy: 0.9442, Validation Accuracy: 0.9365\n",
      "Epoch [5/30], Batch [30/1438] Batch Loss: 0.1936, Training Accuracy: 0.9429, Validation Accuracy: 0.9361\n",
      "Epoch [5/30], Batch [40/1438] Batch Loss: 0.1505, Training Accuracy: 0.9459, Validation Accuracy: 0.9377\n",
      "Epoch [5/30], Batch [50/1438] Batch Loss: 0.2484, Training Accuracy: 0.9489, Validation Accuracy: 0.9404\n",
      "Epoch [5/30], Batch [60/1438] Batch Loss: 0.3227, Training Accuracy: 0.9487, Validation Accuracy: 0.9415\n",
      "Epoch [5/30], Batch [70/1438] Batch Loss: 0.1472, Training Accuracy: 0.9487, Validation Accuracy: 0.9406\n",
      "Epoch [5/30], Batch [80/1438] Batch Loss: 0.2233, Training Accuracy: 0.9508, Validation Accuracy: 0.9416\n",
      "Epoch [5/30], Batch [90/1438] Batch Loss: 0.1303, Training Accuracy: 0.9488, Validation Accuracy: 0.9404\n",
      "Epoch [5/30], Batch [100/1438] Batch Loss: 0.0465, Training Accuracy: 0.9478, Validation Accuracy: 0.9369\n",
      "Epoch [5/30], Batch [110/1438] Batch Loss: 0.1288, Training Accuracy: 0.9469, Validation Accuracy: 0.9362\n",
      "Epoch [5/30], Batch [120/1438] Batch Loss: 0.0877, Training Accuracy: 0.9477, Validation Accuracy: 0.9371\n",
      "Epoch [5/30], Batch [130/1438] Batch Loss: 0.0950, Training Accuracy: 0.9486, Validation Accuracy: 0.9397\n",
      "Epoch [5/30], Batch [140/1438] Batch Loss: 0.1939, Training Accuracy: 0.9512, Validation Accuracy: 0.9421\n",
      "Epoch [5/30], Batch [150/1438] Batch Loss: 0.2447, Training Accuracy: 0.9509, Validation Accuracy: 0.9425\n",
      "Epoch [5/30], Batch [160/1438] Batch Loss: 0.0241, Training Accuracy: 0.9507, Validation Accuracy: 0.9425\n",
      "Epoch [5/30], Batch [170/1438] Batch Loss: 0.1097, Training Accuracy: 0.9502, Validation Accuracy: 0.9411\n",
      "Epoch [5/30], Batch [180/1438] Batch Loss: 0.1792, Training Accuracy: 0.9513, Validation Accuracy: 0.9423\n",
      "Epoch [5/30], Batch [190/1438] Batch Loss: 0.1989, Training Accuracy: 0.9515, Validation Accuracy: 0.9418\n",
      "Epoch [5/30], Batch [200/1438] Batch Loss: 0.2086, Training Accuracy: 0.9489, Validation Accuracy: 0.9391\n",
      "Epoch [5/30], Batch [210/1438] Batch Loss: 0.3228, Training Accuracy: 0.9457, Validation Accuracy: 0.9338\n",
      "Epoch [5/30], Batch [220/1438] Batch Loss: 0.2553, Training Accuracy: 0.9484, Validation Accuracy: 0.9362\n",
      "Epoch [5/30], Batch [230/1438] Batch Loss: 0.0810, Training Accuracy: 0.9453, Validation Accuracy: 0.9339\n",
      "Epoch [5/30], Batch [240/1438] Batch Loss: 0.0806, Training Accuracy: 0.9439, Validation Accuracy: 0.9354\n",
      "Epoch [5/30], Batch [250/1438] Batch Loss: 0.0883, Training Accuracy: 0.9483, Validation Accuracy: 0.9413\n",
      "Epoch [5/30], Batch [260/1438] Batch Loss: 0.0961, Training Accuracy: 0.9482, Validation Accuracy: 0.9415\n",
      "Epoch [5/30], Batch [270/1438] Batch Loss: 0.1408, Training Accuracy: 0.9450, Validation Accuracy: 0.9367\n",
      "Epoch [5/30], Batch [280/1438] Batch Loss: 0.1462, Training Accuracy: 0.9487, Validation Accuracy: 0.9379\n",
      "Epoch [5/30], Batch [290/1438] Batch Loss: 0.0832, Training Accuracy: 0.9503, Validation Accuracy: 0.9427\n",
      "Epoch [5/30], Batch [300/1438] Batch Loss: 0.1189, Training Accuracy: 0.9510, Validation Accuracy: 0.9427\n",
      "Epoch [5/30], Batch [310/1438] Batch Loss: 0.0844, Training Accuracy: 0.9505, Validation Accuracy: 0.9420\n",
      "Epoch [5/30], Batch [320/1438] Batch Loss: 0.0776, Training Accuracy: 0.9519, Validation Accuracy: 0.9423\n",
      "Epoch [5/30], Batch [330/1438] Batch Loss: 0.1083, Training Accuracy: 0.9507, Validation Accuracy: 0.9416\n",
      "Epoch [5/30], Batch [340/1438] Batch Loss: 0.1030, Training Accuracy: 0.9508, Validation Accuracy: 0.9421\n",
      "Epoch [5/30], Batch [350/1438] Batch Loss: 0.1222, Training Accuracy: 0.9495, Validation Accuracy: 0.9429\n",
      "Epoch [5/30], Batch [360/1438] Batch Loss: 0.1151, Training Accuracy: 0.9502, Validation Accuracy: 0.9430\n",
      "Epoch [5/30], Batch [370/1438] Batch Loss: 0.0842, Training Accuracy: 0.9507, Validation Accuracy: 0.9419\n",
      "Epoch [5/30], Batch [380/1438] Batch Loss: 0.2603, Training Accuracy: 0.9519, Validation Accuracy: 0.9420\n",
      "Epoch [5/30], Batch [390/1438] Batch Loss: 0.1322, Training Accuracy: 0.9512, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [400/1438] Batch Loss: 0.0686, Training Accuracy: 0.9513, Validation Accuracy: 0.9407\n",
      "Epoch [5/30], Batch [410/1438] Batch Loss: 0.1443, Training Accuracy: 0.9491, Validation Accuracy: 0.9388\n",
      "Epoch [5/30], Batch [420/1438] Batch Loss: 0.1359, Training Accuracy: 0.9421, Validation Accuracy: 0.9305\n",
      "Epoch [5/30], Batch [430/1438] Batch Loss: 0.1490, Training Accuracy: 0.9501, Validation Accuracy: 0.9379\n",
      "Epoch [5/30], Batch [440/1438] Batch Loss: 0.1463, Training Accuracy: 0.9508, Validation Accuracy: 0.9406\n",
      "Epoch [5/30], Batch [450/1438] Batch Loss: 0.2439, Training Accuracy: 0.9514, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [460/1438] Batch Loss: 0.0411, Training Accuracy: 0.9512, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [470/1438] Batch Loss: 0.2267, Training Accuracy: 0.9463, Validation Accuracy: 0.9338\n",
      "Epoch [5/30], Batch [480/1438] Batch Loss: 0.0778, Training Accuracy: 0.9462, Validation Accuracy: 0.9362\n",
      "Epoch [5/30], Batch [490/1438] Batch Loss: 0.1247, Training Accuracy: 0.9489, Validation Accuracy: 0.9406\n",
      "Epoch [5/30], Batch [500/1438] Batch Loss: 0.1385, Training Accuracy: 0.9510, Validation Accuracy: 0.9411\n",
      "Epoch [5/30], Batch [510/1438] Batch Loss: 0.0791, Training Accuracy: 0.9506, Validation Accuracy: 0.9400\n",
      "Epoch [5/30], Batch [520/1438] Batch Loss: 0.1622, Training Accuracy: 0.9506, Validation Accuracy: 0.9386\n",
      "Epoch [5/30], Batch [530/1438] Batch Loss: 0.0342, Training Accuracy: 0.9470, Validation Accuracy: 0.9352\n",
      "Epoch [5/30], Batch [540/1438] Batch Loss: 0.2182, Training Accuracy: 0.9490, Validation Accuracy: 0.9374\n",
      "Epoch [5/30], Batch [550/1438] Batch Loss: 0.1266, Training Accuracy: 0.9489, Validation Accuracy: 0.9383\n",
      "Epoch [5/30], Batch [560/1438] Batch Loss: 0.2584, Training Accuracy: 0.9515, Validation Accuracy: 0.9412\n",
      "Epoch [5/30], Batch [570/1438] Batch Loss: 0.1603, Training Accuracy: 0.9513, Validation Accuracy: 0.9425\n",
      "Epoch [5/30], Batch [580/1438] Batch Loss: 0.2021, Training Accuracy: 0.9495, Validation Accuracy: 0.9393\n",
      "Epoch [5/30], Batch [590/1438] Batch Loss: 0.2908, Training Accuracy: 0.9499, Validation Accuracy: 0.9397\n",
      "Epoch [5/30], Batch [600/1438] Batch Loss: 0.1606, Training Accuracy: 0.9479, Validation Accuracy: 0.9370\n",
      "Epoch [5/30], Batch [610/1438] Batch Loss: 0.1382, Training Accuracy: 0.9516, Validation Accuracy: 0.9410\n",
      "Epoch [5/30], Batch [620/1438] Batch Loss: 0.1244, Training Accuracy: 0.9516, Validation Accuracy: 0.9411\n",
      "Epoch [5/30], Batch [630/1438] Batch Loss: 0.1431, Training Accuracy: 0.9509, Validation Accuracy: 0.9402\n",
      "Epoch [5/30], Batch [640/1438] Batch Loss: 0.1864, Training Accuracy: 0.9496, Validation Accuracy: 0.9387\n",
      "Epoch [5/30], Batch [650/1438] Batch Loss: 0.1052, Training Accuracy: 0.9461, Validation Accuracy: 0.9356\n",
      "Epoch [5/30], Batch [660/1438] Batch Loss: 0.2186, Training Accuracy: 0.9503, Validation Accuracy: 0.9386\n",
      "Epoch [5/30], Batch [670/1438] Batch Loss: 0.1014, Training Accuracy: 0.9457, Validation Accuracy: 0.9363\n",
      "Epoch [5/30], Batch [680/1438] Batch Loss: 0.2893, Training Accuracy: 0.9435, Validation Accuracy: 0.9339\n",
      "Epoch [5/30], Batch [690/1438] Batch Loss: 0.1152, Training Accuracy: 0.9366, Validation Accuracy: 0.9253\n",
      "Epoch [5/30], Batch [700/1438] Batch Loss: 0.0731, Training Accuracy: 0.9501, Validation Accuracy: 0.9396\n",
      "Epoch [5/30], Batch [710/1438] Batch Loss: 0.1977, Training Accuracy: 0.9502, Validation Accuracy: 0.9403\n",
      "Epoch [5/30], Batch [720/1438] Batch Loss: 0.2603, Training Accuracy: 0.9503, Validation Accuracy: 0.9404\n",
      "Epoch [5/30], Batch [730/1438] Batch Loss: 0.1596, Training Accuracy: 0.9491, Validation Accuracy: 0.9393\n",
      "Epoch [5/30], Batch [740/1438] Batch Loss: 0.1207, Training Accuracy: 0.9501, Validation Accuracy: 0.9389\n",
      "Epoch [5/30], Batch [750/1438] Batch Loss: 0.1607, Training Accuracy: 0.9415, Validation Accuracy: 0.9277\n",
      "Epoch [5/30], Batch [760/1438] Batch Loss: 0.1274, Training Accuracy: 0.9428, Validation Accuracy: 0.9285\n",
      "Epoch [5/30], Batch [770/1438] Batch Loss: 0.1150, Training Accuracy: 0.9493, Validation Accuracy: 0.9381\n",
      "Epoch [5/30], Batch [780/1438] Batch Loss: 0.1161, Training Accuracy: 0.9499, Validation Accuracy: 0.9394\n",
      "Epoch [5/30], Batch [790/1438] Batch Loss: 0.1450, Training Accuracy: 0.9513, Validation Accuracy: 0.9406\n",
      "Epoch [5/30], Batch [800/1438] Batch Loss: 0.0779, Training Accuracy: 0.9514, Validation Accuracy: 0.9416\n",
      "Epoch [5/30], Batch [810/1438] Batch Loss: 0.1618, Training Accuracy: 0.9512, Validation Accuracy: 0.9425\n",
      "Epoch [5/30], Batch [820/1438] Batch Loss: 0.1408, Training Accuracy: 0.9490, Validation Accuracy: 0.9413\n",
      "Epoch [5/30], Batch [830/1438] Batch Loss: 0.1671, Training Accuracy: 0.9485, Validation Accuracy: 0.9382\n",
      "Epoch [5/30], Batch [840/1438] Batch Loss: 0.2033, Training Accuracy: 0.9437, Validation Accuracy: 0.9355\n",
      "Epoch [5/30], Batch [850/1438] Batch Loss: 0.1300, Training Accuracy: 0.9453, Validation Accuracy: 0.9355\n",
      "Epoch [5/30], Batch [860/1438] Batch Loss: 0.1392, Training Accuracy: 0.9469, Validation Accuracy: 0.9384\n",
      "Epoch [5/30], Batch [870/1438] Batch Loss: 0.1462, Training Accuracy: 0.9503, Validation Accuracy: 0.9414\n",
      "Epoch [5/30], Batch [880/1438] Batch Loss: 0.2259, Training Accuracy: 0.9489, Validation Accuracy: 0.9392\n",
      "Epoch [5/30], Batch [890/1438] Batch Loss: 0.2660, Training Accuracy: 0.9428, Validation Accuracy: 0.9332\n",
      "Epoch [5/30], Batch [900/1438] Batch Loss: 0.2276, Training Accuracy: 0.9478, Validation Accuracy: 0.9371\n",
      "Epoch [5/30], Batch [910/1438] Batch Loss: 0.1430, Training Accuracy: 0.9487, Validation Accuracy: 0.9375\n",
      "Epoch [5/30], Batch [920/1438] Batch Loss: 0.1045, Training Accuracy: 0.9493, Validation Accuracy: 0.9404\n",
      "Epoch [5/30], Batch [930/1438] Batch Loss: 0.1874, Training Accuracy: 0.9475, Validation Accuracy: 0.9386\n",
      "Epoch [5/30], Batch [940/1438] Batch Loss: 0.0481, Training Accuracy: 0.9473, Validation Accuracy: 0.9380\n",
      "Epoch [5/30], Batch [950/1438] Batch Loss: 0.3153, Training Accuracy: 0.9484, Validation Accuracy: 0.9374\n",
      "Epoch [5/30], Batch [960/1438] Batch Loss: 0.2254, Training Accuracy: 0.9444, Validation Accuracy: 0.9327\n",
      "Epoch [5/30], Batch [970/1438] Batch Loss: 0.2319, Training Accuracy: 0.9421, Validation Accuracy: 0.9313\n",
      "Epoch [5/30], Batch [980/1438] Batch Loss: 0.1283, Training Accuracy: 0.9495, Validation Accuracy: 0.9390\n",
      "Epoch [5/30], Batch [990/1438] Batch Loss: 0.2585, Training Accuracy: 0.9499, Validation Accuracy: 0.9416\n",
      "Epoch [5/30], Batch [1000/1438] Batch Loss: 0.2003, Training Accuracy: 0.9490, Validation Accuracy: 0.9410\n",
      "Epoch [5/30], Batch [1010/1438] Batch Loss: 0.1920, Training Accuracy: 0.9500, Validation Accuracy: 0.9411\n",
      "Epoch [5/30], Batch [1020/1438] Batch Loss: 0.2446, Training Accuracy: 0.9493, Validation Accuracy: 0.9392\n",
      "Epoch [5/30], Batch [1030/1438] Batch Loss: 0.3366, Training Accuracy: 0.9496, Validation Accuracy: 0.9399\n",
      "Epoch [5/30], Batch [1040/1438] Batch Loss: 0.0710, Training Accuracy: 0.9491, Validation Accuracy: 0.9396\n",
      "Epoch [5/30], Batch [1050/1438] Batch Loss: 0.2133, Training Accuracy: 0.9496, Validation Accuracy: 0.9404\n",
      "Epoch [5/30], Batch [1060/1438] Batch Loss: 0.0762, Training Accuracy: 0.9507, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [1070/1438] Batch Loss: 0.0724, Training Accuracy: 0.9511, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [1080/1438] Batch Loss: 0.2682, Training Accuracy: 0.9514, Validation Accuracy: 0.9413\n",
      "Epoch [5/30], Batch [1090/1438] Batch Loss: 0.1578, Training Accuracy: 0.9521, Validation Accuracy: 0.9417\n",
      "Epoch [5/30], Batch [1100/1438] Batch Loss: 0.1127, Training Accuracy: 0.9524, Validation Accuracy: 0.9437\n",
      "Epoch [5/30], Batch [1110/1438] Batch Loss: 0.1418, Training Accuracy: 0.9522, Validation Accuracy: 0.9437\n",
      "Epoch [5/30], Batch [1120/1438] Batch Loss: 0.1315, Training Accuracy: 0.9513, Validation Accuracy: 0.9419\n",
      "Epoch [5/30], Batch [1130/1438] Batch Loss: 0.1420, Training Accuracy: 0.9497, Validation Accuracy: 0.9399\n",
      "Epoch [5/30], Batch [1140/1438] Batch Loss: 0.1896, Training Accuracy: 0.9469, Validation Accuracy: 0.9347\n",
      "Epoch [5/30], Batch [1150/1438] Batch Loss: 0.2119, Training Accuracy: 0.9495, Validation Accuracy: 0.9389\n",
      "Epoch [5/30], Batch [1160/1438] Batch Loss: 0.1583, Training Accuracy: 0.9510, Validation Accuracy: 0.9403\n",
      "Epoch [5/30], Batch [1170/1438] Batch Loss: 0.0902, Training Accuracy: 0.9512, Validation Accuracy: 0.9416\n",
      "Epoch [5/30], Batch [1180/1438] Batch Loss: 0.1429, Training Accuracy: 0.9507, Validation Accuracy: 0.9413\n",
      "Epoch [5/30], Batch [1190/1438] Batch Loss: 0.0863, Training Accuracy: 0.9513, Validation Accuracy: 0.9418\n",
      "Epoch [5/30], Batch [1200/1438] Batch Loss: 0.1649, Training Accuracy: 0.9512, Validation Accuracy: 0.9401\n",
      "Epoch [5/30], Batch [1210/1438] Batch Loss: 0.0925, Training Accuracy: 0.9514, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [1220/1438] Batch Loss: 0.2579, Training Accuracy: 0.9509, Validation Accuracy: 0.9404\n",
      "Epoch [5/30], Batch [1230/1438] Batch Loss: 0.1420, Training Accuracy: 0.9481, Validation Accuracy: 0.9365\n",
      "Epoch [5/30], Batch [1240/1438] Batch Loss: 0.1942, Training Accuracy: 0.9475, Validation Accuracy: 0.9368\n",
      "Epoch [5/30], Batch [1250/1438] Batch Loss: 0.1313, Training Accuracy: 0.9453, Validation Accuracy: 0.9341\n",
      "Epoch [5/30], Batch [1260/1438] Batch Loss: 0.1459, Training Accuracy: 0.9485, Validation Accuracy: 0.9364\n",
      "Epoch [5/30], Batch [1270/1438] Batch Loss: 0.1105, Training Accuracy: 0.9505, Validation Accuracy: 0.9393\n",
      "Epoch [5/30], Batch [1280/1438] Batch Loss: 0.1326, Training Accuracy: 0.9510, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [1290/1438] Batch Loss: 0.1152, Training Accuracy: 0.9521, Validation Accuracy: 0.9411\n",
      "Epoch [5/30], Batch [1300/1438] Batch Loss: 0.2259, Training Accuracy: 0.9498, Validation Accuracy: 0.9385\n",
      "Epoch [5/30], Batch [1310/1438] Batch Loss: 0.1253, Training Accuracy: 0.9462, Validation Accuracy: 0.9345\n",
      "Epoch [5/30], Batch [1320/1438] Batch Loss: 0.1934, Training Accuracy: 0.9472, Validation Accuracy: 0.9341\n",
      "Epoch [5/30], Batch [1330/1438] Batch Loss: 0.0981, Training Accuracy: 0.9498, Validation Accuracy: 0.9396\n",
      "Epoch [5/30], Batch [1340/1438] Batch Loss: 0.0706, Training Accuracy: 0.9515, Validation Accuracy: 0.9412\n",
      "Epoch [5/30], Batch [1350/1438] Batch Loss: 0.1174, Training Accuracy: 0.9511, Validation Accuracy: 0.9394\n",
      "Epoch [5/30], Batch [1360/1438] Batch Loss: 0.1594, Training Accuracy: 0.9490, Validation Accuracy: 0.9377\n",
      "Epoch [5/30], Batch [1370/1438] Batch Loss: 0.1381, Training Accuracy: 0.9477, Validation Accuracy: 0.9390\n",
      "Epoch [5/30], Batch [1380/1438] Batch Loss: 0.3286, Training Accuracy: 0.9508, Validation Accuracy: 0.9407\n",
      "Epoch [5/30], Batch [1390/1438] Batch Loss: 0.2363, Training Accuracy: 0.9516, Validation Accuracy: 0.9420\n",
      "Epoch [5/30], Batch [1400/1438] Batch Loss: 0.1039, Training Accuracy: 0.9519, Validation Accuracy: 0.9433\n",
      "Epoch [5/30], Batch [1410/1438] Batch Loss: 0.0756, Training Accuracy: 0.9524, Validation Accuracy: 0.9427\n",
      "Epoch [5/30], Batch [1420/1438] Batch Loss: 0.1466, Training Accuracy: 0.9499, Validation Accuracy: 0.9409\n",
      "Epoch [5/30], Batch [1430/1438] Batch Loss: 0.3119, Training Accuracy: 0.9515, Validation Accuracy: 0.9422\n",
      "Epoch [5/30], Epoch Loss: 227.8176\n",
      "Epoch [6/30], Batch [10/1438] Batch Loss: 0.3398, Training Accuracy: 0.9451, Validation Accuracy: 0.9349\n",
      "Epoch [6/30], Batch [20/1438] Batch Loss: 0.1484, Training Accuracy: 0.9515, Validation Accuracy: 0.9410\n",
      "Epoch [6/30], Batch [30/1438] Batch Loss: 0.1881, Training Accuracy: 0.9508, Validation Accuracy: 0.9413\n",
      "Epoch [6/30], Batch [40/1438] Batch Loss: 0.1291, Training Accuracy: 0.9504, Validation Accuracy: 0.9402\n",
      "Epoch [6/30], Batch [50/1438] Batch Loss: 0.1132, Training Accuracy: 0.9513, Validation Accuracy: 0.9416\n",
      "Epoch [6/30], Batch [60/1438] Batch Loss: 0.0813, Training Accuracy: 0.9523, Validation Accuracy: 0.9435\n",
      "Epoch [6/30], Batch [70/1438] Batch Loss: 0.1842, Training Accuracy: 0.9524, Validation Accuracy: 0.9433\n",
      "Epoch [6/30], Batch [80/1438] Batch Loss: 0.1980, Training Accuracy: 0.9524, Validation Accuracy: 0.9421\n",
      "Epoch [6/30], Batch [90/1438] Batch Loss: 0.1129, Training Accuracy: 0.9514, Validation Accuracy: 0.9409\n",
      "Epoch [6/30], Batch [100/1438] Batch Loss: 0.1498, Training Accuracy: 0.9508, Validation Accuracy: 0.9417\n",
      "Epoch [6/30], Batch [110/1438] Batch Loss: 0.2623, Training Accuracy: 0.9517, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [120/1438] Batch Loss: 0.1764, Training Accuracy: 0.9519, Validation Accuracy: 0.9432\n",
      "Epoch [6/30], Batch [130/1438] Batch Loss: 0.1365, Training Accuracy: 0.9520, Validation Accuracy: 0.9440\n",
      "Epoch [6/30], Batch [140/1438] Batch Loss: 0.1992, Training Accuracy: 0.9521, Validation Accuracy: 0.9423\n",
      "Epoch [6/30], Batch [150/1438] Batch Loss: 0.1511, Training Accuracy: 0.9525, Validation Accuracy: 0.9434\n",
      "Epoch [6/30], Batch [160/1438] Batch Loss: 0.2224, Training Accuracy: 0.9529, Validation Accuracy: 0.9443\n",
      "Epoch [6/30], Batch [170/1438] Batch Loss: 0.2052, Training Accuracy: 0.9524, Validation Accuracy: 0.9429\n",
      "Epoch [6/30], Batch [180/1438] Batch Loss: 0.0945, Training Accuracy: 0.9511, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [190/1438] Batch Loss: 0.1831, Training Accuracy: 0.9518, Validation Accuracy: 0.9423\n",
      "Epoch [6/30], Batch [200/1438] Batch Loss: 0.1017, Training Accuracy: 0.9522, Validation Accuracy: 0.9426\n",
      "Epoch [6/30], Batch [210/1438] Batch Loss: 0.0669, Training Accuracy: 0.9510, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [220/1438] Batch Loss: 0.1418, Training Accuracy: 0.9511, Validation Accuracy: 0.9422\n",
      "Epoch [6/30], Batch [230/1438] Batch Loss: 0.0933, Training Accuracy: 0.9514, Validation Accuracy: 0.9418\n",
      "Epoch [6/30], Batch [240/1438] Batch Loss: 0.0639, Training Accuracy: 0.9523, Validation Accuracy: 0.9428\n",
      "Epoch [6/30], Batch [250/1438] Batch Loss: 0.1073, Training Accuracy: 0.9511, Validation Accuracy: 0.9413\n",
      "Epoch [6/30], Batch [260/1438] Batch Loss: 0.1092, Training Accuracy: 0.9487, Validation Accuracy: 0.9397\n",
      "Epoch [6/30], Batch [270/1438] Batch Loss: 0.1220, Training Accuracy: 0.9533, Validation Accuracy: 0.9448\n",
      "Epoch [6/30], Batch [280/1438] Batch Loss: 0.0575, Training Accuracy: 0.9523, Validation Accuracy: 0.9437\n",
      "Epoch [6/30], Batch [290/1438] Batch Loss: 0.0435, Training Accuracy: 0.9502, Validation Accuracy: 0.9404\n",
      "Epoch [6/30], Batch [300/1438] Batch Loss: 0.0946, Training Accuracy: 0.9519, Validation Accuracy: 0.9427\n",
      "Epoch [6/30], Batch [310/1438] Batch Loss: 0.1959, Training Accuracy: 0.9514, Validation Accuracy: 0.9417\n",
      "Epoch [6/30], Batch [320/1438] Batch Loss: 0.1688, Training Accuracy: 0.9502, Validation Accuracy: 0.9413\n",
      "Epoch [6/30], Batch [330/1438] Batch Loss: 0.1462, Training Accuracy: 0.9513, Validation Accuracy: 0.9405\n",
      "Epoch [6/30], Batch [340/1438] Batch Loss: 0.0614, Training Accuracy: 0.9487, Validation Accuracy: 0.9368\n",
      "Epoch [6/30], Batch [350/1438] Batch Loss: 0.1313, Training Accuracy: 0.9499, Validation Accuracy: 0.9416\n",
      "Epoch [6/30], Batch [360/1438] Batch Loss: 0.5398, Training Accuracy: 0.9506, Validation Accuracy: 0.9413\n",
      "Epoch [6/30], Batch [370/1438] Batch Loss: 0.2095, Training Accuracy: 0.9447, Validation Accuracy: 0.9338\n",
      "Epoch [6/30], Batch [380/1438] Batch Loss: 0.2213, Training Accuracy: 0.9520, Validation Accuracy: 0.9432\n",
      "Epoch [6/30], Batch [390/1438] Batch Loss: 0.1218, Training Accuracy: 0.9534, Validation Accuracy: 0.9443\n",
      "Epoch [6/30], Batch [400/1438] Batch Loss: 0.1902, Training Accuracy: 0.9522, Validation Accuracy: 0.9439\n",
      "Epoch [6/30], Batch [410/1438] Batch Loss: 0.0869, Training Accuracy: 0.9497, Validation Accuracy: 0.9388\n",
      "Epoch [6/30], Batch [420/1438] Batch Loss: 0.1318, Training Accuracy: 0.9513, Validation Accuracy: 0.9414\n",
      "Epoch [6/30], Batch [430/1438] Batch Loss: 0.1206, Training Accuracy: 0.9527, Validation Accuracy: 0.9434\n",
      "Epoch [6/30], Batch [440/1438] Batch Loss: 0.0804, Training Accuracy: 0.9530, Validation Accuracy: 0.9432\n",
      "Epoch [6/30], Batch [450/1438] Batch Loss: 0.0826, Training Accuracy: 0.9522, Validation Accuracy: 0.9428\n",
      "Epoch [6/30], Batch [460/1438] Batch Loss: 0.2250, Training Accuracy: 0.9499, Validation Accuracy: 0.9389\n",
      "Epoch [6/30], Batch [470/1438] Batch Loss: 0.1339, Training Accuracy: 0.9464, Validation Accuracy: 0.9356\n",
      "Epoch [6/30], Batch [480/1438] Batch Loss: 0.1161, Training Accuracy: 0.9503, Validation Accuracy: 0.9407\n",
      "Epoch [6/30], Batch [490/1438] Batch Loss: 0.0574, Training Accuracy: 0.9524, Validation Accuracy: 0.9423\n",
      "Epoch [6/30], Batch [500/1438] Batch Loss: 0.1139, Training Accuracy: 0.9529, Validation Accuracy: 0.9417\n",
      "Epoch [6/30], Batch [510/1438] Batch Loss: 0.2666, Training Accuracy: 0.9507, Validation Accuracy: 0.9386\n",
      "Epoch [6/30], Batch [520/1438] Batch Loss: 0.0895, Training Accuracy: 0.9480, Validation Accuracy: 0.9352\n",
      "Epoch [6/30], Batch [530/1438] Batch Loss: 0.1198, Training Accuracy: 0.9458, Validation Accuracy: 0.9336\n",
      "Epoch [6/30], Batch [540/1438] Batch Loss: 0.1641, Training Accuracy: 0.9491, Validation Accuracy: 0.9369\n",
      "Epoch [6/30], Batch [550/1438] Batch Loss: 0.0834, Training Accuracy: 0.9519, Validation Accuracy: 0.9400\n",
      "Epoch [6/30], Batch [560/1438] Batch Loss: 0.1076, Training Accuracy: 0.9526, Validation Accuracy: 0.9393\n",
      "Epoch [6/30], Batch [570/1438] Batch Loss: 0.0724, Training Accuracy: 0.9536, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [580/1438] Batch Loss: 0.1039, Training Accuracy: 0.9530, Validation Accuracy: 0.9414\n",
      "Epoch [6/30], Batch [590/1438] Batch Loss: 0.1720, Training Accuracy: 0.9533, Validation Accuracy: 0.9414\n",
      "Epoch [6/30], Batch [600/1438] Batch Loss: 0.0770, Training Accuracy: 0.9540, Validation Accuracy: 0.9439\n",
      "Epoch [6/30], Batch [610/1438] Batch Loss: 0.0303, Training Accuracy: 0.9544, Validation Accuracy: 0.9432\n",
      "Epoch [6/30], Batch [620/1438] Batch Loss: 0.1357, Training Accuracy: 0.9536, Validation Accuracy: 0.9427\n",
      "Epoch [6/30], Batch [630/1438] Batch Loss: 0.1956, Training Accuracy: 0.9522, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [640/1438] Batch Loss: 0.1188, Training Accuracy: 0.9532, Validation Accuracy: 0.9418\n",
      "Epoch [6/30], Batch [650/1438] Batch Loss: 0.1410, Training Accuracy: 0.9533, Validation Accuracy: 0.9431\n",
      "Epoch [6/30], Batch [660/1438] Batch Loss: 0.0453, Training Accuracy: 0.9535, Validation Accuracy: 0.9439\n",
      "Epoch [6/30], Batch [670/1438] Batch Loss: 0.0349, Training Accuracy: 0.9533, Validation Accuracy: 0.9440\n",
      "Epoch [6/30], Batch [680/1438] Batch Loss: 0.0359, Training Accuracy: 0.9536, Validation Accuracy: 0.9409\n",
      "Epoch [6/30], Batch [690/1438] Batch Loss: 0.1498, Training Accuracy: 0.9480, Validation Accuracy: 0.9352\n",
      "Epoch [6/30], Batch [700/1438] Batch Loss: 0.0764, Training Accuracy: 0.9530, Validation Accuracy: 0.9407\n",
      "Epoch [6/30], Batch [710/1438] Batch Loss: 0.1689, Training Accuracy: 0.9532, Validation Accuracy: 0.9418\n",
      "Epoch [6/30], Batch [720/1438] Batch Loss: 0.0085, Training Accuracy: 0.9511, Validation Accuracy: 0.9427\n",
      "Epoch [6/30], Batch [730/1438] Batch Loss: 0.1238, Training Accuracy: 0.9525, Validation Accuracy: 0.9435\n",
      "Epoch [6/30], Batch [740/1438] Batch Loss: 0.1535, Training Accuracy: 0.9524, Validation Accuracy: 0.9422\n",
      "Epoch [6/30], Batch [750/1438] Batch Loss: 0.1270, Training Accuracy: 0.9536, Validation Accuracy: 0.9421\n",
      "Epoch [6/30], Batch [760/1438] Batch Loss: 0.0617, Training Accuracy: 0.9520, Validation Accuracy: 0.9395\n",
      "Epoch [6/30], Batch [770/1438] Batch Loss: 0.1162, Training Accuracy: 0.9537, Validation Accuracy: 0.9427\n",
      "Epoch [6/30], Batch [780/1438] Batch Loss: 0.1472, Training Accuracy: 0.9532, Validation Accuracy: 0.9428\n",
      "Epoch [6/30], Batch [790/1438] Batch Loss: 0.0419, Training Accuracy: 0.9522, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [800/1438] Batch Loss: 0.2193, Training Accuracy: 0.9502, Validation Accuracy: 0.9397\n",
      "Epoch [6/30], Batch [810/1438] Batch Loss: 0.1901, Training Accuracy: 0.9484, Validation Accuracy: 0.9397\n",
      "Epoch [6/30], Batch [820/1438] Batch Loss: 0.2117, Training Accuracy: 0.9535, Validation Accuracy: 0.9429\n",
      "Epoch [6/30], Batch [830/1438] Batch Loss: 0.2386, Training Accuracy: 0.9509, Validation Accuracy: 0.9409\n",
      "Epoch [6/30], Batch [840/1438] Batch Loss: 0.1730, Training Accuracy: 0.9504, Validation Accuracy: 0.9397\n",
      "Epoch [6/30], Batch [850/1438] Batch Loss: 0.0786, Training Accuracy: 0.9513, Validation Accuracy: 0.9404\n",
      "Epoch [6/30], Batch [860/1438] Batch Loss: 0.1538, Training Accuracy: 0.9542, Validation Accuracy: 0.9426\n",
      "Epoch [6/30], Batch [870/1438] Batch Loss: 0.0859, Training Accuracy: 0.9547, Validation Accuracy: 0.9432\n",
      "Epoch [6/30], Batch [880/1438] Batch Loss: 0.3023, Training Accuracy: 0.9548, Validation Accuracy: 0.9431\n",
      "Epoch [6/30], Batch [890/1438] Batch Loss: 0.1403, Training Accuracy: 0.9536, Validation Accuracy: 0.9407\n",
      "Epoch [6/30], Batch [900/1438] Batch Loss: 0.1236, Training Accuracy: 0.9520, Validation Accuracy: 0.9390\n",
      "Epoch [6/30], Batch [910/1438] Batch Loss: 0.2230, Training Accuracy: 0.9524, Validation Accuracy: 0.9393\n",
      "Epoch [6/30], Batch [920/1438] Batch Loss: 0.0539, Training Accuracy: 0.9526, Validation Accuracy: 0.9383\n",
      "Epoch [6/30], Batch [930/1438] Batch Loss: 0.1440, Training Accuracy: 0.9528, Validation Accuracy: 0.9398\n",
      "Epoch [6/30], Batch [940/1438] Batch Loss: 0.1583, Training Accuracy: 0.9532, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [950/1438] Batch Loss: 0.0997, Training Accuracy: 0.9532, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [960/1438] Batch Loss: 0.0740, Training Accuracy: 0.9534, Validation Accuracy: 0.9429\n",
      "Epoch [6/30], Batch [970/1438] Batch Loss: 0.1360, Training Accuracy: 0.9535, Validation Accuracy: 0.9442\n",
      "Epoch [6/30], Batch [980/1438] Batch Loss: 0.1514, Training Accuracy: 0.9545, Validation Accuracy: 0.9439\n",
      "Epoch [6/30], Batch [990/1438] Batch Loss: 0.1452, Training Accuracy: 0.9541, Validation Accuracy: 0.9433\n",
      "Epoch [6/30], Batch [1000/1438] Batch Loss: 0.0872, Training Accuracy: 0.9526, Validation Accuracy: 0.9421\n",
      "Epoch [6/30], Batch [1010/1438] Batch Loss: 0.2048, Training Accuracy: 0.9517, Validation Accuracy: 0.9388\n",
      "Epoch [6/30], Batch [1020/1438] Batch Loss: 0.1409, Training Accuracy: 0.9534, Validation Accuracy: 0.9443\n",
      "Epoch [6/30], Batch [1030/1438] Batch Loss: 0.1977, Training Accuracy: 0.9498, Validation Accuracy: 0.9407\n",
      "Epoch [6/30], Batch [1040/1438] Batch Loss: 0.2273, Training Accuracy: 0.9440, Validation Accuracy: 0.9345\n",
      "Epoch [6/30], Batch [1050/1438] Batch Loss: 0.1245, Training Accuracy: 0.9512, Validation Accuracy: 0.9423\n",
      "Epoch [6/30], Batch [1060/1438] Batch Loss: 0.1413, Training Accuracy: 0.9521, Validation Accuracy: 0.9430\n",
      "Epoch [6/30], Batch [1070/1438] Batch Loss: 0.1379, Training Accuracy: 0.9528, Validation Accuracy: 0.9422\n",
      "Epoch [6/30], Batch [1080/1438] Batch Loss: 0.1803, Training Accuracy: 0.9524, Validation Accuracy: 0.9417\n",
      "Epoch [6/30], Batch [1090/1438] Batch Loss: 0.1905, Training Accuracy: 0.9492, Validation Accuracy: 0.9400\n",
      "Epoch [6/30], Batch [1100/1438] Batch Loss: 0.1403, Training Accuracy: 0.9485, Validation Accuracy: 0.9402\n",
      "Epoch [6/30], Batch [1110/1438] Batch Loss: 0.1120, Training Accuracy: 0.9513, Validation Accuracy: 0.9413\n",
      "Epoch [6/30], Batch [1120/1438] Batch Loss: 0.0646, Training Accuracy: 0.9512, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [1130/1438] Batch Loss: 0.1572, Training Accuracy: 0.9518, Validation Accuracy: 0.9415\n",
      "Epoch [6/30], Batch [1140/1438] Batch Loss: 0.2039, Training Accuracy: 0.9531, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [1150/1438] Batch Loss: 0.1651, Training Accuracy: 0.9521, Validation Accuracy: 0.9407\n",
      "Epoch [6/30], Batch [1160/1438] Batch Loss: 0.1778, Training Accuracy: 0.9514, Validation Accuracy: 0.9391\n",
      "Epoch [6/30], Batch [1170/1438] Batch Loss: 0.1865, Training Accuracy: 0.9525, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [1180/1438] Batch Loss: 0.0716, Training Accuracy: 0.9528, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [1190/1438] Batch Loss: 0.1620, Training Accuracy: 0.9527, Validation Accuracy: 0.9413\n",
      "Epoch [6/30], Batch [1200/1438] Batch Loss: 0.1560, Training Accuracy: 0.9535, Validation Accuracy: 0.9431\n",
      "Epoch [6/30], Batch [1210/1438] Batch Loss: 0.2214, Training Accuracy: 0.9536, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [1220/1438] Batch Loss: 0.1689, Training Accuracy: 0.9535, Validation Accuracy: 0.9422\n",
      "Epoch [6/30], Batch [1230/1438] Batch Loss: 0.1701, Training Accuracy: 0.9517, Validation Accuracy: 0.9409\n",
      "Epoch [6/30], Batch [1240/1438] Batch Loss: 0.0979, Training Accuracy: 0.9505, Validation Accuracy: 0.9400\n",
      "Epoch [6/30], Batch [1250/1438] Batch Loss: 0.1423, Training Accuracy: 0.9494, Validation Accuracy: 0.9374\n",
      "Epoch [6/30], Batch [1260/1438] Batch Loss: 0.1684, Training Accuracy: 0.9515, Validation Accuracy: 0.9407\n",
      "Epoch [6/30], Batch [1270/1438] Batch Loss: 0.0358, Training Accuracy: 0.9474, Validation Accuracy: 0.9377\n",
      "Epoch [6/30], Batch [1280/1438] Batch Loss: 0.0813, Training Accuracy: 0.9473, Validation Accuracy: 0.9383\n",
      "Epoch [6/30], Batch [1290/1438] Batch Loss: 0.2172, Training Accuracy: 0.9517, Validation Accuracy: 0.9413\n",
      "Epoch [6/30], Batch [1300/1438] Batch Loss: 0.1074, Training Accuracy: 0.9544, Validation Accuracy: 0.9450\n",
      "Epoch [6/30], Batch [1310/1438] Batch Loss: 0.1398, Training Accuracy: 0.9543, Validation Accuracy: 0.9449\n",
      "Epoch [6/30], Batch [1320/1438] Batch Loss: 0.2235, Training Accuracy: 0.9497, Validation Accuracy: 0.9380\n",
      "Epoch [6/30], Batch [1330/1438] Batch Loss: 0.0485, Training Accuracy: 0.9516, Validation Accuracy: 0.9397\n",
      "Epoch [6/30], Batch [1340/1438] Batch Loss: 0.1834, Training Accuracy: 0.9530, Validation Accuracy: 0.9429\n",
      "Epoch [6/30], Batch [1350/1438] Batch Loss: 0.1120, Training Accuracy: 0.9542, Validation Accuracy: 0.9435\n",
      "Epoch [6/30], Batch [1360/1438] Batch Loss: 0.0734, Training Accuracy: 0.9535, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [1370/1438] Batch Loss: 0.0566, Training Accuracy: 0.9526, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [1380/1438] Batch Loss: 0.0942, Training Accuracy: 0.9521, Validation Accuracy: 0.9418\n",
      "Epoch [6/30], Batch [1390/1438] Batch Loss: 0.0415, Training Accuracy: 0.9523, Validation Accuracy: 0.9416\n",
      "Epoch [6/30], Batch [1400/1438] Batch Loss: 0.1057, Training Accuracy: 0.9522, Validation Accuracy: 0.9419\n",
      "Epoch [6/30], Batch [1410/1438] Batch Loss: 0.0892, Training Accuracy: 0.9543, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Batch [1420/1438] Batch Loss: 0.2347, Training Accuracy: 0.9545, Validation Accuracy: 0.9420\n",
      "Epoch [6/30], Batch [1430/1438] Batch Loss: 0.1373, Training Accuracy: 0.9538, Validation Accuracy: 0.9425\n",
      "Epoch [6/30], Epoch Loss: 204.1585\n",
      "Epoch [7/30], Batch [10/1438] Batch Loss: 0.0366, Training Accuracy: 0.9518, Validation Accuracy: 0.9385\n",
      "Epoch [7/30], Batch [20/1438] Batch Loss: 0.2171, Training Accuracy: 0.9380, Validation Accuracy: 0.9251\n",
      "Epoch [7/30], Batch [30/1438] Batch Loss: 0.0296, Training Accuracy: 0.9472, Validation Accuracy: 0.9343\n",
      "Epoch [7/30], Batch [40/1438] Batch Loss: 0.1484, Training Accuracy: 0.9447, Validation Accuracy: 0.9320\n",
      "Epoch [7/30], Batch [50/1438] Batch Loss: 0.0668, Training Accuracy: 0.9481, Validation Accuracy: 0.9375\n",
      "Epoch [7/30], Batch [60/1438] Batch Loss: 0.1782, Training Accuracy: 0.9499, Validation Accuracy: 0.9394\n",
      "Epoch [7/30], Batch [70/1438] Batch Loss: 0.1295, Training Accuracy: 0.9514, Validation Accuracy: 0.9400\n",
      "Epoch [7/30], Batch [80/1438] Batch Loss: 0.1467, Training Accuracy: 0.9515, Validation Accuracy: 0.9404\n",
      "Epoch [7/30], Batch [90/1438] Batch Loss: 0.1362, Training Accuracy: 0.9500, Validation Accuracy: 0.9393\n",
      "Epoch [7/30], Batch [100/1438] Batch Loss: 0.3198, Training Accuracy: 0.9442, Validation Accuracy: 0.9320\n",
      "Epoch [7/30], Batch [110/1438] Batch Loss: 0.0723, Training Accuracy: 0.9511, Validation Accuracy: 0.9378\n",
      "Epoch [7/30], Batch [120/1438] Batch Loss: 0.2257, Training Accuracy: 0.9393, Validation Accuracy: 0.9232\n",
      "Epoch [7/30], Batch [130/1438] Batch Loss: 0.1991, Training Accuracy: 0.9400, Validation Accuracy: 0.9272\n",
      "Epoch [7/30], Batch [140/1438] Batch Loss: 0.1978, Training Accuracy: 0.9487, Validation Accuracy: 0.9373\n",
      "Epoch [7/30], Batch [150/1438] Batch Loss: 0.1170, Training Accuracy: 0.9477, Validation Accuracy: 0.9359\n",
      "Epoch [7/30], Batch [160/1438] Batch Loss: 0.0854, Training Accuracy: 0.9414, Validation Accuracy: 0.9311\n",
      "Epoch [7/30], Batch [170/1438] Batch Loss: 0.1366, Training Accuracy: 0.9435, Validation Accuracy: 0.9324\n",
      "Epoch [7/30], Batch [180/1438] Batch Loss: 0.1075, Training Accuracy: 0.9446, Validation Accuracy: 0.9330\n",
      "Epoch [7/30], Batch [190/1438] Batch Loss: 0.1766, Training Accuracy: 0.9515, Validation Accuracy: 0.9413\n",
      "Epoch [7/30], Batch [200/1438] Batch Loss: 0.1323, Training Accuracy: 0.9526, Validation Accuracy: 0.9425\n",
      "Epoch [7/30], Batch [210/1438] Batch Loss: 0.2509, Training Accuracy: 0.9504, Validation Accuracy: 0.9390\n",
      "Epoch [7/30], Batch [220/1438] Batch Loss: 0.1968, Training Accuracy: 0.9511, Validation Accuracy: 0.9382\n",
      "Epoch [7/30], Batch [230/1438] Batch Loss: 0.1388, Training Accuracy: 0.9515, Validation Accuracy: 0.9408\n",
      "Epoch [7/30], Batch [240/1438] Batch Loss: 0.0766, Training Accuracy: 0.9502, Validation Accuracy: 0.9387\n",
      "Epoch [7/30], Batch [250/1438] Batch Loss: 0.1395, Training Accuracy: 0.9506, Validation Accuracy: 0.9382\n",
      "Epoch [7/30], Batch [260/1438] Batch Loss: 0.1163, Training Accuracy: 0.9497, Validation Accuracy: 0.9366\n",
      "Epoch [7/30], Batch [270/1438] Batch Loss: 0.2162, Training Accuracy: 0.9433, Validation Accuracy: 0.9316\n",
      "Epoch [7/30], Batch [280/1438] Batch Loss: 0.0543, Training Accuracy: 0.9505, Validation Accuracy: 0.9399\n",
      "Epoch [7/30], Batch [290/1438] Batch Loss: 0.1289, Training Accuracy: 0.9498, Validation Accuracy: 0.9400\n",
      "Epoch [7/30], Batch [300/1438] Batch Loss: 0.1044, Training Accuracy: 0.9486, Validation Accuracy: 0.9378\n",
      "Epoch [7/30], Batch [310/1438] Batch Loss: 0.0476, Training Accuracy: 0.9504, Validation Accuracy: 0.9400\n",
      "Epoch [7/30], Batch [320/1438] Batch Loss: 0.1534, Training Accuracy: 0.9498, Validation Accuracy: 0.9392\n",
      "Epoch [7/30], Batch [330/1438] Batch Loss: 0.1214, Training Accuracy: 0.9494, Validation Accuracy: 0.9372\n",
      "Epoch [7/30], Batch [340/1438] Batch Loss: 0.2458, Training Accuracy: 0.9486, Validation Accuracy: 0.9377\n",
      "Epoch [7/30], Batch [350/1438] Batch Loss: 0.1134, Training Accuracy: 0.9508, Validation Accuracy: 0.9396\n",
      "Epoch [7/30], Batch [360/1438] Batch Loss: 0.0688, Training Accuracy: 0.9503, Validation Accuracy: 0.9401\n",
      "Epoch [7/30], Batch [370/1438] Batch Loss: 0.1117, Training Accuracy: 0.9486, Validation Accuracy: 0.9396\n",
      "Epoch [7/30], Batch [380/1438] Batch Loss: 0.0443, Training Accuracy: 0.9499, Validation Accuracy: 0.9403\n",
      "Epoch [7/30], Batch [390/1438] Batch Loss: 0.1382, Training Accuracy: 0.9516, Validation Accuracy: 0.9383\n",
      "Epoch [7/30], Batch [400/1438] Batch Loss: 0.1202, Training Accuracy: 0.9516, Validation Accuracy: 0.9384\n",
      "Epoch [7/30], Batch [410/1438] Batch Loss: 0.1935, Training Accuracy: 0.9503, Validation Accuracy: 0.9365\n",
      "Epoch [7/30], Batch [420/1438] Batch Loss: 0.2783, Training Accuracy: 0.9519, Validation Accuracy: 0.9400\n",
      "Epoch [7/30], Batch [430/1438] Batch Loss: 0.2342, Training Accuracy: 0.9517, Validation Accuracy: 0.9419\n",
      "Epoch [7/30], Batch [440/1438] Batch Loss: 0.1678, Training Accuracy: 0.9516, Validation Accuracy: 0.9418\n",
      "Epoch [7/30], Batch [450/1438] Batch Loss: 0.1641, Training Accuracy: 0.9499, Validation Accuracy: 0.9397\n",
      "Epoch [7/30], Batch [460/1438] Batch Loss: 0.1573, Training Accuracy: 0.9519, Validation Accuracy: 0.9409\n",
      "Epoch [7/30], Batch [470/1438] Batch Loss: 0.1236, Training Accuracy: 0.9518, Validation Accuracy: 0.9409\n",
      "Epoch [7/30], Batch [480/1438] Batch Loss: 0.1861, Training Accuracy: 0.9530, Validation Accuracy: 0.9411\n",
      "Epoch [7/30], Batch [490/1438] Batch Loss: 0.0653, Training Accuracy: 0.9530, Validation Accuracy: 0.9426\n",
      "Epoch [7/30], Batch [500/1438] Batch Loss: 0.2055, Training Accuracy: 0.9481, Validation Accuracy: 0.9375\n",
      "Epoch [7/30], Batch [510/1438] Batch Loss: 0.1337, Training Accuracy: 0.9476, Validation Accuracy: 0.9383\n",
      "Epoch [7/30], Batch [520/1438] Batch Loss: 0.1743, Training Accuracy: 0.9507, Validation Accuracy: 0.9413\n",
      "Epoch [7/30], Batch [530/1438] Batch Loss: 0.1246, Training Accuracy: 0.9533, Validation Accuracy: 0.9439\n",
      "Epoch [7/30], Batch [540/1438] Batch Loss: 0.1808, Training Accuracy: 0.9533, Validation Accuracy: 0.9421\n",
      "Epoch [7/30], Batch [550/1438] Batch Loss: 0.1419, Training Accuracy: 0.9524, Validation Accuracy: 0.9411\n",
      "Epoch [7/30], Batch [560/1438] Batch Loss: 0.0996, Training Accuracy: 0.9513, Validation Accuracy: 0.9388\n",
      "Epoch [7/30], Batch [570/1438] Batch Loss: 0.1216, Training Accuracy: 0.9539, Validation Accuracy: 0.9424\n",
      "Epoch [7/30], Batch [580/1438] Batch Loss: 0.0948, Training Accuracy: 0.9540, Validation Accuracy: 0.9439\n",
      "Epoch [7/30], Batch [590/1438] Batch Loss: 0.1341, Training Accuracy: 0.9538, Validation Accuracy: 0.9436\n",
      "Epoch [7/30], Batch [600/1438] Batch Loss: 0.1442, Training Accuracy: 0.9532, Validation Accuracy: 0.9435\n",
      "Epoch [7/30], Batch [610/1438] Batch Loss: 0.1307, Training Accuracy: 0.9524, Validation Accuracy: 0.9427\n",
      "Epoch [7/30], Batch [620/1438] Batch Loss: 0.1574, Training Accuracy: 0.9500, Validation Accuracy: 0.9413\n",
      "Epoch [7/30], Batch [630/1438] Batch Loss: 0.2761, Training Accuracy: 0.9497, Validation Accuracy: 0.9394\n",
      "Epoch [7/30], Batch [640/1438] Batch Loss: 0.1333, Training Accuracy: 0.9494, Validation Accuracy: 0.9393\n",
      "Epoch [7/30], Batch [650/1438] Batch Loss: 0.1465, Training Accuracy: 0.9511, Validation Accuracy: 0.9416\n",
      "Epoch [7/30], Batch [660/1438] Batch Loss: 0.1228, Training Accuracy: 0.9529, Validation Accuracy: 0.9436\n",
      "Epoch [7/30], Batch [670/1438] Batch Loss: 0.2973, Training Accuracy: 0.9533, Validation Accuracy: 0.9441\n",
      "Epoch [7/30], Batch [680/1438] Batch Loss: 0.1693, Training Accuracy: 0.9517, Validation Accuracy: 0.9411\n",
      "Epoch [7/30], Batch [690/1438] Batch Loss: 0.2181, Training Accuracy: 0.9520, Validation Accuracy: 0.9418\n",
      "Epoch [7/30], Batch [700/1438] Batch Loss: 0.0939, Training Accuracy: 0.9508, Validation Accuracy: 0.9414\n",
      "Epoch [7/30], Batch [710/1438] Batch Loss: 0.0594, Training Accuracy: 0.9521, Validation Accuracy: 0.9427\n",
      "Epoch [7/30], Batch [720/1438] Batch Loss: 0.1433, Training Accuracy: 0.9529, Validation Accuracy: 0.9439\n",
      "Epoch [7/30], Batch [730/1438] Batch Loss: 0.3025, Training Accuracy: 0.9537, Validation Accuracy: 0.9443\n",
      "Epoch [7/30], Batch [740/1438] Batch Loss: 0.1270, Training Accuracy: 0.9538, Validation Accuracy: 0.9433\n",
      "Epoch [7/30], Batch [750/1438] Batch Loss: 0.1500, Training Accuracy: 0.9537, Validation Accuracy: 0.9424\n",
      "Epoch [7/30], Batch [760/1438] Batch Loss: 0.0947, Training Accuracy: 0.9519, Validation Accuracy: 0.9406\n",
      "Epoch [7/30], Batch [770/1438] Batch Loss: 0.0689, Training Accuracy: 0.9532, Validation Accuracy: 0.9423\n",
      "Epoch [7/30], Batch [780/1438] Batch Loss: 0.3128, Training Accuracy: 0.9534, Validation Accuracy: 0.9430\n",
      "Epoch [7/30], Batch [790/1438] Batch Loss: 0.2119, Training Accuracy: 0.9544, Validation Accuracy: 0.9431\n",
      "Epoch [7/30], Batch [800/1438] Batch Loss: 0.1683, Training Accuracy: 0.9536, Validation Accuracy: 0.9424\n",
      "Epoch [7/30], Batch [810/1438] Batch Loss: 0.1164, Training Accuracy: 0.9533, Validation Accuracy: 0.9420\n",
      "Epoch [7/30], Batch [820/1438] Batch Loss: 0.1586, Training Accuracy: 0.9534, Validation Accuracy: 0.9415\n",
      "Epoch [7/30], Batch [830/1438] Batch Loss: 0.2259, Training Accuracy: 0.9519, Validation Accuracy: 0.9400\n",
      "Epoch [7/30], Batch [840/1438] Batch Loss: 0.0900, Training Accuracy: 0.9489, Validation Accuracy: 0.9372\n",
      "Epoch [7/30], Batch [850/1438] Batch Loss: 0.0534, Training Accuracy: 0.9532, Validation Accuracy: 0.9403\n",
      "Epoch [7/30], Batch [860/1438] Batch Loss: 0.2046, Training Accuracy: 0.9481, Validation Accuracy: 0.9377\n",
      "Epoch [7/30], Batch [870/1438] Batch Loss: 0.1296, Training Accuracy: 0.9390, Validation Accuracy: 0.9292\n",
      "Epoch [7/30], Batch [880/1438] Batch Loss: 0.1810, Training Accuracy: 0.9440, Validation Accuracy: 0.9349\n",
      "Epoch [7/30], Batch [890/1438] Batch Loss: 0.0784, Training Accuracy: 0.9487, Validation Accuracy: 0.9384\n",
      "Epoch [7/30], Batch [900/1438] Batch Loss: 0.1826, Training Accuracy: 0.9508, Validation Accuracy: 0.9416\n",
      "Epoch [7/30], Batch [910/1438] Batch Loss: 0.0613, Training Accuracy: 0.9498, Validation Accuracy: 0.9393\n",
      "Epoch [7/30], Batch [920/1438] Batch Loss: 0.1493, Training Accuracy: 0.9494, Validation Accuracy: 0.9378\n",
      "Epoch [7/30], Batch [930/1438] Batch Loss: 0.1505, Training Accuracy: 0.9495, Validation Accuracy: 0.9393\n",
      "Epoch [7/30], Batch [940/1438] Batch Loss: 0.1900, Training Accuracy: 0.9484, Validation Accuracy: 0.9383\n",
      "Epoch [7/30], Batch [950/1438] Batch Loss: 0.3271, Training Accuracy: 0.9501, Validation Accuracy: 0.9413\n",
      "Epoch [7/30], Batch [960/1438] Batch Loss: 0.1964, Training Accuracy: 0.9517, Validation Accuracy: 0.9426\n",
      "Epoch [7/30], Batch [970/1438] Batch Loss: 0.1691, Training Accuracy: 0.9447, Validation Accuracy: 0.9343\n",
      "Epoch [7/30], Batch [980/1438] Batch Loss: 0.2161, Training Accuracy: 0.9530, Validation Accuracy: 0.9429\n",
      "Epoch [7/30], Batch [990/1438] Batch Loss: 0.1433, Training Accuracy: 0.9537, Validation Accuracy: 0.9443\n",
      "Epoch [7/30], Batch [1000/1438] Batch Loss: 0.1715, Training Accuracy: 0.9533, Validation Accuracy: 0.9439\n",
      "Epoch [7/30], Batch [1010/1438] Batch Loss: 0.1447, Training Accuracy: 0.9529, Validation Accuracy: 0.9437\n",
      "Epoch [7/30], Batch [1020/1438] Batch Loss: 0.1325, Training Accuracy: 0.9530, Validation Accuracy: 0.9437\n",
      "Epoch [7/30], Batch [1030/1438] Batch Loss: 0.1438, Training Accuracy: 0.9517, Validation Accuracy: 0.9420\n",
      "Epoch [7/30], Batch [1040/1438] Batch Loss: 0.1161, Training Accuracy: 0.9536, Validation Accuracy: 0.9442\n",
      "Epoch [7/30], Batch [1050/1438] Batch Loss: 0.0382, Training Accuracy: 0.9537, Validation Accuracy: 0.9438\n",
      "Epoch [7/30], Batch [1060/1438] Batch Loss: 0.0666, Training Accuracy: 0.9503, Validation Accuracy: 0.9399\n",
      "Epoch [7/30], Batch [1070/1438] Batch Loss: 0.1455, Training Accuracy: 0.9493, Validation Accuracy: 0.9386\n",
      "Epoch [7/30], Batch [1080/1438] Batch Loss: 0.0909, Training Accuracy: 0.9524, Validation Accuracy: 0.9417\n",
      "Epoch [7/30], Batch [1090/1438] Batch Loss: 0.2096, Training Accuracy: 0.9511, Validation Accuracy: 0.9416\n",
      "Epoch [7/30], Batch [1100/1438] Batch Loss: 0.1518, Training Accuracy: 0.9513, Validation Accuracy: 0.9415\n",
      "Epoch [7/30], Batch [1110/1438] Batch Loss: 0.1492, Training Accuracy: 0.9531, Validation Accuracy: 0.9411\n",
      "Epoch [7/30], Batch [1120/1438] Batch Loss: 0.1869, Training Accuracy: 0.9530, Validation Accuracy: 0.9418\n",
      "Epoch [7/30], Batch [1130/1438] Batch Loss: 0.0570, Training Accuracy: 0.9520, Validation Accuracy: 0.9403\n",
      "Epoch [7/30], Batch [1140/1438] Batch Loss: 0.0444, Training Accuracy: 0.9520, Validation Accuracy: 0.9407\n",
      "Epoch [7/30], Batch [1150/1438] Batch Loss: 0.0647, Training Accuracy: 0.9523, Validation Accuracy: 0.9424\n",
      "Epoch [7/30], Batch [1160/1438] Batch Loss: 0.1896, Training Accuracy: 0.9516, Validation Accuracy: 0.9416\n",
      "Epoch [7/30], Batch [1170/1438] Batch Loss: 0.0762, Training Accuracy: 0.9515, Validation Accuracy: 0.9391\n",
      "Epoch [7/30], Batch [1180/1438] Batch Loss: 0.0911, Training Accuracy: 0.9532, Validation Accuracy: 0.9410\n",
      "Epoch [7/30], Batch [1190/1438] Batch Loss: 0.1933, Training Accuracy: 0.9523, Validation Accuracy: 0.9404\n",
      "Epoch [7/30], Batch [1200/1438] Batch Loss: 0.1793, Training Accuracy: 0.9518, Validation Accuracy: 0.9415\n",
      "Epoch [7/30], Batch [1210/1438] Batch Loss: 0.1145, Training Accuracy: 0.9518, Validation Accuracy: 0.9413\n",
      "Epoch [7/30], Batch [1220/1438] Batch Loss: 0.1540, Training Accuracy: 0.9517, Validation Accuracy: 0.9429\n",
      "Epoch [7/30], Batch [1230/1438] Batch Loss: 0.0994, Training Accuracy: 0.9511, Validation Accuracy: 0.9417\n",
      "Epoch [7/30], Batch [1240/1438] Batch Loss: 0.1546, Training Accuracy: 0.9524, Validation Accuracy: 0.9405\n",
      "Epoch [7/30], Batch [1250/1438] Batch Loss: 0.1737, Training Accuracy: 0.9520, Validation Accuracy: 0.9411\n",
      "Epoch [7/30], Batch [1260/1438] Batch Loss: 0.0620, Training Accuracy: 0.9532, Validation Accuracy: 0.9425\n",
      "Epoch [7/30], Batch [1270/1438] Batch Loss: 0.0551, Training Accuracy: 0.9537, Validation Accuracy: 0.9420\n",
      "Epoch [7/30], Batch [1280/1438] Batch Loss: 0.1494, Training Accuracy: 0.9524, Validation Accuracy: 0.9415\n",
      "Epoch [7/30], Batch [1290/1438] Batch Loss: 0.1261, Training Accuracy: 0.9505, Validation Accuracy: 0.9385\n",
      "Epoch [7/30], Batch [1300/1438] Batch Loss: 0.2667, Training Accuracy: 0.9511, Validation Accuracy: 0.9404\n",
      "Epoch [7/30], Batch [1310/1438] Batch Loss: 0.1437, Training Accuracy: 0.9527, Validation Accuracy: 0.9396\n",
      "Epoch [7/30], Batch [1320/1438] Batch Loss: 0.1377, Training Accuracy: 0.9540, Validation Accuracy: 0.9420\n",
      "Epoch [7/30], Batch [1330/1438] Batch Loss: 0.2191, Training Accuracy: 0.9523, Validation Accuracy: 0.9409\n",
      "Epoch [7/30], Batch [1340/1438] Batch Loss: 0.2370, Training Accuracy: 0.9508, Validation Accuracy: 0.9402\n",
      "Epoch [7/30], Batch [1350/1438] Batch Loss: 0.1049, Training Accuracy: 0.9531, Validation Accuracy: 0.9407\n",
      "Epoch [7/30], Batch [1360/1438] Batch Loss: 0.1013, Training Accuracy: 0.9528, Validation Accuracy: 0.9409\n",
      "Epoch [7/30], Batch [1370/1438] Batch Loss: 0.0505, Training Accuracy: 0.9521, Validation Accuracy: 0.9416\n",
      "Epoch [7/30], Batch [1380/1438] Batch Loss: 0.1634, Training Accuracy: 0.9528, Validation Accuracy: 0.9427\n",
      "Epoch [7/30], Batch [1390/1438] Batch Loss: 0.1202, Training Accuracy: 0.9525, Validation Accuracy: 0.9432\n",
      "Epoch [7/30], Batch [1400/1438] Batch Loss: 0.0706, Training Accuracy: 0.9500, Validation Accuracy: 0.9397\n",
      "Epoch [7/30], Batch [1410/1438] Batch Loss: 0.2016, Training Accuracy: 0.9511, Validation Accuracy: 0.9403\n",
      "Epoch [7/30], Batch [1420/1438] Batch Loss: 0.1319, Training Accuracy: 0.9524, Validation Accuracy: 0.9429\n",
      "Epoch [7/30], Batch [1430/1438] Batch Loss: 0.1627, Training Accuracy: 0.9526, Validation Accuracy: 0.9430\n",
      "Epoch [7/30], Epoch Loss: 206.6670\n",
      "Epoch [8/30], Batch [10/1438] Batch Loss: 0.0661, Training Accuracy: 0.9517, Validation Accuracy: 0.9403\n",
      "Epoch [8/30], Batch [20/1438] Batch Loss: 0.1538, Training Accuracy: 0.9524, Validation Accuracy: 0.9405\n",
      "Epoch [8/30], Batch [30/1438] Batch Loss: 0.2399, Training Accuracy: 0.9530, Validation Accuracy: 0.9415\n",
      "Epoch [8/30], Batch [40/1438] Batch Loss: 0.0553, Training Accuracy: 0.9550, Validation Accuracy: 0.9416\n",
      "Epoch [8/30], Batch [50/1438] Batch Loss: 0.1278, Training Accuracy: 0.9560, Validation Accuracy: 0.9443\n",
      "Epoch [8/30], Batch [60/1438] Batch Loss: 0.0823, Training Accuracy: 0.9558, Validation Accuracy: 0.9457\n",
      "\n",
      "Training interrupted. Exiting gracefully and saving model.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noams\\anaconda3\\envs\\DeepLearningProjectNew\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import signal\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('models/LSTM/best_val', exist_ok=True)\n",
    "os.makedirs('models/LSTM/500_batch', exist_ok=True)\n",
    "os.makedirs('models/LSTM/full_epochs', exist_ok=True)\n",
    "\n",
    "# Step 2: Pad sequences\n",
    "def collate_fn(batch):\n",
    "    X_batch, y_batch = zip(*batch)\n",
    "    lengths = torch.tensor([len(x) for x in X_batch])  # Store original sequence lengths\n",
    "    X_batch = pad_sequence(X_batch, batch_first=True)\n",
    "    y_batch = torch.tensor(y_batch)\n",
    "    return X_batch, y_batch, lengths\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=40, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=40, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=40, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "# Step 3: Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed_x)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = torch.mean(lstm_out, dim=1)  # Mean pooling across time steps\n",
    "        out = self.fc(out)  # Use the final hidden state\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 300  # Size of FastText embedding\n",
    "hidden_size = 256\n",
    "num_classes = 6\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Graceful stopping handler\n",
    "def signal_handler(sig, frame):\n",
    "    print(\"\\nTraining interrupted. Exiting gracefully and saving model.\")\n",
    "    torch.save(model.state_dict(), 'model_checkpoint.pth')\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in dataloader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    model.train()\n",
    "    return correct / total\n",
    "\n",
    "best_val_accuracy = 0\n",
    "# Step 5: Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_epoch_loss = 0\n",
    "    num_batches = len(train_dataloader)  # Total number of batches in the current epoch\n",
    "    for batch_idx, (X_batch, y_batch, lengths) in enumerate(train_dataloader, start=1):\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            # Print loss and accuracies\n",
    "            train_accuracy = calculate_accuracy(model, train_dataloader)\n",
    "            val_accuracy = calculate_accuracy(model, val_dataloader)\n",
    "        \n",
    "            if val_accuracy >= best_val_accuracy+0.01 and val_accuracy >= 0.9:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), f'models/LSTM/best_val/model_checkpoint_val_{int(val_accuracy*100)}.pth')\n",
    "                \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{num_batches}] Batch Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        if batch_idx % 500 == 499:\n",
    "            # Save the model every 500 batches\n",
    "            torch.save(model.state_dict(), f'models/LSTM/500_batch/model_checkpoint_epoch_{epoch}_batch_{batch_idx}.pth')\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Epoch Loss: {total_epoch_loss:.4f}\")\n",
    "    # save the model with the epoch number, in models folder\n",
    "    torch.save(model.state_dict(), f'models/LSTM/full_epochs/model_checkpoint_epoch_{epoch}.pth')\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    89832.000000\n",
       "mean        19.438986\n",
       "std         11.085368\n",
       "min          1.000000\n",
       "25%         11.000000\n",
       "50%         17.000000\n",
       "75%         26.000000\n",
       "max        100.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed[\"text\"].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing tokens: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Check the percentage of tokens in the dataset that are missing in the FastText vocabulary\n",
    "missing_tokens = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for seq in X_train:\n",
    "    for token in seq:\n",
    "        total_tokens += 1\n",
    "        if token not in fasttext.wv:\n",
    "            missing_tokens += 1\n",
    "\n",
    "missing_percentage = (missing_tokens / total_tokens) * 100\n",
    "print(f\"Percentage of missing tokens: {missing_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningProjectNew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
