{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i feel awful about it too because it s my job ...      0\n",
       "1                              im alone i feel awful      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################## IMPORT DATA ##################\n",
    "\n",
    "import pandas as pd\n",
    "# Read the Parquet file\n",
    "file_path = \"train-00000-of-00001.parquet\"  # Replace with your Parquet file path\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after undersampling:\n",
      "label\n",
      "0    14972\n",
      "1    14972\n",
      "2    14972\n",
      "3    14972\n",
      "4    14972\n",
      "5    14972\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noams\\AppData\\Local\\Temp\\ipykernel_21188\\3946450975.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('label')\n"
     ]
    }
   ],
   "source": [
    "################## BALANCED DF CREATION ##################\n",
    "\n",
    "# Count samples for each label\n",
    "label_counts = df['label'].value_counts()\n",
    "minority_class_size = label_counts.min()\n",
    "\n",
    "# Group by the category and sample the minority class size for each group\n",
    "balanced_df = (\n",
    "    df.groupby('label')\n",
    "    .apply(lambda x: x.sample(n=minority_class_size, random_state=42))  # Randomly select samples\n",
    "    .reset_index(drop=True)  # Reset the index\n",
    ")\n",
    "\n",
    "print(\"Class distribution after undersampling:\")\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89832, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel sorry about you because your point of v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like he s watching quietly because he s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i feel sorry about you because your point of v...      0\n",
       "1  i feel like he s watching quietly because he s...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = balanced_df\n",
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84900, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel sorry about you because your point of v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like he s watching quietly because he s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i feel sorry about you because your point of v...      0\n",
       "1  i feel like he s watching quietly because he s...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all the sentences that are longer than 40 words\n",
    "df_preprocessed = df_preprocessed[df_preprocessed['text'].apply(lambda x: len(x.split()) <= 40 and len(x.split()) >= 3)]\n",
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing The Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84900/84900 [00:07<00:00, 11281.01it/s]\n",
      "C:\\Users\\noams\\AppData\\Local\\Temp\\ipykernel_21188\\1856993039.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_preprocessed['tokenized_text'] = df_preprocessed['text'].progress_apply(lambda x: tokenizer.tokenize(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel sorry about you because your point of v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, feel, sorry, about, you, because, your, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like he s watching quietly because he s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, feel, like, he, s, watching, quietly, beca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  i feel sorry about you because your point of v...      0   \n",
       "1  i feel like he s watching quietly because he s...      0   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [i, feel, sorry, about, you, because, your, po...  \n",
       "1  [i, feel, like, he, s, watching, quietly, beca...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Tokenize each text and store tokenized output in a new column\n",
    "df_preprocessed['tokenized_text'] = df_preprocessed['text'].progress_apply(lambda x: tokenizer.tokenize(x))\n",
    "df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using The Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.15019275e-03  1.05880331e-02  5.85333593e-02 -1.80088747e-02\n",
      "  2.17381623e-02 -6.36548251e-02  4.79345582e-02  5.82478940e-04\n",
      " -2.71546952e-02  4.41303067e-02 -2.70192716e-02  6.18359353e-03\n",
      " -1.98998721e-03  3.27444300e-02  4.41248150e-04  3.95078957e-02\n",
      " -3.66007313e-02 -2.67524342e-03  5.90687105e-03  2.27801464e-02\n",
      " -3.65645699e-02 -4.19695750e-02 -7.16437120e-03 -4.24787998e-02\n",
      "  4.29837480e-02  2.15609614e-02  2.45211683e-02  1.77936815e-02\n",
      "  1.41702415e-02 -2.20726691e-02  3.10446555e-03 -7.80420452e-02\n",
      "  1.17998419e-03 -9.21797939e-03  6.47169817e-03 -2.28024814e-02\n",
      "  4.87457886e-02 -4.71891044e-03 -2.68589742e-02  2.32070964e-02\n",
      " -1.11313999e-01 -4.35941219e-02 -2.73997393e-02  9.44887754e-03\n",
      " -3.80694109e-04  7.03306049e-02  2.92210635e-02  2.60734255e-03\n",
      " -1.09368563e-01  3.23175937e-02  1.05939796e-02 -1.16323661e-02\n",
      "  3.07511836e-02  2.60094590e-02 -8.07798654e-02  1.67720411e-02\n",
      "  3.48081253e-02  2.77459081e-02 -1.08297467e-01 -5.31493500e-03\n",
      "  1.44641800e-02 -4.21016067e-02  1.74080562e-02  4.53213602e-02\n",
      " -2.03130450e-02  2.74548884e-02 -6.88411202e-03 -2.46204473e-02\n",
      "  2.61567142e-02 -3.83009203e-02  1.16500808e-02 -8.17218609e-03\n",
      "  8.02695658e-03  3.42647592e-03  3.51003483e-02 -1.82733461e-02\n",
      "  4.32076678e-03 -4.56346609e-02 -3.58041981e-03  7.33157969e-04\n",
      " -7.00480789e-02  1.51639162e-02 -2.93170307e-02 -2.26665963e-03\n",
      " -1.46285852e-03 -4.92869085e-03  1.23468535e-02 -1.64976879e-03\n",
      "  8.98252148e-03  2.62601525e-02 -2.32274830e-02  3.29418071e-02\n",
      " -6.57121825e-04 -1.07547581e-01  2.33253278e-02  1.39847517e-01\n",
      "  1.58148892e-02 -1.64823269e-03 -2.84349229e-02  5.61736058e-03\n",
      " -1.67369023e-02  2.66228225e-02  6.94100326e-03  8.41293857e-03\n",
      "  1.51664426e-03 -3.75236198e-02 -1.37651367e-02  1.14708664e-02\n",
      "  2.40539201e-02  4.66846898e-02 -6.71437234e-02 -1.71261318e-02\n",
      "  3.35687393e-04 -5.62422201e-02 -3.22531536e-02 -4.00833748e-02\n",
      " -4.11019251e-02  2.01759152e-02 -2.96448208e-02 -1.05031177e-01\n",
      " -3.46056521e-02  2.21012570e-02  4.47731242e-02 -1.29172336e-02\n",
      "  8.09648912e-03  4.79777828e-02  1.44451531e-02  2.04787706e-03\n",
      " -3.01583912e-02 -2.26560347e-02 -5.61325066e-03 -1.58980675e-02\n",
      " -2.46041715e-02  6.54169312e-03  1.08528119e-02 -1.99720380e-03\n",
      " -3.64154987e-02 -2.72105075e-03 -1.79823264e-02 -1.05963647e-02\n",
      "  2.41233371e-02 -5.69446757e-03 -6.53078929e-02  1.23347845e-02\n",
      " -8.17020983e-02 -3.35735152e-03 -8.71597230e-03 -8.98809284e-02\n",
      "  3.94005841e-03  3.25443298e-02 -2.04956699e-02 -5.04665375e-02\n",
      " -3.28555182e-02 -2.24030763e-03  7.86474533e-03 -2.72138901e-02\n",
      "  7.01845298e-03  2.00957097e-02  2.62387749e-02 -1.19036958e-02\n",
      "  1.24580692e-02  4.23062369e-02  1.99226998e-02 -3.18395346e-02\n",
      " -2.77881511e-02  6.25803843e-02 -7.63805816e-03  2.92812139e-02\n",
      "  2.07703561e-02 -2.04395689e-02  1.50343403e-02  1.24745467e-03\n",
      " -9.26262699e-03  1.06976014e-02 -1.53755967e-03  1.53396921e-02\n",
      " -1.47519382e-02  4.05351724e-03 -2.72588097e-02 -2.23288313e-02\n",
      " -1.43179251e-02 -1.48736862e-02 -6.97966572e-03 -6.25730827e-02\n",
      " -9.24157277e-02 -4.99901269e-03  3.90528282e-03  1.08032107e-01\n",
      " -2.22198945e-02  5.37365675e-04 -5.61015308e-03 -1.26408292e-02\n",
      " -3.95569131e-02 -3.68211940e-02 -1.47302449e-02 -4.51780204e-03\n",
      " -8.49625387e-04  1.00242789e-03 -4.06150566e-03  1.16499308e-02\n",
      " -8.09082165e-02 -3.61052516e-04  2.45568417e-02  5.52267116e-03\n",
      "  1.62120517e-02 -5.66453338e-02  3.00804935e-02  1.02142263e-02\n",
      "  7.48731475e-03  1.39939431e-02 -7.50672892e-02  1.99853852e-02\n",
      "  9.86382961e-02  6.40084501e-03 -1.68929789e-02  2.70522330e-02\n",
      "  1.90184899e-02 -5.54622570e-03 -2.61672381e-02 -1.32325664e-02\n",
      " -2.06757500e-03 -2.54046312e-03 -5.19485585e-02 -9.45406593e-03\n",
      "  9.06954631e-02  1.59165375e-02 -9.35550686e-03  8.86306074e-03\n",
      "  6.12023054e-03  1.54464273e-02  8.62891413e-03 -3.93600389e-02\n",
      "  4.45931181e-02 -6.83805253e-03  1.77116469e-02 -9.93025862e-03\n",
      " -1.07441898e-02  9.28017893e-04  1.87607743e-02  2.53707031e-03\n",
      "  4.24202196e-02 -1.79975517e-02  3.28217214e-03 -1.37960566e-02\n",
      " -2.63354816e-02 -2.29472443e-02 -9.94787086e-03  1.50219081e-02\n",
      " -2.90881302e-02  2.60935370e-02  1.01650074e-01 -2.07196809e-02\n",
      " -1.18812323e-02  1.88614316e-02 -2.04383582e-02  2.92565790e-03\n",
      "  1.22541916e-02 -7.16641406e-03  5.64003875e-03  9.38325282e-03\n",
      " -5.17074578e-02  6.79351836e-02 -1.79398097e-02  2.81262994e-02\n",
      "  1.78447235e-02  6.18008785e-02 -2.20022872e-02  7.90594984e-03\n",
      "  7.04661012e-03 -5.23740351e-02 -2.28937902e-02 -1.43831018e-02\n",
      " -3.28972042e-02  6.78121019e-03 -4.62064613e-03  3.64981256e-02\n",
      " -1.44333867e-02  5.65662235e-03  2.54673953e-03  1.99225899e-02\n",
      "  2.87093278e-02  3.56950378e-03 -1.34036522e-02  3.17565687e-02\n",
      " -3.86744738e-03  1.56376343e-02 -4.04994860e-02  4.72432235e-03\n",
      " -3.55616203e-06  2.50393637e-02  3.22945490e-02 -9.45789367e-03\n",
      "  3.54374829e-03  4.07337630e-03  5.78961801e-03  5.26164994e-02\n",
      "  8.23039282e-03  2.14898866e-02  3.39484848e-02  1.17234122e-02]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "fasttext_path = \"crawl-300d-2M-subword/crawl-300d-2M-subword.bin\"\n",
    "fasttext = load_facebook_model(fasttext_path)\n",
    "\n",
    "# Check an example word vector\n",
    "print(fasttext.wv['happy'])  # This should now work without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noams\\AppData\\Local\\Temp\\ipykernel_21188\\2915091153.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_fakvb73nko\\croot\\pytorch-select_1730848725921\\work\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  self.X = [torch.tensor([fasttext.wv[token] if token in fasttext.wv else np.zeros(fasttext.vector_size)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "# it's assumed that the FastText model is already loaded and stored in the `fasttext` variable\n",
    "fasttext\n",
    "\n",
    "# data\n",
    "X = df_preprocessed['tokenized_text'].tolist()\n",
    "y = df_preprocessed['label'].tolist()\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Preprocess data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y, fasttext):\n",
    "        # Use fasttext.wv for word lookup\n",
    "        self.X = [torch.tensor([fasttext.wv[token] if token in fasttext.wv else np.zeros(fasttext.vector_size)\n",
    "                                for token in seq]) for seq in X]\n",
    "        self.y = torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = TextDataset(X_train, y_train, fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Batch [1/1359], Batch Loss: 1.7913, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [2/1359], Batch Loss: 2.1665, Training Accuracy: 0.1683\n",
      "Epoch [1/20], Batch [3/1359], Batch Loss: 1.7913, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [4/1359], Batch Loss: 1.7930, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [5/1359], Batch Loss: 1.8293, Training Accuracy: 0.1665\n",
      "Epoch [1/20], Batch [6/1359], Batch Loss: 1.8041, Training Accuracy: 0.1696\n",
      "Epoch [1/20], Batch [7/1359], Batch Loss: 1.8480, Training Accuracy: 0.1674\n",
      "Epoch [1/20], Batch [8/1359], Batch Loss: 6.1032, Training Accuracy: 0.1671\n",
      "Epoch [1/20], Batch [9/1359], Batch Loss: 1.8043, Training Accuracy: 0.1669\n",
      "Epoch [1/20], Batch [10/1359], Batch Loss: 1.7876, Training Accuracy: 0.1652\n",
      "Epoch [1/20], Batch [11/1359], Batch Loss: 1.9283, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [12/1359], Batch Loss: 1.7687, Training Accuracy: 0.1643\n",
      "Epoch [1/20], Batch [13/1359], Batch Loss: 2.1693, Training Accuracy: 0.1682\n",
      "Epoch [1/20], Batch [14/1359], Batch Loss: 1.9782, Training Accuracy: 0.1699\n",
      "Epoch [1/20], Batch [15/1359], Batch Loss: 2.0431, Training Accuracy: 0.1648\n",
      "Epoch [1/20], Batch [16/1359], Batch Loss: 2.0944, Training Accuracy: 0.1651\n",
      "Epoch [1/20], Batch [17/1359], Batch Loss: 2.4536, Training Accuracy: 0.1648\n",
      "Epoch [1/20], Batch [18/1359], Batch Loss: 2.3365, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [19/1359], Batch Loss: 2.0164, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [20/1359], Batch Loss: 1.9205, Training Accuracy: 0.1668\n",
      "Epoch [1/20], Batch [21/1359], Batch Loss: 2.0516, Training Accuracy: 0.1666\n",
      "Epoch [1/20], Batch [22/1359], Batch Loss: 2.1429, Training Accuracy: 0.1689\n",
      "Epoch [1/20], Batch [23/1359], Batch Loss: 2.0364, Training Accuracy: 0.1686\n",
      "Epoch [1/20], Batch [24/1359], Batch Loss: 1.7464, Training Accuracy: 0.1686\n",
      "Epoch [1/20], Batch [25/1359], Batch Loss: 1.8172, Training Accuracy: 0.1678\n",
      "Epoch [1/20], Batch [26/1359], Batch Loss: 1.8374, Training Accuracy: 0.1674\n",
      "Epoch [1/20], Batch [27/1359], Batch Loss: 1.8653, Training Accuracy: 0.1681\n",
      "Epoch [1/20], Batch [28/1359], Batch Loss: 1.8498, Training Accuracy: 0.1679\n",
      "Epoch [1/20], Batch [29/1359], Batch Loss: 1.8536, Training Accuracy: 0.1707\n",
      "Epoch [1/20], Batch [30/1359], Batch Loss: 1.9646, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [31/1359], Batch Loss: 1.8821, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [32/1359], Batch Loss: 1.8829, Training Accuracy: 0.1647\n",
      "Epoch [1/20], Batch [33/1359], Batch Loss: 1.8001, Training Accuracy: 0.1673\n",
      "Epoch [1/20], Batch [34/1359], Batch Loss: 1.8590, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [35/1359], Batch Loss: 1.8303, Training Accuracy: 0.1673\n",
      "Epoch [1/20], Batch [36/1359], Batch Loss: 1.8070, Training Accuracy: 0.1690\n",
      "Epoch [1/20], Batch [37/1359], Batch Loss: 1.7528, Training Accuracy: 0.1658\n",
      "Epoch [1/20], Batch [38/1359], Batch Loss: 1.7902, Training Accuracy: 0.1674\n",
      "Epoch [1/20], Batch [39/1359], Batch Loss: 1.7674, Training Accuracy: 0.1667\n",
      "Epoch [1/20], Batch [40/1359], Batch Loss: 1.7984, Training Accuracy: 0.1714\n",
      "Epoch [1/20], Batch [41/1359], Batch Loss: 1.8144, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [42/1359], Batch Loss: 1.7997, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [43/1359], Batch Loss: 1.8095, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [44/1359], Batch Loss: 1.7780, Training Accuracy: 0.1684\n",
      "Epoch [1/20], Batch [45/1359], Batch Loss: 1.8027, Training Accuracy: 0.1692\n",
      "Epoch [1/20], Batch [46/1359], Batch Loss: 1.8309, Training Accuracy: 0.1652\n",
      "Epoch [1/20], Batch [47/1359], Batch Loss: 1.8095, Training Accuracy: 0.1682\n",
      "Epoch [1/20], Batch [48/1359], Batch Loss: 1.8418, Training Accuracy: 0.1667\n",
      "Epoch [1/20], Batch [49/1359], Batch Loss: 1.7546, Training Accuracy: 0.1689\n",
      "Epoch [1/20], Batch [50/1359], Batch Loss: 1.7545, Training Accuracy: 0.1664\n",
      "Epoch [1/20], Batch [51/1359], Batch Loss: 1.8287, Training Accuracy: 0.1678\n",
      "Epoch [1/20], Batch [52/1359], Batch Loss: 1.8219, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [53/1359], Batch Loss: 1.8472, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [54/1359], Batch Loss: 1.7943, Training Accuracy: 0.1666\n",
      "Epoch [1/20], Batch [55/1359], Batch Loss: 1.8055, Training Accuracy: 0.1682\n",
      "Epoch [1/20], Batch [56/1359], Batch Loss: 1.8156, Training Accuracy: 0.1685\n",
      "Epoch [1/20], Batch [57/1359], Batch Loss: 1.8387, Training Accuracy: 0.1650\n",
      "Epoch [1/20], Batch [58/1359], Batch Loss: 1.7986, Training Accuracy: 0.1671\n",
      "Epoch [1/20], Batch [59/1359], Batch Loss: 1.7775, Training Accuracy: 0.1676\n",
      "Epoch [1/20], Batch [60/1359], Batch Loss: 1.8066, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [61/1359], Batch Loss: 1.8223, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [62/1359], Batch Loss: 1.8198, Training Accuracy: 0.1673\n",
      "Epoch [1/20], Batch [63/1359], Batch Loss: 1.7983, Training Accuracy: 0.1679\n",
      "Epoch [1/20], Batch [64/1359], Batch Loss: 1.8071, Training Accuracy: 0.1676\n",
      "Epoch [1/20], Batch [65/1359], Batch Loss: 1.8196, Training Accuracy: 0.1676\n",
      "Epoch [1/20], Batch [66/1359], Batch Loss: 1.8028, Training Accuracy: 0.1676\n",
      "Epoch [1/20], Batch [67/1359], Batch Loss: 1.7997, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [68/1359], Batch Loss: 1.8004, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [69/1359], Batch Loss: 1.7952, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [70/1359], Batch Loss: 1.8003, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [71/1359], Batch Loss: 1.8010, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [72/1359], Batch Loss: 1.8048, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [73/1359], Batch Loss: 1.7634, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [74/1359], Batch Loss: 1.8027, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [75/1359], Batch Loss: 1.8045, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [76/1359], Batch Loss: 1.8285, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [77/1359], Batch Loss: 1.7976, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [78/1359], Batch Loss: 1.7775, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [79/1359], Batch Loss: 1.7876, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [80/1359], Batch Loss: 1.7974, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [81/1359], Batch Loss: 1.8074, Training Accuracy: 0.1671\n",
      "Epoch [1/20], Batch [82/1359], Batch Loss: 1.7617, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [83/1359], Batch Loss: 1.7759, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [84/1359], Batch Loss: 1.7980, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [85/1359], Batch Loss: 1.8348, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [86/1359], Batch Loss: 1.8207, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [87/1359], Batch Loss: 1.7938, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [88/1359], Batch Loss: 1.7794, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [89/1359], Batch Loss: 1.7867, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [90/1359], Batch Loss: 1.7881, Training Accuracy: 0.1681\n",
      "Epoch [1/20], Batch [91/1359], Batch Loss: 1.7859, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [92/1359], Batch Loss: 1.7888, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [93/1359], Batch Loss: 1.7965, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [94/1359], Batch Loss: 1.8175, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [95/1359], Batch Loss: 1.7986, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [96/1359], Batch Loss: 1.7682, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [97/1359], Batch Loss: 1.7906, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [98/1359], Batch Loss: 1.8085, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [99/1359], Batch Loss: 1.7998, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [100/1359], Batch Loss: 1.7467, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [101/1359], Batch Loss: 1.7898, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [102/1359], Batch Loss: 1.8015, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [103/1359], Batch Loss: 1.7975, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [104/1359], Batch Loss: 1.8073, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [105/1359], Batch Loss: 1.7968, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [106/1359], Batch Loss: 1.8081, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [107/1359], Batch Loss: 1.7860, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [108/1359], Batch Loss: 1.7577, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [109/1359], Batch Loss: 1.7587, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [110/1359], Batch Loss: 1.8116, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [111/1359], Batch Loss: 1.7853, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [112/1359], Batch Loss: 1.7889, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [113/1359], Batch Loss: 1.7987, Training Accuracy: 0.1682\n",
      "Epoch [1/20], Batch [114/1359], Batch Loss: 1.8302, Training Accuracy: 0.1681\n",
      "Epoch [1/20], Batch [115/1359], Batch Loss: 1.8436, Training Accuracy: 0.1681\n",
      "Epoch [1/20], Batch [116/1359], Batch Loss: 1.7847, Training Accuracy: 0.1673\n",
      "Epoch [1/20], Batch [117/1359], Batch Loss: 1.7747, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [118/1359], Batch Loss: 1.8129, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [119/1359], Batch Loss: 1.7935, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [120/1359], Batch Loss: 1.7772, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [121/1359], Batch Loss: 1.7909, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [122/1359], Batch Loss: 1.8140, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [123/1359], Batch Loss: 1.8008, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [124/1359], Batch Loss: 1.8005, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [125/1359], Batch Loss: 1.7886, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [126/1359], Batch Loss: 1.7717, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [127/1359], Batch Loss: 1.8072, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [128/1359], Batch Loss: 1.7670, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [129/1359], Batch Loss: 1.7969, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [130/1359], Batch Loss: 1.8073, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [131/1359], Batch Loss: 1.8243, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [132/1359], Batch Loss: 1.7948, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [133/1359], Batch Loss: 1.7887, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [134/1359], Batch Loss: 1.7991, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [135/1359], Batch Loss: 1.8207, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [136/1359], Batch Loss: 1.7849, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [137/1359], Batch Loss: 1.7885, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [138/1359], Batch Loss: 1.7799, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [139/1359], Batch Loss: 1.8037, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [140/1359], Batch Loss: 1.7742, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [141/1359], Batch Loss: 1.7901, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [142/1359], Batch Loss: 1.7923, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [143/1359], Batch Loss: 1.7810, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [144/1359], Batch Loss: 1.7765, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [145/1359], Batch Loss: 1.7819, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [146/1359], Batch Loss: 1.8058, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [147/1359], Batch Loss: 1.7969, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [148/1359], Batch Loss: 1.7860, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [149/1359], Batch Loss: 1.7924, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [150/1359], Batch Loss: 1.8121, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [151/1359], Batch Loss: 1.7958, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [152/1359], Batch Loss: 1.7823, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [153/1359], Batch Loss: 1.8110, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [154/1359], Batch Loss: 1.8017, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [155/1359], Batch Loss: 1.8070, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [156/1359], Batch Loss: 1.7976, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [157/1359], Batch Loss: 1.7967, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [158/1359], Batch Loss: 1.7786, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [159/1359], Batch Loss: 1.7992, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [160/1359], Batch Loss: 1.7973, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [161/1359], Batch Loss: 1.7937, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [162/1359], Batch Loss: 1.7949, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [163/1359], Batch Loss: 1.7861, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [164/1359], Batch Loss: 1.7987, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [165/1359], Batch Loss: 1.8053, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [166/1359], Batch Loss: 1.7638, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [167/1359], Batch Loss: 1.8017, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [168/1359], Batch Loss: 1.7813, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [169/1359], Batch Loss: 1.7929, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [170/1359], Batch Loss: 1.7922, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [171/1359], Batch Loss: 1.7815, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [172/1359], Batch Loss: 1.7833, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [173/1359], Batch Loss: 1.8073, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [174/1359], Batch Loss: 1.7952, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [175/1359], Batch Loss: 1.7958, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [176/1359], Batch Loss: 1.7743, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [177/1359], Batch Loss: 1.8083, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [178/1359], Batch Loss: 1.7933, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [179/1359], Batch Loss: 1.8036, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [180/1359], Batch Loss: 1.8043, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [181/1359], Batch Loss: 1.7776, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [182/1359], Batch Loss: 1.7911, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [183/1359], Batch Loss: 1.8017, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [184/1359], Batch Loss: 1.8000, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [185/1359], Batch Loss: 1.8112, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [186/1359], Batch Loss: 1.7964, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [187/1359], Batch Loss: 1.8050, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [188/1359], Batch Loss: 1.8025, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [189/1359], Batch Loss: 1.8097, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [190/1359], Batch Loss: 1.8219, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [191/1359], Batch Loss: 1.7955, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [192/1359], Batch Loss: 1.8021, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [193/1359], Batch Loss: 1.8006, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [194/1359], Batch Loss: 1.7901, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [195/1359], Batch Loss: 1.7874, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [196/1359], Batch Loss: 1.8064, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [197/1359], Batch Loss: 1.7892, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [198/1359], Batch Loss: 1.7919, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [199/1359], Batch Loss: 1.7902, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [200/1359], Batch Loss: 1.7970, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [201/1359], Batch Loss: 1.7839, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [202/1359], Batch Loss: 1.8053, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [203/1359], Batch Loss: 1.7853, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [204/1359], Batch Loss: 1.7930, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [205/1359], Batch Loss: 1.7854, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [206/1359], Batch Loss: 1.7920, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [207/1359], Batch Loss: 1.7933, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [208/1359], Batch Loss: 1.8034, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [209/1359], Batch Loss: 1.7985, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [210/1359], Batch Loss: 1.7936, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [211/1359], Batch Loss: 1.7974, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [212/1359], Batch Loss: 1.7960, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [213/1359], Batch Loss: 1.7956, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [214/1359], Batch Loss: 1.7939, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [215/1359], Batch Loss: 1.7895, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [216/1359], Batch Loss: 1.8013, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [217/1359], Batch Loss: 1.7885, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [218/1359], Batch Loss: 1.7949, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [219/1359], Batch Loss: 1.7968, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [220/1359], Batch Loss: 1.7889, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [221/1359], Batch Loss: 1.7987, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [222/1359], Batch Loss: 1.7994, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [223/1359], Batch Loss: 1.7865, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [224/1359], Batch Loss: 1.7943, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [225/1359], Batch Loss: 1.7912, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [226/1359], Batch Loss: 1.7892, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [227/1359], Batch Loss: 1.7955, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [228/1359], Batch Loss: 1.7964, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [229/1359], Batch Loss: 1.7937, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [230/1359], Batch Loss: 1.7958, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [231/1359], Batch Loss: 1.7986, Training Accuracy: 0.1680\n",
      "Epoch [1/20], Batch [232/1359], Batch Loss: 1.8064, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [233/1359], Batch Loss: 1.7929, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [234/1359], Batch Loss: 1.7893, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [235/1359], Batch Loss: 1.7895, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [236/1359], Batch Loss: 1.7936, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [237/1359], Batch Loss: 1.7879, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [238/1359], Batch Loss: 1.7969, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [239/1359], Batch Loss: 1.7933, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [240/1359], Batch Loss: 1.8021, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [241/1359], Batch Loss: 1.7859, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [242/1359], Batch Loss: 1.7963, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [243/1359], Batch Loss: 1.7754, Training Accuracy: 0.1671\n",
      "Epoch [1/20], Batch [244/1359], Batch Loss: 1.7862, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [245/1359], Batch Loss: 1.7880, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [246/1359], Batch Loss: 1.7621, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [247/1359], Batch Loss: 1.8009, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [248/1359], Batch Loss: 1.8093, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [249/1359], Batch Loss: 1.7758, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [250/1359], Batch Loss: 1.7961, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [251/1359], Batch Loss: 1.8423, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [252/1359], Batch Loss: 1.8045, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [253/1359], Batch Loss: 1.8183, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [254/1359], Batch Loss: 1.8036, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [255/1359], Batch Loss: 1.7954, Training Accuracy: 0.1672\n",
      "Epoch [1/20], Batch [256/1359], Batch Loss: 1.7937, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [257/1359], Batch Loss: 1.8019, Training Accuracy: 0.1676\n",
      "Epoch [1/20], Batch [258/1359], Batch Loss: 1.7895, Training Accuracy: 0.1675\n",
      "Epoch [1/20], Batch [259/1359], Batch Loss: 1.7963, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [260/1359], Batch Loss: 1.8018, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [261/1359], Batch Loss: 1.7894, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [262/1359], Batch Loss: 1.7829, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [263/1359], Batch Loss: 1.7998, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [264/1359], Batch Loss: 1.7891, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [265/1359], Batch Loss: 1.8141, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [266/1359], Batch Loss: 1.7800, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [267/1359], Batch Loss: 1.7912, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [268/1359], Batch Loss: 1.7832, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [269/1359], Batch Loss: 1.7904, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [270/1359], Batch Loss: 1.7956, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [271/1359], Batch Loss: 1.7795, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [272/1359], Batch Loss: 1.7953, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [273/1359], Batch Loss: 1.7860, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [274/1359], Batch Loss: 1.7813, Training Accuracy: 0.1677\n",
      "Epoch [1/20], Batch [275/1359], Batch Loss: 1.7904, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [276/1359], Batch Loss: 1.7966, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [277/1359], Batch Loss: 1.7915, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [278/1359], Batch Loss: 1.7934, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [279/1359], Batch Loss: 1.8048, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [280/1359], Batch Loss: 1.8014, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [281/1359], Batch Loss: 1.7917, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [282/1359], Batch Loss: 1.7959, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [283/1359], Batch Loss: 1.7875, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [284/1359], Batch Loss: 1.7852, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [285/1359], Batch Loss: 1.7858, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [286/1359], Batch Loss: 1.7936, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [287/1359], Batch Loss: 1.7703, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [288/1359], Batch Loss: 1.7793, Training Accuracy: 0.1670\n",
      "Epoch [1/20], Batch [289/1359], Batch Loss: 1.8088, Training Accuracy: 0.1670\n",
      "\n",
      "Training interrupted. Exiting gracefully.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noams\\anaconda3\\envs\\DeepLearningProjectNew\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Step 2: Pad sequences\n",
    "def collate_fn(batch):\n",
    "    X_batch, y_batch = zip(*batch)\n",
    "    X_batch = pad_sequence(X_batch, batch_first=True)\n",
    "    y_batch = torch.tensor(y_batch)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=40, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "\n",
    "# Step 3: Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 100)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "        out = hidden[-1]  # the last layer's hidden state\n",
    "        out = self.fc1(out) \n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 300  # Size of FastText embedding\n",
    "hidden_size = 128  # Increased hidden size\n",
    "num_layers = 2  # Increased number of layers\n",
    "num_classes = 6\n",
    "num_epochs = 20  # Increased number of epochs\n",
    "learning_rate = 0.02  # Reduced learning rate\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Graceful stopping handler\n",
    "def signal_handler(sig, frame):\n",
    "    print(\"\\nTraining interrupted. Exiting gracefully.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    model.train()\n",
    "    return correct / total\n",
    "\n",
    "# Step 5: Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_epoch_loss = 0\n",
    "    num_batches = len(dataloader)  # Total number of batches in the current epoch\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(dataloader, start=1):  # Start batch_idx from 1\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        \n",
    "        train_accuracy = calculate_accuracy(model, dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{num_batches}], Batch Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "        sys.stdout.flush()  # Force immediate printing\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Epoch Loss: {total_epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01899331621825695\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.01701652817428112\n",
      "Gradient norm for parameter torch.Size([1024]): 0.041358835995197296\n",
      "Gradient norm for parameter torch.Size([1024]): 0.041358835995197296\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.062035225331783295\n",
      "Gradient norm for parameter torch.Size([6]): 0.14706331491470337\n",
      "Epoch [1/5], Loss: 1.7872, Training Accuracy: 0.1675\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.036493152379989624\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.12676629424095154\n",
      "Gradient norm for parameter torch.Size([1024]): 0.14075160026550293\n",
      "Gradient norm for parameter torch.Size([1024]): 0.14075160026550293\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.19672709703445435\n",
      "Gradient norm for parameter torch.Size([6]): 0.21300971508026123\n",
      "Epoch [1/5], Loss: 3.4322, Training Accuracy: 0.1675\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.1528807282447815\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.9613111019134521\n",
      "Gradient norm for parameter torch.Size([1024]): 0.1742972731590271\n",
      "Gradient norm for parameter torch.Size([1024]): 0.1742972731590271\n",
      "Gradient norm for parameter torch.Size([6, 256]): 5.545125484466553\n",
      "Gradient norm for parameter torch.Size([6]): 0.6565085053443909\n",
      "Epoch [1/5], Loss: 8.2713, Training Accuracy: 0.1675\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.1785762757062912\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.7711305022239685\n",
      "Gradient norm for parameter torch.Size([1024]): 0.24664205312728882\n",
      "Gradient norm for parameter torch.Size([1024]): 0.24664205312728882\n",
      "Gradient norm for parameter torch.Size([6, 256]): 2.4847569465637207\n",
      "Gradient norm for parameter torch.Size([6]): 0.482284277677536\n",
      "Epoch [1/5], Loss: 10.8678, Training Accuracy: 0.1675\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.024749189615249634\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.05029620975255966\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06021224707365036\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06021224707365036\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.19707271456718445\n",
      "Gradient norm for parameter torch.Size([6]): 0.22444525361061096\n",
      "Epoch [1/5], Loss: 12.6540, Training Accuracy: 0.1657\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.035145994275808334\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.05934285372495651\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0809900239109993\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0809900239109993\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.14078238606452942\n",
      "Gradient norm for parameter torch.Size([6]): 0.2198711484670639\n",
      "Epoch [1/5], Loss: 14.4327, Training Accuracy: 0.1658\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.1070498451590538\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 1.2150911092758179\n",
      "Gradient norm for parameter torch.Size([1024]): 0.27234020829200745\n",
      "Gradient norm for parameter torch.Size([1024]): 0.27234020829200745\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.8951750993728638\n",
      "Gradient norm for parameter torch.Size([6]): 0.13560448586940765\n",
      "Epoch [1/5], Loss: 16.2558, Training Accuracy: 0.1639\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.013471354730427265\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.023634174838662148\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04329792410135269\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04329792410135269\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.08885575085878372\n",
      "Gradient norm for parameter torch.Size([6]): 0.1615392118692398\n",
      "Epoch [1/5], Loss: 18.0460, Training Accuracy: 0.1648\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.021904004737734795\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.056397922337055206\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09944029152393341\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09944029152393341\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.15558552742004395\n",
      "Gradient norm for parameter torch.Size([6]): 0.29327157139778137\n",
      "Epoch [1/5], Loss: 19.8658, Training Accuracy: 0.1645\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.02093243971467018\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03220735862851143\n",
      "Gradient norm for parameter torch.Size([1024]): 0.048144128173589706\n",
      "Gradient norm for parameter torch.Size([1024]): 0.048144128173589706\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.13013418018817902\n",
      "Gradient norm for parameter torch.Size([6]): 0.21431860327720642\n",
      "Epoch [1/5], Loss: 21.6583, Training Accuracy: 0.1722\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.011361591517925262\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.020227329805493355\n",
      "Gradient norm for parameter torch.Size([1024]): 0.042741112411022186\n",
      "Gradient norm for parameter torch.Size([1024]): 0.042741112411022186\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07942315191030502\n",
      "Gradient norm for parameter torch.Size([6]): 0.16734062135219574\n",
      "Epoch [1/5], Loss: 23.4323, Training Accuracy: 0.1726\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.0179576613008976\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03612971305847168\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06457392126321793\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06457392126321793\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.15725742280483246\n",
      "Gradient norm for parameter torch.Size([6]): 0.2754969596862793\n",
      "Epoch [1/5], Loss: 25.2462, Training Accuracy: 0.1677\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01613316871225834\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0328347347676754\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06081835925579071\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06081835925579071\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.1343804895877838\n",
      "Gradient norm for parameter torch.Size([6]): 0.2639957070350647\n",
      "Epoch [1/5], Loss: 27.0423, Training Accuracy: 0.1678\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.015216733328998089\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.027543717995285988\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05233737453818321\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05233737453818321\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.12267376482486725\n",
      "Gradient norm for parameter torch.Size([6]): 0.23307988047599792\n",
      "Epoch [1/5], Loss: 28.8491, Training Accuracy: 0.1695\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01867472194135189\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03364728018641472\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06481167674064636\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06481167674064636\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.12387160211801529\n",
      "Gradient norm for parameter torch.Size([6]): 0.23417755961418152\n",
      "Epoch [1/5], Loss: 30.6337, Training Accuracy: 0.1679\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.011439427733421326\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.023798257112503052\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04758419841527939\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04758419841527939\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.10487516224384308\n",
      "Gradient norm for parameter torch.Size([6]): 0.20982132852077484\n",
      "Epoch [1/5], Loss: 32.4073, Training Accuracy: 0.1674\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01016098540276289\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0199278574436903\n",
      "Gradient norm for parameter torch.Size([1024]): 0.043651312589645386\n",
      "Gradient norm for parameter torch.Size([1024]): 0.043651312589645386\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07138197124004364\n",
      "Gradient norm for parameter torch.Size([6]): 0.15515679121017456\n",
      "Epoch [1/5], Loss: 34.1873, Training Accuracy: 0.1674\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.016720661893486977\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.033546071499586105\n",
      "Gradient norm for parameter torch.Size([1024]): 0.048516660928726196\n",
      "Gradient norm for parameter torch.Size([1024]): 0.048516660928726196\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.14793342351913452\n",
      "Gradient norm for parameter torch.Size([6]): 0.2258506417274475\n",
      "Epoch [1/5], Loss: 35.9861, Training Accuracy: 0.1691\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.013725710101425648\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.02916652336716652\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04650087282061577\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04650087282061577\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.12067215144634247\n",
      "Gradient norm for parameter torch.Size([6]): 0.19004885852336884\n",
      "Epoch [1/5], Loss: 37.7619, Training Accuracy: 0.1793\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.012656454928219318\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03529757633805275\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05604046210646629\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05604046210646629\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.15311580896377563\n",
      "Gradient norm for parameter torch.Size([6]): 0.25015437602996826\n",
      "Epoch [1/5], Loss: 39.5987, Training Accuracy: 0.1856\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.011475145816802979\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.034314028918743134\n",
      "Gradient norm for parameter torch.Size([1024]): 0.055467285215854645\n",
      "Gradient norm for parameter torch.Size([1024]): 0.055467285215854645\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.10731782764196396\n",
      "Gradient norm for parameter torch.Size([6]): 0.16647383570671082\n",
      "Epoch [1/5], Loss: 41.4192, Training Accuracy: 0.1819\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.021590007469058037\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.05233263596892357\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09665574133396149\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09665574133396149\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.11771449446678162\n",
      "Gradient norm for parameter torch.Size([6]): 0.21478058397769928\n",
      "Epoch [1/5], Loss: 43.2221, Training Accuracy: 0.1835\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.02264544554054737\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0342259481549263\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04543359950184822\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04543359950184822\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.10542309284210205\n",
      "Gradient norm for parameter torch.Size([6]): 0.16203027963638306\n",
      "Epoch [1/5], Loss: 45.0345, Training Accuracy: 0.1801\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.015478642657399178\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.02023671567440033\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03327803313732147\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03327803313732147\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.1031256690621376\n",
      "Gradient norm for parameter torch.Size([6]): 0.16671335697174072\n",
      "Epoch [1/5], Loss: 46.8550, Training Accuracy: 0.1787\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.013957520946860313\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.022643424570560455\n",
      "Gradient norm for parameter torch.Size([1024]): 0.047577764838933945\n",
      "Gradient norm for parameter torch.Size([1024]): 0.047577764838933945\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.10015994310379028\n",
      "Gradient norm for parameter torch.Size([6]): 0.21882374584674835\n",
      "Epoch [1/5], Loss: 48.6475, Training Accuracy: 0.1784\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.011201600544154644\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03677572309970856\n",
      "Gradient norm for parameter torch.Size([1024]): 0.08505323529243469\n",
      "Gradient norm for parameter torch.Size([1024]): 0.08505323529243469\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06990806013345718\n",
      "Gradient norm for parameter torch.Size([6]): 0.16167838871479034\n",
      "Epoch [1/5], Loss: 50.4121, Training Accuracy: 0.1756\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.0138157419860363\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.020917363464832306\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03315099701285362\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03315099701285362\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.09243782609701157\n",
      "Gradient norm for parameter torch.Size([6]): 0.18561093509197235\n",
      "Epoch [1/5], Loss: 52.1815, Training Accuracy: 0.1759\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01053464226424694\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03458850085735321\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05039579048752785\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05039579048752785\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.10247691720724106\n",
      "Gradient norm for parameter torch.Size([6]): 0.17997857928276062\n",
      "Epoch [1/5], Loss: 53.9477, Training Accuracy: 0.1723\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.02494898810982704\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.04258740693330765\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06788279861211777\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06788279861211777\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.1874915063381195\n",
      "Gradient norm for parameter torch.Size([6]): 0.2841920852661133\n",
      "Epoch [1/5], Loss: 55.7474, Training Accuracy: 0.1721\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.03225915506482124\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.15984518826007843\n",
      "Gradient norm for parameter torch.Size([1024]): 0.12967552244663239\n",
      "Gradient norm for parameter torch.Size([1024]): 0.12967552244663239\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.20884230732917786\n",
      "Gradient norm for parameter torch.Size([6]): 0.2622460126876831\n",
      "Epoch [1/5], Loss: 57.5774, Training Accuracy: 0.1785\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.02344946190714836\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.042014338076114655\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05000269040465355\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05000269040465355\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.12021293491125107\n",
      "Gradient norm for parameter torch.Size([6]): 0.21139246225357056\n",
      "Epoch [1/5], Loss: 59.3382, Training Accuracy: 0.1871\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.014511079527437687\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0214941818267107\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05597101151943207\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05597101151943207\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.0807410478591919\n",
      "Gradient norm for parameter torch.Size([6]): 0.23907668888568878\n",
      "Epoch [1/5], Loss: 61.1290, Training Accuracy: 0.1907\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.017506049945950508\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03094903938472271\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06300336122512817\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06300336122512817\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06715736538171768\n",
      "Gradient norm for parameter torch.Size([6]): 0.12409289181232452\n",
      "Epoch [1/5], Loss: 62.9079, Training Accuracy: 0.1888\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.025821072980761528\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.04308565706014633\n",
      "Gradient norm for parameter torch.Size([1024]): 0.08957599103450775\n",
      "Gradient norm for parameter torch.Size([1024]): 0.08957599103450775\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07512293756008148\n",
      "Gradient norm for parameter torch.Size([6]): 0.13853397965431213\n",
      "Epoch [1/5], Loss: 64.6989, Training Accuracy: 0.1876\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.014789760112762451\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03850478678941727\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03191289305686951\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03191289305686951\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.12079701572656631\n",
      "Gradient norm for parameter torch.Size([6]): 0.16601283848285675\n",
      "Epoch [1/5], Loss: 66.4238, Training Accuracy: 0.1852\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.020215990021824837\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.026193294674158096\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03338877484202385\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03338877484202385\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.09017299115657806\n",
      "Gradient norm for parameter torch.Size([6]): 0.15545053780078888\n",
      "Epoch [1/5], Loss: 68.1817, Training Accuracy: 0.1837\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.050296805799007416\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0716596245765686\n",
      "Gradient norm for parameter torch.Size([1024]): 0.11515385657548904\n",
      "Gradient norm for parameter torch.Size([1024]): 0.11515385657548904\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.11732801795005798\n",
      "Gradient norm for parameter torch.Size([6]): 0.15834198892116547\n",
      "Epoch [1/5], Loss: 69.9408, Training Accuracy: 0.1860\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.04890801012516022\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.11417125165462494\n",
      "Gradient norm for parameter torch.Size([1024]): 0.15474078059196472\n",
      "Gradient norm for parameter torch.Size([1024]): 0.15474078059196472\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.2507328987121582\n",
      "Gradient norm for parameter torch.Size([6]): 0.3327813148498535\n",
      "Epoch [1/5], Loss: 71.8693, Training Accuracy: 0.1856\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.03251722455024719\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.08015282452106476\n",
      "Gradient norm for parameter torch.Size([1024]): 0.11487732827663422\n",
      "Gradient norm for parameter torch.Size([1024]): 0.11487732827663422\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.11661872267723083\n",
      "Gradient norm for parameter torch.Size([6]): 0.09805012494325638\n",
      "Epoch [1/5], Loss: 73.6355, Training Accuracy: 0.1845\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.019009359180927277\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0559261254966259\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07913368195295334\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07913368195295334\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.11798695474863052\n",
      "Gradient norm for parameter torch.Size([6]): 0.17314513027668\n",
      "Epoch [1/5], Loss: 75.4033, Training Accuracy: 0.1837\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.016004275530576706\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.02411644160747528\n",
      "Gradient norm for parameter torch.Size([1024]): 0.02782275527715683\n",
      "Gradient norm for parameter torch.Size([1024]): 0.02782275527715683\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.08063740283250809\n",
      "Gradient norm for parameter torch.Size([6]): 0.12965604662895203\n",
      "Epoch [1/5], Loss: 77.1649, Training Accuracy: 0.1808\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.02337288111448288\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.050100184977054596\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07755362242460251\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07755362242460251\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.08956126123666763\n",
      "Gradient norm for parameter torch.Size([6]): 0.15075741708278656\n",
      "Epoch [1/5], Loss: 78.9199, Training Accuracy: 0.1760\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01593804359436035\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03510056808590889\n",
      "Gradient norm for parameter torch.Size([1024]): 0.050341758877038956\n",
      "Gradient norm for parameter torch.Size([1024]): 0.050341758877038956\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.18789605796337128\n",
      "Gradient norm for parameter torch.Size([6]): 0.30764883756637573\n",
      "Epoch [1/5], Loss: 80.6973, Training Accuracy: 0.1737\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.026199443265795708\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.04709683358669281\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05174054205417633\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05174054205417633\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.12142987549304962\n",
      "Gradient norm for parameter torch.Size([6]): 0.17453746497631073\n",
      "Epoch [1/5], Loss: 82.5103, Training Accuracy: 0.1741\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.028623633086681366\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.07016842812299728\n",
      "Gradient norm for parameter torch.Size([1024]): 0.10302320867776871\n",
      "Gradient norm for parameter torch.Size([1024]): 0.10302320867776871\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.1986764520406723\n",
      "Gradient norm for parameter torch.Size([6]): 0.33282673358917236\n",
      "Epoch [1/5], Loss: 84.2773, Training Accuracy: 0.1712\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.018817206844687462\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.04532652348279953\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09444942325353622\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09444942325353622\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.11293277889490128\n",
      "Gradient norm for parameter torch.Size([6]): 0.21661892533302307\n",
      "Epoch [1/5], Loss: 86.0776, Training Accuracy: 0.1729\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.030224410817027092\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.06670882552862167\n",
      "Gradient norm for parameter torch.Size([1024]): 0.1458692103624344\n",
      "Gradient norm for parameter torch.Size([1024]): 0.1458692103624344\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.13854026794433594\n",
      "Gradient norm for parameter torch.Size([6]): 0.32166022062301636\n",
      "Epoch [1/5], Loss: 87.8192, Training Accuracy: 0.1691\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.014018846675753593\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03489580377936363\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0378640815615654\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0378640815615654\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.21340273320674896\n",
      "Gradient norm for parameter torch.Size([6]): 0.22251494228839874\n",
      "Epoch [1/5], Loss: 89.5990, Training Accuracy: 0.1655\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.033342424780130386\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.08756539970636368\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04984504356980324\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04984504356980324\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.2108156830072403\n",
      "Gradient norm for parameter torch.Size([6]): 0.2101621776819229\n",
      "Epoch [1/5], Loss: 91.4720, Training Accuracy: 0.1648\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01645381562411785\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03420970216393471\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04625169187784195\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04625169187784195\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.12973198294639587\n",
      "Gradient norm for parameter torch.Size([6]): 0.21520137786865234\n",
      "Epoch [1/5], Loss: 93.2761, Training Accuracy: 0.1648\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.010352465324103832\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0228432584553957\n",
      "Gradient norm for parameter torch.Size([1024]): 0.044639889150857925\n",
      "Gradient norm for parameter torch.Size([1024]): 0.044639889150857925\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.1147366389632225\n",
      "Gradient norm for parameter torch.Size([6]): 0.18337863683700562\n",
      "Epoch [1/5], Loss: 95.0694, Training Accuracy: 0.1664\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.03295683488249779\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.07472172379493713\n",
      "Gradient norm for parameter torch.Size([1024]): 0.1482740044593811\n",
      "Gradient norm for parameter torch.Size([1024]): 0.1482740044593811\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.14099076390266418\n",
      "Gradient norm for parameter torch.Size([6]): 0.2812449336051941\n",
      "Epoch [1/5], Loss: 96.9185, Training Accuracy: 0.1664\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.019400455057621002\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.03359818831086159\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07483745366334915\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07483745366334915\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.13053005933761597\n",
      "Gradient norm for parameter torch.Size([6]): 0.3370020091533661\n",
      "Epoch [1/5], Loss: 98.7372, Training Accuracy: 0.1668\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.018959511071443558\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.027153244242072105\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0662497878074646\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0662497878074646\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07907784730195999\n",
      "Gradient norm for parameter torch.Size([6]): 0.2178421914577484\n",
      "Epoch [1/5], Loss: 100.4913, Training Accuracy: 0.1671\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.007555236108601093\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.012968771159648895\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05047404021024704\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05047404021024704\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06516527384519577\n",
      "Gradient norm for parameter torch.Size([6]): 0.24892453849315643\n",
      "Epoch [1/5], Loss: 102.2467, Training Accuracy: 0.1685\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.00653046416118741\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.008374379016458988\n",
      "Gradient norm for parameter torch.Size([1024]): 0.02567272074520588\n",
      "Gradient norm for parameter torch.Size([1024]): 0.02567272074520588\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.052822891622781754\n",
      "Gradient norm for parameter torch.Size([6]): 0.1597272902727127\n",
      "Epoch [1/5], Loss: 104.0040, Training Accuracy: 0.1687\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.00863539706915617\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.009588634595274925\n",
      "Gradient norm for parameter torch.Size([1024]): 0.028899691998958588\n",
      "Gradient norm for parameter torch.Size([1024]): 0.028899691998958588\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06906279921531677\n",
      "Gradient norm for parameter torch.Size([6]): 0.19964781403541565\n",
      "Epoch [1/5], Loss: 105.7663, Training Accuracy: 0.1682\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.008857018314301968\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010211721993982792\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03938126564025879\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03938126564025879\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.08068903535604477\n",
      "Gradient norm for parameter torch.Size([6]): 0.290332168340683\n",
      "Epoch [1/5], Loss: 107.5548, Training Accuracy: 0.1682\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.012514295987784863\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.01806572452187538\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06644240766763687\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06644240766763687\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07907533645629883\n",
      "Gradient norm for parameter torch.Size([6]): 0.2833750545978546\n",
      "Epoch [1/5], Loss: 109.3177, Training Accuracy: 0.1681\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01576048508286476\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.01711614616215229\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0580608956515789\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0580608956515789\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.08427317440509796\n",
      "Gradient norm for parameter torch.Size([6]): 0.29682180285453796\n",
      "Epoch [1/5], Loss: 111.1403, Training Accuracy: 0.1685\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01341951172798872\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.016321158036589622\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0463624969124794\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0463624969124794\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.0792083591222763\n",
      "Gradient norm for parameter torch.Size([6]): 0.2643239498138428\n",
      "Epoch [1/5], Loss: 112.9339, Training Accuracy: 0.1690\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.007132760714739561\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.009665912948548794\n",
      "Gradient norm for parameter torch.Size([1024]): 0.019856655970215797\n",
      "Gradient norm for parameter torch.Size([1024]): 0.019856655970215797\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.04064592346549034\n",
      "Gradient norm for parameter torch.Size([6]): 0.12272653728723526\n",
      "Epoch [1/5], Loss: 114.7033, Training Accuracy: 0.1704\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01128243375569582\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.017747044563293457\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0539894625544548\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0539894625544548\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07012966275215149\n",
      "Gradient norm for parameter torch.Size([6]): 0.24050652980804443\n",
      "Epoch [1/5], Loss: 116.5059, Training Accuracy: 0.1716\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.003989741671830416\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010245848447084427\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03093738481402397\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03093738481402397\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.03907762095332146\n",
      "Gradient norm for parameter torch.Size([6]): 0.13693198561668396\n",
      "Epoch [1/5], Loss: 118.3026, Training Accuracy: 0.1742\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.0094172153621912\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.017796779051423073\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05945288762450218\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05945288762450218\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07400482892990112\n",
      "Gradient norm for parameter torch.Size([6]): 0.25083523988723755\n",
      "Epoch [1/5], Loss: 120.0931, Training Accuracy: 0.1786\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.0060560996644198895\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.00822941679507494\n",
      "Gradient norm for parameter torch.Size([1024]): 0.02712470293045044\n",
      "Gradient norm for parameter torch.Size([1024]): 0.02712470293045044\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.045477643609046936\n",
      "Gradient norm for parameter torch.Size([6]): 0.14988172054290771\n",
      "Epoch [1/5], Loss: 121.8495, Training Accuracy: 0.1809\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.008716876618564129\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010749222710728645\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03067137859761715\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03067137859761715\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05799263343214989\n",
      "Gradient norm for parameter torch.Size([6]): 0.16832184791564941\n",
      "Epoch [1/5], Loss: 123.6646, Training Accuracy: 0.1814\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.008408485911786556\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.012212257832288742\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04268778860569\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04268778860569\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06625589728355408\n",
      "Gradient norm for parameter torch.Size([6]): 0.222031369805336\n",
      "Epoch [1/5], Loss: 125.4644, Training Accuracy: 0.1813\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.009801024571061134\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.01338197197765112\n",
      "Gradient norm for parameter torch.Size([1024]): 0.039506103843450546\n",
      "Gradient norm for parameter torch.Size([1024]): 0.039506103843450546\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.0660313293337822\n",
      "Gradient norm for parameter torch.Size([6]): 0.21165843307971954\n",
      "Epoch [1/5], Loss: 127.2731, Training Accuracy: 0.1805\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.005166822113096714\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.005885088350623846\n",
      "Gradient norm for parameter torch.Size([1024]): 0.014185035601258278\n",
      "Gradient norm for parameter torch.Size([1024]): 0.014185035601258278\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.03651758283376694\n",
      "Gradient norm for parameter torch.Size([6]): 0.09868532419204712\n",
      "Epoch [1/5], Loss: 129.0722, Training Accuracy: 0.1799\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.005578590556979179\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010534276254475117\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03760642930865288\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03760642930865288\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06704273074865341\n",
      "Gradient norm for parameter torch.Size([6]): 0.24924494326114655\n",
      "Epoch [1/5], Loss: 130.9014, Training Accuracy: 0.1794\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.012233994901180267\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010713462717831135\n",
      "Gradient norm for parameter torch.Size([1024]): 0.036013465374708176\n",
      "Gradient norm for parameter torch.Size([1024]): 0.036013465374708176\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06757540255784988\n",
      "Gradient norm for parameter torch.Size([6]): 0.22324784100055695\n",
      "Epoch [1/5], Loss: 132.7274, Training Accuracy: 0.1793\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.007892108522355556\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010010880418121815\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04022917151451111\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04022917151451111\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.04971480369567871\n",
      "Gradient norm for parameter torch.Size([6]): 0.202961727976799\n",
      "Epoch [1/5], Loss: 134.5313, Training Accuracy: 0.1794\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.006595768500119448\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.00825710128992796\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03536748141050339\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03536748141050339\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05459684878587723\n",
      "Gradient norm for parameter torch.Size([6]): 0.24603845179080963\n",
      "Epoch [1/5], Loss: 136.3069, Training Accuracy: 0.1803\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.00960961077362299\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010033429600298405\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0311661995947361\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0311661995947361\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05717092379927635\n",
      "Gradient norm for parameter torch.Size([6]): 0.2008979618549347\n",
      "Epoch [1/5], Loss: 138.0972, Training Accuracy: 0.1860\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.007459725718945265\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.008527249097824097\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04356320574879646\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04356320574879646\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05352567136287689\n",
      "Gradient norm for parameter torch.Size([6]): 0.27855589985847473\n",
      "Epoch [1/5], Loss: 139.8684, Training Accuracy: 0.1869\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.009740338660776615\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.008335841819643974\n",
      "Gradient norm for parameter torch.Size([1024]): 0.036021240055561066\n",
      "Gradient norm for parameter torch.Size([1024]): 0.036021240055561066\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.046930912882089615\n",
      "Gradient norm for parameter torch.Size([6]): 0.21013353765010834\n",
      "Epoch [1/5], Loss: 141.6646, Training Accuracy: 0.1890\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.0074675907380878925\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.009439646266400814\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05013071373105049\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05013071373105049\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.04059961810708046\n",
      "Gradient norm for parameter torch.Size([6]): 0.19814470410346985\n",
      "Epoch [1/5], Loss: 143.4457, Training Accuracy: 0.1899\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.004043679218739271\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.005307640414685011\n",
      "Gradient norm for parameter torch.Size([1024]): 0.019229883328080177\n",
      "Gradient norm for parameter torch.Size([1024]): 0.019229883328080177\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.024176528677344322\n",
      "Gradient norm for parameter torch.Size([6]): 0.08553215116262436\n",
      "Epoch [1/5], Loss: 145.2204, Training Accuracy: 0.1901\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.010099637322127819\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010388188064098358\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06679792702198029\n",
      "Gradient norm for parameter torch.Size([1024]): 0.06679792702198029\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.0523613877594471\n",
      "Gradient norm for parameter torch.Size([6]): 0.34747475385665894\n",
      "Epoch [1/5], Loss: 146.9834, Training Accuracy: 0.1904\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.005606707651168108\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.005499439779669046\n",
      "Gradient norm for parameter torch.Size([1024]): 0.018825681880116463\n",
      "Gradient norm for parameter torch.Size([1024]): 0.018825681880116463\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.03705059364438057\n",
      "Gradient norm for parameter torch.Size([6]): 0.1409403383731842\n",
      "Epoch [1/5], Loss: 148.7585, Training Accuracy: 0.1893\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.00976052787154913\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.008433332666754723\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0429873913526535\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0429873913526535\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05484365299344063\n",
      "Gradient norm for parameter torch.Size([6]): 0.28303298354148865\n",
      "Epoch [1/5], Loss: 150.5545, Training Accuracy: 0.1890\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.010078241117298603\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.013283709064126015\n",
      "Gradient norm for parameter torch.Size([1024]): 0.08183782547712326\n",
      "Gradient norm for parameter torch.Size([1024]): 0.08183782547712326\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05078386515378952\n",
      "Gradient norm for parameter torch.Size([6]): 0.3043058514595032\n",
      "Epoch [1/5], Loss: 152.3223, Training Accuracy: 0.1911\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.010039689019322395\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.007826538756489754\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04583102464675903\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04583102464675903\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.0434134304523468\n",
      "Gradient norm for parameter torch.Size([6]): 0.20014086365699768\n",
      "Epoch [1/5], Loss: 154.1077, Training Accuracy: 0.1918\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01319543831050396\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.009218435734510422\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05893491208553314\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05893491208553314\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.04647678881883621\n",
      "Gradient norm for parameter torch.Size([6]): 0.29175227880477905\n",
      "Epoch [1/5], Loss: 155.9140, Training Accuracy: 0.1919\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.007212568074464798\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.006457152776420116\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03384251892566681\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03384251892566681\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.036882732063531876\n",
      "Gradient norm for parameter torch.Size([6]): 0.17486152052879333\n",
      "Epoch [1/5], Loss: 157.7106, Training Accuracy: 0.1915\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.009653122164309025\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.007764440029859543\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04741761460900307\n",
      "Gradient norm for parameter torch.Size([1024]): 0.04741761460900307\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.02895723655819893\n",
      "Gradient norm for parameter torch.Size([6]): 0.15554095804691315\n",
      "Epoch [1/5], Loss: 159.4948, Training Accuracy: 0.1926\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.006623673252761364\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.006555901374667883\n",
      "Gradient norm for parameter torch.Size([1024]): 0.030907845124602318\n",
      "Gradient norm for parameter torch.Size([1024]): 0.030907845124602318\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.055811647325754166\n",
      "Gradient norm for parameter torch.Size([6]): 0.17481260001659393\n",
      "Epoch [1/5], Loss: 161.2824, Training Accuracy: 0.1904\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.0071412986144423485\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.0092111611738801\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0382530577480793\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0382530577480793\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06131483241915703\n",
      "Gradient norm for parameter torch.Size([6]): 0.279039204120636\n",
      "Epoch [1/5], Loss: 163.0384, Training Accuracy: 0.1902\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01929842121899128\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.020588211715221405\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05718231946229935\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05718231946229935\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06496492028236389\n",
      "Gradient norm for parameter torch.Size([6]): 0.21881666779518127\n",
      "Epoch [1/5], Loss: 164.7788, Training Accuracy: 0.1903\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.012793985195457935\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.017291992902755737\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0625680610537529\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0625680610537529\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06423940509557724\n",
      "Gradient norm for parameter torch.Size([6]): 0.2347295880317688\n",
      "Epoch [1/5], Loss: 166.5617, Training Accuracy: 0.1869\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.010842455551028252\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.017826726660132408\n",
      "Gradient norm for parameter torch.Size([1024]): 0.014695461839437485\n",
      "Gradient norm for parameter torch.Size([1024]): 0.014695461839437485\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05070950463414192\n",
      "Gradient norm for parameter torch.Size([6]): 0.09382849931716919\n",
      "Epoch [1/5], Loss: 168.3015, Training Accuracy: 0.1847\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.00949652399867773\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.016377795487642288\n",
      "Gradient norm for parameter torch.Size([1024]): 0.046117402613162994\n",
      "Gradient norm for parameter torch.Size([1024]): 0.046117402613162994\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06652619689702988\n",
      "Gradient norm for parameter torch.Size([6]): 0.16675618290901184\n",
      "Epoch [1/5], Loss: 170.0978, Training Accuracy: 0.1808\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.028397828340530396\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.046005528420209885\n",
      "Gradient norm for parameter torch.Size([1024]): 0.13908535242080688\n",
      "Gradient norm for parameter torch.Size([1024]): 0.13908535242080688\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.11489509046077728\n",
      "Gradient norm for parameter torch.Size([6]): 0.29281312227249146\n",
      "Epoch [1/5], Loss: 171.9770, Training Accuracy: 0.1851\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.015905221924185753\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.026746593415737152\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07292325794696808\n",
      "Gradient norm for parameter torch.Size([1024]): 0.07292325794696808\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.11183679848909378\n",
      "Gradient norm for parameter torch.Size([6]): 0.3450130522251129\n",
      "Epoch [1/5], Loss: 173.7330, Training Accuracy: 0.1860\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.02438395842909813\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.04354950785636902\n",
      "Gradient norm for parameter torch.Size([1024]): 0.10005130618810654\n",
      "Gradient norm for parameter torch.Size([1024]): 0.10005130618810654\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.1004302054643631\n",
      "Gradient norm for parameter torch.Size([6]): 0.2504495680332184\n",
      "Epoch [1/5], Loss: 175.5791, Training Accuracy: 0.1905\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.00945263635367155\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.00788384210318327\n",
      "Gradient norm for parameter torch.Size([1024]): 0.045859720557928085\n",
      "Gradient norm for parameter torch.Size([1024]): 0.045859720557928085\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.036458808928728104\n",
      "Gradient norm for parameter torch.Size([6]): 0.17232142388820648\n",
      "Epoch [1/5], Loss: 177.3772, Training Accuracy: 0.1922\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.01703321747481823\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.027608299627900124\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05003991723060608\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05003991723060608\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.08263765275478363\n",
      "Gradient norm for parameter torch.Size([6]): 0.22196663916110992\n",
      "Epoch [1/5], Loss: 179.0907, Training Accuracy: 0.1933\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.0057999868877232075\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.010568656958639622\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03572968766093254\n",
      "Gradient norm for parameter torch.Size([1024]): 0.03572968766093254\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.0617830790579319\n",
      "Gradient norm for parameter torch.Size([6]): 0.1448146551847458\n",
      "Epoch [1/5], Loss: 180.8667, Training Accuracy: 0.1932\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.023175878450274467\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.01981385238468647\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09657032787799835\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09657032787799835\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.08840290457010269\n",
      "Gradient norm for parameter torch.Size([6]): 0.2446511685848236\n",
      "Epoch [1/5], Loss: 182.6916, Training Accuracy: 0.1967\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.005586783867329359\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.006631473079323769\n",
      "Gradient norm for parameter torch.Size([1024]): 0.027488013729453087\n",
      "Gradient norm for parameter torch.Size([1024]): 0.027488013729453087\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.044538553804159164\n",
      "Gradient norm for parameter torch.Size([6]): 0.13269442319869995\n",
      "Epoch [1/5], Loss: 184.4576, Training Accuracy: 0.1964\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.013402837328612804\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.009854407049715519\n",
      "Gradient norm for parameter torch.Size([1024]): 0.049262989312410355\n",
      "Gradient norm for parameter torch.Size([1024]): 0.049262989312410355\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.06256746500730515\n",
      "Gradient norm for parameter torch.Size([6]): 0.2331325113773346\n",
      "Epoch [1/5], Loss: 186.2160, Training Accuracy: 0.1968\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.018555743619799614\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.017349883913993835\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05637254938483238\n",
      "Gradient norm for parameter torch.Size([1024]): 0.05637254938483238\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05530576407909393\n",
      "Gradient norm for parameter torch.Size([6]): 0.2051084339618683\n",
      "Epoch [1/5], Loss: 188.0356, Training Accuracy: 0.1943\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.019890127703547478\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.014972448348999023\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09618417918682098\n",
      "Gradient norm for parameter torch.Size([1024]): 0.09618417918682098\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.04136989638209343\n",
      "Gradient norm for parameter torch.Size([6]): 0.26404547691345215\n",
      "Epoch [1/5], Loss: 189.8118, Training Accuracy: 0.1922\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.026672476902604103\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.017193831503391266\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0936923399567604\n",
      "Gradient norm for parameter torch.Size([1024]): 0.0936923399567604\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.05692039802670479\n",
      "Gradient norm for parameter torch.Size([6]): 0.2552388608455658\n",
      "Epoch [1/5], Loss: 191.6170, Training Accuracy: 0.1906\n",
      "Gradient norm for parameter torch.Size([1024, 300]): 0.008920429274439812\n",
      "Gradient norm for parameter torch.Size([1024, 256]): 0.01692642644047737\n",
      "Gradient norm for parameter torch.Size([1024]): 0.028887661173939705\n",
      "Gradient norm for parameter torch.Size([1024]): 0.028887661173939705\n",
      "Gradient norm for parameter torch.Size([6, 256]): 0.07395429164171219\n",
      "Gradient norm for parameter torch.Size([6]): 0.18801994621753693\n",
      "\n",
      "Training interrupted. Exiting gracefully.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noams\\anaconda3\\envs\\DeepLearningProjectNew\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Step 2: Pad sequences\n",
    "def collate_fn(batch):\n",
    "    X_batch, y_batch = zip(*batch)\n",
    "    X_batch = pad_sequence(X_batch, batch_first=True)\n",
    "    y_batch = torch.tensor(y_batch)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate_fn)\n",
    "\n",
    "# Step 3: Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = torch.mean(lstm_out, dim=1)  # Mean pooling across time steps\n",
    "        out = self.fc(out)  # Use the final hidden state\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 300  # Size of FastText embedding\n",
    "hidden_size = 256\n",
    "num_classes = 6\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Graceful stopping handler\n",
    "def signal_handler(sig, frame):\n",
    "    print(\"\\nTraining interrupted. Exiting gracefully.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    model.train()\n",
    "    return correct / total\n",
    "\n",
    "# Step 5: Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_epoch_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "         # Debugging: Print gradient norms\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                print(f\"Gradient norm for parameter {p.shape}: {p.grad.norm().item()}\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "    \n",
    "        # Print loss and accuracies\n",
    "        train_accuracy = calculate_accuracy(model, dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_epoch_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    89832.000000\n",
       "mean        19.438986\n",
       "std         11.085368\n",
       "min          1.000000\n",
       "25%         11.000000\n",
       "50%         17.000000\n",
       "75%         26.000000\n",
       "max        100.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed[\"text\"].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing tokens: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Check the percentage of tokens in the dataset that are missing in the FastText vocabulary\n",
    "missing_tokens = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for seq in X_train:\n",
    "    for token in seq:\n",
    "        total_tokens += 1\n",
    "        if token not in fasttext.wv:\n",
    "            missing_tokens += 1\n",
    "\n",
    "missing_percentage = (missing_tokens / total_tokens) * 100\n",
    "print(f\"Percentage of missing tokens: {missing_percentage:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningProjectNew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
